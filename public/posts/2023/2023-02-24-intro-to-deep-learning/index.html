<!doctype html><html lang=vi dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Intro to Deep Learning | The Financial Engineer</title>
<meta name=keywords content="Python,Deep Learning,Machine Learning,Kaggle"><meta name=description content="My Kaggle Learning Note 1"><meta name=author content="Kean Teng Blog"><link rel=canonical href=http://localhost:1313/posts/2023/2023-02-24-intro-to-deep-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=vi href=http://localhost:1313/posts/2023/2023-02-24-intro-to-deep-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Intro to Deep Learning"><meta property="og:description" content="My Kaggle Learning Note 1"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/2023/2023-02-24-intro-to-deep-learning/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-24T00:00:00+00:00"><meta property="article:modified_time" content="2023-02-24T00:00:00+00:00"><meta property="og:site_name" content="QHung's Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Intro to Deep Learning"><meta name=twitter:description content="My Kaggle Learning Note 1"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Intro to Deep Learning","item":"http://localhost:1313/posts/2023/2023-02-24-intro-to-deep-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Intro to Deep Learning","name":"Intro to Deep Learning","description":"My Kaggle Learning Note 1","keywords":["Python","Deep Learning","Machine Learning","Kaggle"],"articleBody":" Disclaimer: This article is my learning note from the courses I took from Kaggle.\n0. Concepts Creation and evaluation of a deep learning model is a procedural work with steps. Data preparation » Model optimization» Model fitting » Model evaluation » Model prediction\nNote:\nModel optimization (adam optimizer) Model fitting (batch, epoch) Model evaluation(dropout, batch normalization) 1. Modelling \u0026 Neural Network Terms: layer, activation function, neural network, dense, rectified linear unit\nThe above neural network can be generated with code using Python. The network above organized neurons into layers. Collecting linear units with common set of inputs result in a dense layer.\n1 2 3 4 5 6 7 8 9 10 11 from tensorflow import keras from tensorflow.keras import layers # 1 linear unit network model = keras.Sequential([ layers.Dense(units = 1, input_shape = [1]) ]) model = keras.Sequential([ layers.Dense(units = 1, input_shape = [3]) ]) If we want to fit a curve (non-linear), a feature called activation function is needed, otherwise the neural network can only learn linear relationships. A common example is the rectifier function max(0,x), where we get a rectified linear unit when we attach the function to a linear unit.\nWe can also stack layers to achieve a complex data transformation.\nUsing code to define the network:\n1 2 3 4 5 6 7 8 9 from tensorflow import keras from tensorflow.keras import keras # [layer1, layer2, layer3,...] model = keras.Sequential([ layers.Dense(units = 4, activation = 'relu', input_shape = [2], layers.Dense(units = 3, activation = 'relu'), layers.Dense(units = 1) ]) 2. Model Optimization \u0026 Fitting Terms: loss function, stochastic gradient descent, batch, epoch, error/loss\nSimilar to regression, we need to know how well our model fits the data. We use loss function to measures the difference between observed and predicted values. The common measure would be the mean absolute error where it computed the average length between the fitted curve and data points. Other errors examples are mean-squared error or Huber loss.\nHow can we minimize the error? We can use optimization algorithms — the stochastic gradient descent. The concepts as follows:\nGet random sample (batch) from original dataset and run through the network for prediction. Measure the error (loss) Adjust the weight in a direction that makes the loss smaller (an epoch for each complete round a training) We can use a built-in “adam” optimizer to optimize our model. This allows self-tuning to minimize loss. Below is code to fit 256 rows of training data for 10 times:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # get training data dimension (row, column) print(X_train.shape) # define model from tensorflow import keras from tensorflow.keras import layers mode = keras.Sequential([ layers.Dense(512, activation = 'relu', input_shape = [11]), layers.Dense(512, activation='relu'), layers.Dense(512, activation='relu'), layers.Dense(1), ]) # add in optimizer and loss function model.compile( optimizer = 'adam', loss = 'mae', ) # model fitting history = model.fit( X_train, y_train, validation_data = (X_valid, y_valid), batch_size = 256, epochs = 10, ) # visualize outcome import pandas as pd # training history to data frame history_df = pd.DataFrame(history.history) # plot the loss history_df['loss'].plot(); 3. Model Evaluation 1 Terms: underfitting, overfitting, capacity, training, callback, stopping criteria\nTraining data normally contains signal and noise where signal helps our model make prediction from new data and noise represent the random fluctuation coming from data in the real world. To see if our model fits the data well, we will compare learning curve between training set and validation set.\nUnderfitting the training set is when the loss is not as low as it could be because the model hasn’t learned enough signal. Overfitting the training set is when the loss is not as low as it could be because the model learned too much noise.\nA model’s capacity is the size and complexity of the patterns it is able to learn. We can adjust a model’s capacity if we detect overfitting or underfitting scenarios.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # sample model model = keras.Sequential([ layers.Dense(16, activation = 'relu'), layers.Dense(1), ]) # wider model = keras.Sequential([ layers.Dense(32, activation = 'relu'), layers.Dense(1), ]) # deeper model = keras.Sequential([ layers.Dense(16, activation = 'relu'), layers.Dense(16, activation = 'relu'), layers.Dense(1), ]) Sometimes the validation increase during training after a certain point although it kept decreasing early on. This can due to a model is learning noise from the datasets. We can overcome this issue by imposing an early stopping criterion.\nA callback function is used where we detect when the validation loss starts to rise again, and we reset the weights back the where the minimum occurred. An example “if there is not at least 0.001 improvement in the validation loss over 20 epochs, then stop training and keep the best model you found”.\n1 2 3 4 5 6 7 from tensorflow.keras.callbacks import EarlyStopping early_stopping = EarlyStopping( min_delta = 0. 001, # min change to count as improvement patience = 20, # how many epochs to wait before stopping restore_best_weights = True, ) 4. Model Evaluation 2 Terms: dropout, batch normalization\nTo correct overfitting in our model, we can implement the idea of dropout where we randomly drop out some fraction of a layer’s input units every step of training. This avoids the network to learn spurious patterns in the training data which leads to overfitting.\n1 2 3 4 5 6 model = keras.Sequential([ ... layers.Dropout(rate = 0.3) # 30% dropout to the next layer layers.Dense(16), ... ]) To correct slow or unstable training, we can apply batch normalization to put all data on a common scale. SGD will shift the network weights in proportion to how large an activation the data produces. Features that tend to produce activations of very different sizes can make for unstable training behavior.\n1 2 3 4 5 6 7 8 9 10 11 model = keras.Sequential([ layers.Dense(16, activation = 'relu'), layers.BatchNormalization(), ]) # or model = keras.Sequential([ layers.Dense(16), layers.BatchNormalization(), layers.Activation('relu'), ]) Example for a full model fitting process:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from tensorflow import keras from tensorflow.keras import layers # define network model = keras.Sequential([ layers.Dense(1024, activation='relu', input_shape=[11]), layers.Dropout(0.3), layers.BatchNormalization(), layers.Dense(1024, activation='relu'), layers.Dropout(0.3), layers.BatchNormalization(), layers.Dense(1024, activation='relu'), layers.Dropout(0.3), layers.BatchNormalization(), layers.Dense(1), ]) # add optimier model.compile( optimizer = 'adam', loss = 'mae', ) # model fitting history = model.fit( X_train, y_train, validation_data = (X_valid, y_valid), batch_size = 256, epochs = 100, verbose = 0, ) # visualize history_df = pd.DataFrame(history.history) history_df.loc[:,['loss','val_loss']].plot(); 5. Binary Classification Terms: sigmoid, binary, accuracy\nWe cannot use accuracy to measure model performance as it does not change smoothly — it changes in jumps as it represents ratio counts. Thus, a replacement would be the cross-entropy function where it measures the distance between probabilities. To convert outputs from dense layer into probabilities, we will use the sigmoid activation function.\nCode example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 from tensorflow import keras from tensorflow.keras import layers # define network model = keras.Sequential([ layers.Dense(4, activation='relu', input_shape=[33]), layers.Dense(4, activation='relu'), # final layer need sigmoid function to produce class probabilities layers.Dense(1, activation='sigmoid'), ]) # add optimizer model.compile( optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['binary_accuracy'] ) # add stopping criteria early_stopping = keras.callbacks.EarlyStopping( patience = 10, min_delta = 0.001, restore_best_weights = True, ) # model fitting history = model.fit( X_train, y_train, validation_data=(X_valid, y_valid), batch_size=512, epochs=1000, callbacks = [early_stopping], verbose = 0 # hide output since too many to display ) # model levaluation history_df = pd.DataFrame(history.history) # starts at epoch 5 history_df.loc[5:, ['loss', 'val_loss']].plot() history_df.loc[5:, ['binary_accuracy', 'val_binary_accuracy']].plot() print((\"Best Validation Loss: {:0.4f}\" +\\ \"\\nBest Validation Accuracy: {:0.4f}\")\\ .format(history_df['val_loss'].min(), history_df['val_binary_accuracy'].max())) 6. Reference Learn Intro to Deep Learning Tutorials - Kaggle ","wordCount":"1366","inLanguage":"vi","datePublished":"2023-02-24T00:00:00Z","dateModified":"2023-02-24T00:00:00Z","author":{"@type":"Person","name":"Kean Teng Blog"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/2023/2023-02-24-intro-to-deep-learning/"},"publisher":{"@type":"Organization","name":"The Financial Engineer","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="The Financial Engineer (Alt + H)">The Financial Engineer</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=About><span>About</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Intro to Deep Learning</h1><div class=post-description>My Kaggle Learning Note 1</div><div class=post-meta><span title='2023-02-24 00:00:00 +0000 UTC'>tháng 2 24, 2023</span>&nbsp;·&nbsp;7 phút&nbsp;·&nbsp;Kean Teng Blog</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Mục lục</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#0-concepts>0. Concepts</a></li><li><a href=#1-modelling--neural-network>1. Modelling & Neural Network</a></li><li><a href=#2-model-optimization--fitting>2. Model Optimization & Fitting</a></li><li><a href=#3-model-evaluation-1>3. Model Evaluation 1</a></li><li><a href=#4-model-evaluation-2>4. Model Evaluation 2</a></li><li><a href=#5-binary-classification>5. Binary Classification</a></li><li><a href=#6-reference>6. Reference</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p><em>Disclaimer: This article is my learning note from the courses I took from Kaggle.</em></p></blockquote><h2 id=0-concepts>0. Concepts<a hidden class=anchor aria-hidden=true href=#0-concepts>#</a></h2><p>Creation and evaluation of a deep learning model is a procedural work with steps. <em>Data preparation &#187; Model optimization&#187; Model fitting &#187; Model evaluation &#187; Model prediction</em></p><p><strong>Note:</strong></p><ul><li>Model optimization (adam optimizer)</li><li>Model fitting (batch, epoch)</li><li>Model evaluation(dropout, batch normalization)</li></ul><h2 id=1-modelling--neural-network>1. Modelling & Neural Network<a hidden class=anchor aria-hidden=true href=#1-modelling--neural-network>#</a></h2><p><em>Terms: layer, activation function, neural network, dense, rectified linear unit</em></p><p>The above neural network can be generated with code using Python. The network above organized neurons into layers. Collecting linear units with common set of inputs result in a dense layer.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras <span style=color:#f92672>import</span> layers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 1 linear unit network</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(units <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>, input_shape <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(units <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>, input_shape <span style=color:#f92672>=</span> [<span style=color:#ae81ff>3</span>])
</span></span><span style=display:flex><span>])
</span></span></code></pre></td></tr></table></div></div><p>If we want to fit a curve (non-linear), a feature called activation function is needed, otherwise the neural network can only learn linear relationships. A common example is the rectifier function <code>max(0,x)</code>, where we get a rectified linear unit when we attach the function to a linear unit.</p><p>We can also stack layers to achieve a complex data transformation.</p><p>Using code to define the network:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">9
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># [layer1, layer2, layer3,...]</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(units <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>, activation <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;relu&#39;</span>, input_shape <span style=color:#f92672>=</span> [<span style=color:#ae81ff>2</span>],
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(units <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>, activation <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(units <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>])
</span></span></code></pre></td></tr></table></div></div><h2 id=2-model-optimization--fitting>2. Model Optimization & Fitting<a hidden class=anchor aria-hidden=true href=#2-model-optimization--fitting>#</a></h2><p><em>Terms: loss function, stochastic gradient descent, batch, epoch, error/loss</em></p><p>Similar to regression, we need to know how well our model fits the data. We use <strong>loss function</strong> to measures the difference between observed and predicted values. The common measure would be the mean absolute error where it computed the average length between the fitted curve and data points. Other errors examples are mean-squared error or Huber loss.</p><p>How can we minimize the error? We can use optimization algorithms — the stochastic gradient descent. The concepts as follows:</p><ul><li>Get random sample (<code>batch</code>) from original dataset and run through the network for prediction.</li><li>Measure the error (<code>loss</code>)</li><li>Adjust the weight in a direction that makes the loss smaller (an epoch for each complete round a training)</li></ul><p>We can use a built-in “<code>adam</code>” optimizer to optimize our model. This allows self-tuning to minimize loss. Below is code to fit 256 rows of training data for 10 times:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#75715e># get training data dimension (row, column)</span>
</span></span><span style=display:flex><span>print(X_train<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># define model</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras <span style=color:#f92672>import</span> layers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mode <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>512</span>, activation <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;relu&#39;</span>, input_shape <span style=color:#f92672>=</span> [<span style=color:#ae81ff>11</span>]),
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>512</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>512</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># add in optimizer and loss function</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>compile(
</span></span><span style=display:flex><span>  optimizer <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;adam&#39;</span>,
</span></span><span style=display:flex><span>  loss <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;mae&#39;</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># model fitting</span>
</span></span><span style=display:flex><span>history <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(
</span></span><span style=display:flex><span>  X_train, y_train,
</span></span><span style=display:flex><span>  validation_data <span style=color:#f92672>=</span> (X_valid, y_valid),
</span></span><span style=display:flex><span>  batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>256</span>,
</span></span><span style=display:flex><span>  epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># visualize outcome</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#75715e># training history to data frame</span>
</span></span><span style=display:flex><span>history_df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(history<span style=color:#f92672>.</span>history) 
</span></span><span style=display:flex><span><span style=color:#75715e># plot the loss</span>
</span></span><span style=display:flex><span>history_df[<span style=color:#e6db74>&#39;loss&#39;</span>]<span style=color:#f92672>.</span>plot(); 
</span></span></code></pre></td></tr></table></div></div><h2 id=3-model-evaluation-1>3. Model Evaluation 1<a hidden class=anchor aria-hidden=true href=#3-model-evaluation-1>#</a></h2><p><em>Terms: underfitting, overfitting, capacity, training, callback, stopping criteria</em></p><p>Training data normally contains signal and noise where signal helps our model make prediction from new data and noise represent the random fluctuation coming from data in the real world. To see if our model fits the data well, we will compare learning curve between training set and validation set.</p><p><strong>Underfitting</strong> the training set is when the loss is not as low as it could be because the model hasn’t learned enough signal. <strong>Overfitting</strong> the training set is when the loss is not as low as it could be because the model learned too much noise.</p><p>A model’s capacity is the size and complexity of the patterns it is able to learn. We can adjust a model’s capacity if we detect overfitting or underfitting scenarios.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#75715e># sample model</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>16</span>, activation <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># wider </span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>32</span>, activation <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># deeper</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>16</span>, activation <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>16</span>, activation <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>])
</span></span></code></pre></td></tr></table></div></div><p>Sometimes the validation increase during training after a certain point although it kept decreasing early on. This can due to a model is learning noise from the datasets. We can overcome this issue by imposing an early stopping criterion.</p><p>A callback function is used where we detect when the validation loss starts to rise again, and we reset the weights back the where the minimum occurred. An example “if there is not at least 0.001 improvement in the validation loss over 20 epochs, then stop training and keep the best model you found”.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">7
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.callbacks <span style=color:#f92672>import</span> EarlyStopping
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>early_stopping <span style=color:#f92672>=</span> EarlyStopping(
</span></span><span style=display:flex><span>  min_delta <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.</span> <span style=color:#ae81ff>001</span>, <span style=color:#75715e># min change to count as improvement</span>
</span></span><span style=display:flex><span>  patience <span style=color:#f92672>=</span> <span style=color:#ae81ff>20</span>, <span style=color:#75715e># how many epochs to wait before stopping</span>
</span></span><span style=display:flex><span>  restore_best_weights <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>)
</span></span></code></pre></td></tr></table></div></div><h2 id=4-model-evaluation-2>4. Model Evaluation 2<a hidden class=anchor aria-hidden=true href=#4-model-evaluation-2>#</a></h2><p><em>Terms: dropout, batch normalization</em></p><p>To correct overfitting in our model, we can implement the idea of dropout where we randomly drop out some fraction of a layer’s input units every step of training. This avoids the network to learn spurious patterns in the training data which leads to overfitting.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>  <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dropout(rate <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.3</span>) <span style=color:#75715e># 30% dropout to the next layer</span>
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>16</span>),  
</span></span><span style=display:flex><span>  <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>])
</span></span></code></pre></td></tr></table></div></div><p>To correct slow or unstable training, we can apply batch normalization to put all data on a common scale. SGD will shift the network weights in proportion to how large an activation the data produces. Features that tend to produce activations of very different sizes can make for unstable training behavior.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>16</span>, activation <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;relu&#39;</span>),  
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>BatchNormalization(),
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># or</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>16</span>),  
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>BatchNormalization(),
</span></span><span style=display:flex><span>  layers<span style=color:#f92672>.</span>Activation(<span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>])
</span></span></code></pre></td></tr></table></div></div><p>Example for a full model fitting process:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras <span style=color:#f92672>import</span> layers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># define network</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1024</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>, input_shape<span style=color:#f92672>=</span>[<span style=color:#ae81ff>11</span>]),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.3</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>BatchNormalization(),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1024</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.3</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>BatchNormalization(),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1024</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dropout(<span style=color:#ae81ff>0.3</span>),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>BatchNormalization(),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># add optimier</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>compile(
</span></span><span style=display:flex><span>  optimizer <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;adam&#39;</span>,
</span></span><span style=display:flex><span>  loss <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;mae&#39;</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># model fitting</span>
</span></span><span style=display:flex><span>history <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(
</span></span><span style=display:flex><span>  X_train, y_train,
</span></span><span style=display:flex><span>  validation_data <span style=color:#f92672>=</span> (X_valid, y_valid),
</span></span><span style=display:flex><span>  batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>256</span>,
</span></span><span style=display:flex><span>  epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>,
</span></span><span style=display:flex><span>  verbose <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># visualize</span>
</span></span><span style=display:flex><span>history_df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(history<span style=color:#f92672>.</span>history)
</span></span><span style=display:flex><span>history_df<span style=color:#f92672>.</span>loc[:,[<span style=color:#e6db74>&#39;loss&#39;</span>,<span style=color:#e6db74>&#39;val_loss&#39;</span>]]<span style=color:#f92672>.</span>plot();
</span></span></code></pre></td></tr></table></div></div><h2 id=5-binary-classification>5. Binary Classification<a hidden class=anchor aria-hidden=true href=#5-binary-classification>#</a></h2><p><em>Terms: sigmoid, binary, accuracy</em></p><p>We cannot use accuracy to measure model performance as it does not change smoothly — it changes in jumps as it represents ratio counts. Thus, a replacement would be the cross-entropy function where it measures the distance between probabilities. To convert outputs from dense layer into probabilities, we will use the sigmoid activation function.</p><p><strong>Code example:</strong></p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">37
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">38
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">39
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">40
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">41
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">42
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">43
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">44
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">45
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">46
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras <span style=color:#f92672>import</span> layers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># define network</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>4</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>, input_shape<span style=color:#f92672>=</span>[<span style=color:#ae81ff>33</span>]),
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>4</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),    
</span></span><span style=display:flex><span> <span style=color:#75715e># final layer need sigmoid function to produce class probabilities</span>
</span></span><span style=display:flex><span>    layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;sigmoid&#39;</span>),
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># add optimizer</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>compile(
</span></span><span style=display:flex><span>  optimizer <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;adam&#39;</span>,
</span></span><span style=display:flex><span>  loss <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;binary_crossentropy&#39;</span>,
</span></span><span style=display:flex><span>  metrics <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;binary_accuracy&#39;</span>]
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># add stopping criteria</span>
</span></span><span style=display:flex><span>early_stopping <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>callbacks<span style=color:#f92672>.</span>EarlyStopping(
</span></span><span style=display:flex><span>  patience <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>,
</span></span><span style=display:flex><span>  min_delta <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.001</span>,
</span></span><span style=display:flex><span>  restore_best_weights <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#  model fitting</span>
</span></span><span style=display:flex><span>history <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(
</span></span><span style=display:flex><span>  X_train, y_train,
</span></span><span style=display:flex><span>  validation_data<span style=color:#f92672>=</span>(X_valid, y_valid),
</span></span><span style=display:flex><span>  batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>,
</span></span><span style=display:flex><span>  epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>,
</span></span><span style=display:flex><span>  callbacks <span style=color:#f92672>=</span> [early_stopping],
</span></span><span style=display:flex><span>  verbose <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> <span style=color:#75715e># hide output since too many to display</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># model levaluation</span>
</span></span><span style=display:flex><span>history_df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(history<span style=color:#f92672>.</span>history)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># starts at epoch 5</span>
</span></span><span style=display:flex><span>history_df<span style=color:#f92672>.</span>loc[<span style=color:#ae81ff>5</span>:, [<span style=color:#e6db74>&#39;loss&#39;</span>, <span style=color:#e6db74>&#39;val_loss&#39;</span>]]<span style=color:#f92672>.</span>plot()
</span></span><span style=display:flex><span>history_df<span style=color:#f92672>.</span>loc[<span style=color:#ae81ff>5</span>:, [<span style=color:#e6db74>&#39;binary_accuracy&#39;</span>, <span style=color:#e6db74>&#39;val_binary_accuracy&#39;</span>]]<span style=color:#f92672>.</span>plot()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print((<span style=color:#e6db74>&#34;Best Validation Loss: </span><span style=color:#e6db74>{:0.4f}</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>+</span>\
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Best Validation Accuracy: </span><span style=color:#e6db74>{:0.4f}</span><span style=color:#e6db74>&#34;</span>)\
</span></span><span style=display:flex><span>      <span style=color:#f92672>.</span>format(history_df[<span style=color:#e6db74>&#39;val_loss&#39;</span>]<span style=color:#f92672>.</span>min(), 
</span></span><span style=display:flex><span>              history_df[<span style=color:#e6db74>&#39;val_binary_accuracy&#39;</span>]<span style=color:#f92672>.</span>max()))
</span></span></code></pre></td></tr></table></div></div><h2 id=6-reference>6. Reference<a hidden class=anchor aria-hidden=true href=#6-reference>#</a></h2><ul><li><a href=https://www.kaggle.com/learn/intro-to-deep-learning>Learn Intro to Deep Learning Tutorials - Kaggle</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/python/>Python</a></li><li><a href=http://localhost:1313/tags/deep-learning/>Deep Learning</a></li><li><a href=http://localhost:1313/tags/machine-learning/>Machine Learning</a></li><li><a href=http://localhost:1313/tags/kaggle/>Kaggle</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/2023/2023-02-25-stable-diffusion-webui-with-civitai-loras/><span class=title>« Bài mới hơn</span><br><span>Stable Diffusion WebUI with Civitai LORAs</span>
</a><a class=next href=http://localhost:1313/posts/2023/2023-02-19-images-scraping-from-web-pages/><span class=title>Bài cũ hơn »</span><br><span>Images Scraping from Web Pages</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Intro to Deep Learning on x" href="https://x.com/intent/tweet/?text=Intro%20to%20Deep%20Learning&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-02-24-intro-to-deep-learning%2f&amp;hashtags=Python%2cDeepLearning%2cMachineLearning%2cKaggle"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Intro to Deep Learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-02-24-intro-to-deep-learning%2f&amp;title=Intro%20to%20Deep%20Learning&amp;summary=Intro%20to%20Deep%20Learning&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-02-24-intro-to-deep-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Intro to Deep Learning on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-02-24-intro-to-deep-learning%2f&title=Intro%20to%20Deep%20Learning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Intro to Deep Learning on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-02-24-intro-to-deep-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Intro to Deep Learning on whatsapp" href="https://api.whatsapp.com/send?text=Intro%20to%20Deep%20Learning%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-02-24-intro-to-deep-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Intro to Deep Learning on telegram" href="https://telegram.me/share/url?text=Intro%20to%20Deep%20Learning&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-02-24-intro-to-deep-learning%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Intro to Deep Learning on ycombinator" href="https://news.ycombinator.com/submitlink?t=Intro%20to%20Deep%20Learning&u=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-02-24-intro-to-deep-learning%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>The Financial Engineer</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="Sao chép";function s(){t.innerHTML="Đã sao chép!",setTimeout(()=>{t.innerHTML="Sao chép"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>