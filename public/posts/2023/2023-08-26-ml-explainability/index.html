<!doctype html><html lang=vi dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Machine Learning Explainability | The Financial Engineer</title>
<meta name=keywords content='Python,“Machine Learning",SHAP'><meta name=description content="My Kaggle Learning Note "><meta name=author content="Kean Teng Blog"><link rel=canonical href=http://localhost:1313/posts/2023/2023-08-26-ml-explainability/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=vi href=http://localhost:1313/posts/2023/2023-08-26-ml-explainability/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Machine Learning Explainability"><meta property="og:description" content="My Kaggle Learning Note "><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/2023/2023-08-26-ml-explainability/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-26T00:00:00+00:00"><meta property="article:modified_time" content="2023-08-26T00:00:00+00:00"><meta property="og:site_name" content="QHung's Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Machine Learning Explainability"><meta name=twitter:description content="My Kaggle Learning Note "><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Machine Learning Explainability","item":"http://localhost:1313/posts/2023/2023-08-26-ml-explainability/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Machine Learning Explainability","name":"Machine Learning Explainability","description":"My Kaggle Learning Note ","keywords":["Python","“Machine Learning\"","SHAP"],"articleBody":" Disclaimer: This article is my learning note from the courses I took from Kaggle.\nMany people tend to say that machine learning models are “black boxes” because they can make good predictions but cannot understand the logic behind those predictions.\nIn this course, we will learn on methods to extract insights from model:\nWhat features in the data the model think as the most important? How each feature affect the prediction? So, why do we need model insights? Model insights can be useful in a couple of ways:\nDebugging\nGiven the frequency and potentially disastrous consequences of bugs, debugging is one of the most valuable skills in data science. Understanding the patterns a model is finding will help you identify when those are at odds with your knowledge of the real world, and this is typically the first step in tracking down bugs.\nInforming Feature Engineering\nFeature engineering is usually the most effective way to improve model accuracy. Feature engineering usually involves repeatedly creating new features using transformations of your raw data or features you have previously created.\nSometimes you can go through this process using nothing but intuition about the underlying topic. But you’ll need more direction when you have 100s of raw features or when you lack background knowledge about the topic you are working on.\nDirecting Future Data Collection\nYou have no control over datasets you download online. But many businesses and organizations using data science have opportunities to expand what types of data they collect. Collecting new types of data can be expensive or inconvenient, so they only want to do this if they know it will be worthwhile. Model-based insights give you a good understanding of the value of features you currently have, which will help you reason about what new values may be most helpful.\nInforming Human Decision-Making\nSome decisions are made automatically by models. Amazon doesn’t have humans (or elves) scurry to decide what to show you whenever you go to their website. But many important decisions are made by humans. For these decisions, insights can be more valuable than predictions.\nBuilding Trust\nMany people won’t assume they can trust your model for important decisions without verifying some basic facts. This is a smart precaution given the frequency of data errors. In practice, showing insights that fit their general understanding of the problem will help build trust, even among people with little deep knowledge of data science.\n1. Permutation Importance Permutation importance is calculated after a model has been fitted. Imagine that now, we want to predict a person’s height when they become 20 years old using only data that is available at age 10.\nNow, if we randomly shuffle a single column of the validation data but all the other columns remain in place, how would the accuracy of the prediction be affected?\nOf course, such an approach would reduce the model accuracy, since the data no longer corresponds to what we can observe in the real world. Model accuracy especially suffers if we shuffle a column that the model relied on heavily for predictions. In this case, shuffling height at age 10 would cause terrible predictions. If we shuffled socks owned instead, the resulting predictions wouldn’t suffer nearly as much.\nSo, here is what we can do:\nGet a trained model Shuffle values in a single column and make prediction from it. Calculate the difference of prediction and target value with a loss function. The performance deterioration is the importance of the variable that we shuffled Return to step 2 until all the importance for each column is calculated 1.1 Example Here is how to calculate the importance with eli5 library:\n1 2 3 4 5 import eli5 from eli5.sklearn import PermutationImportance perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y) eli5.show_weights(perm, feature_names = val_X.columns.tolist()) From the above figure, the values towards the top is the most important features. The first number in each row shows how much the model performance decreased with a random shuffling. There is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the ± measures how performance varied from one-reshuffling to the next.\nYou’ll occasionally see negative values for permutation importance. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn’t matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck/chance.\n2. Partial Plots Similar to permutation importance, partial dependence plots are calculated after a model has been fit. To see how partial plots separate out the effect of each feature, we start by considering a single row of data. For example, that row of data might represent a team that had the ball 50% of the time, made 100 passes, took 10 shots and scored 1 goal.\nWe will use the fitted model to predict our outcome (probability their player won “man of the match”). But we repeatedly alter the value for one variable to make a series of predictions. We could predict the outcome if the team had the ball only 40% of the time. We then predict with them having the ball 50% of the time. Then predict again for 60%. And so on. We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis).\n2.1 Example Let’s get a decision tree from the model:\n1 2 3 4 5 from sklearn import tree import graphviz tree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=feature_names) graphviz.Source(tree_graph) Produce a partial dependence plot:\n1 2 3 4 5 6 from matplotlib import pyplot as plt from sklearn.inspection import PartialDependenceDisplay # Create and plot the data disp1 = PartialDependenceDisplay.from_estimator(tree_model, val_X, ['Goal Scored']) plt.show() The y-axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\nFrom this particular graph, we see that scoring a goal substantially increases your chances of winning “Man of The Match.” But extra goals beyond that appear to have little impact on predictions.\n1 2 3 feature_to_plot = 'Distance Covered (Kms)' disp2 = PartialDependenceDisplay.from_estimator(tree_model, val_X, [feature_to_plot]) plt.show() This graph seems too simple to represent reality. But that’s because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model’s structure.\n1 2 3 4 5 # Build Random Forest model rf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) disp3 = PartialDependenceDisplay.from_estimator(rf_model, val_X, [feature_to_plot]) plt.show() This model thinks you are more likely to win Man of the Match if your players run a total of 100km over the course of the game. Though running much more causes lower predictions.\nIn general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model. Though this dataset is small enough that we would be careful in how we interpret any model.\n2.2 2D Partial Dependence Plots We will use the same datasets as of above:\n1 2 3 4 5 fig, ax = plt.subplots(figsize=(8, 6)) f_names = [('Goal Scored', 'Distance Covered (Kms)')] # Similar to previous PDP plot except we use tuple of features instead of single feature disp4 = PartialDependenceDisplay.from_estimator(tree_model, val_X, f_names, ax=ax) plt.show() From the plot above, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km. If they score 0 goals, distance covered doesn’t matter. Can you see this by tracing through the decision tree with 0 goals?\nBut distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot.\n3. SHAP Value SHAP or SHapley Additive exPlanations is used to break down a prediction t0 show the impact of each feature. It interprets the impact of having a certain value for a given feature in comparison to the prediction we would make if that feature took some baseline value.\nFor example, consider the Man of the Match award example for previous section, we could ask questions like how much prediction driven by the fact that the team scored 3 goals?\nBut for each team, they are many features, so if we answer for the number of goals, we could repeat the process for other features too. SHAP values of all features sum up to explain why my prediction was different from the baseline.\nsum(SHAP values for all features) = pred_for_team - pred_for_baseline_values To interpret the graph:\nWe predicted 0.7, whereas the base_value is 0.4979. Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature’s effect. Feature values decreasing the prediction are in blue. The biggest impact comes from Goal Scored being 2. Though the ball possession value has a meaningful effect decreasing the prediction.\nIf you subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output.\nHow to Do That in Code Let’s get the model ready:\n1 2 3 4 5 6 7 8 9 10 11 import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier data = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv') y = (data['Man of the Match'] == \"Yes\") # Convert from string \"Yes\"/\"No\" to binary feature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]] X = data[feature_names] train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1) my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) We will look for SHAP value for a single row of the dataset. Let’s check on the raw prediction first:\n1 2 3 4 5 6 row_to_show = 5 data_for_prediction = val_X.iloc[row_to_show] # use 1 row of data here. Could use multiple rows if desired data_for_prediction_array = data_for_prediction.values.reshape(1, -1) my_model.predict_proba(data_for_prediction_array) The output is array([[0.29, 0.71]]), the team is 70% likely to have a player win the award\n1 2 3 4 5 6 7 8 9 10 import shap # package used to calculate Shap values # Create object that can calculate shap values explainer = shap.TreeExplainer(my_model) # Calculate Shap values shap_values = explainer.shap_values(data_for_prediction) shap.initjs() shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction) The shap_values object above is a list with two arrays. The first array is the SHAP values for a negative outcome (don’t win the award), and the second array is the list of SHAP values for the positive outcome (wins the award). We typically think about predictions in terms of the prediction of a positive outcome, so we’ll pull out SHAP values for positive outcomes (pulling out shap_values[1]).\nOf course, SHAP package also has explainers for every type of model:\nshap.DeepExplainer works with Deep Learning models. shap.KernelExplainer works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values. 4. Advanced Uses of SHAP Value Shap values show how much a given feature changed our prediction (compared to if we made that prediction at some baseline value of that feature).\nConsider the equation:\ny = 4 * x1 + 2 * x2 If x1 takes the value 2, instead of a baseline value of 0, then our SHAP value for x1 would be 8 (from 4 times 2).\nThese are harder to calculate with the sophisticated models we use in practice. But through some algorithmic cleverness, Shap values allow us to decompose any prediction into the sum of effects of each feature value, yielding a graph like this:\nIn addition to this nice breakdown for each prediction, the Shap library offers great visualizations of groups of Shap values. We will focus on two of these visualizations. These visualizations have conceptual similarities to permutation importance and partial dependence plots\nSHAP summary plots give us a birds-eye view of feature importance and what is driving it. We’ll walk through an example plot for the soccer data:\nThis plot is made of many dots. Each dot has three characteristics:\nVertical location shows what feature it is depicting Color shows whether that feature was high or low for that row of the dataset Horizontal location shows whether the effect of that value caused a higher or lower prediction. Some things you should be able to easily pick out:\nThe model ignored the Red and Yellow \u0026 Red features. Usually Yellow Card doesn’t affect the prediction, but there is an extreme case where a high value caused a much lower prediction. High values of Goal scored caused higher predictions, and low values caused low predictions How to Do That in Code Get the data and model ready:\n1 2 3 4 5 6 7 8 9 10 11 import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier data = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv') y = (data['Man of the Match'] == \"Yes\") # Convert from string \"Yes\"/\"No\" to binary feature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]] X = data[feature_names] train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1) my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y) Let’s get a SHAP summary plot:\n1 2 3 4 5 6 7 8 9 10 11 import shap # package used to calculate Shap values # Create object that can calculate shap values explainer = shap.TreeExplainer(my_model) # calculate shap values. This is what we will plot. # Calculate shap_values for all of val_X rather than a single row, to have more data for plot. shap_values = explainer.shap_values(val_X) # Make plot. Index of [1] is explained in text below. shap.summary_plot(shap_values[1], val_X) The code isn’t too complex. But there are a few caveats.\nWhen plotting, we call shap_values[1]. For classification problems, there is a separate array of SHAP values for each possible outcome. In this case, we index in to get the SHAP values for the prediction of “True”. Calculating SHAP values can be slow. It isn’t a problem here, because this dataset is small. But you’ll want to be careful when running these to plot with reasonably sized datasets. The exception is when using an xgboost model, which SHAP has some optimizations for and which is thus much faster. SHAP Dependence Contributions Plots For SHAP dependence contribution plots provide a similar insight to partial dependence plot’s, but they add a lot more detail.\nEach dot represents a row of the data. The horizontal location is the actual value from the dataset, and the vertical location shows what having that value did to the prediction. The fact this slopes upward says that the more you possess the ball, the higher the model’s prediction is for winning the Man of the Match award.\nThe spread suggests that other features must interact with Ball Possession %. For example, here we have highlighted two points with similar ball possession values. That value caused one prediction to increase, and it caused the other prediction to decrease.\nFor comparison, a simple linear regression would produce plots that are perfect lines, without this spread.\nThis suggests we delve into the interactions, and the plots include color coding to help do that. While the primary trend is upward, you can visually inspect whether that varies by dot color.\nThese two points stand out spatially as being far away from the upward trend. They are both colored purple, indicating the team scored one goal. You can interpret this to say In general, having the ball increases a team’s chance of having their player win the award. But if they only score one goal, that trend reverses and the award judges may penalize them for having the ball so much if they score that little.\nHow to Do That in Code 1 2 3 4 5 6 7 8 9 10 import shap # package used to calculate Shap values # Create object that can calculate shap values explainer = shap.TreeExplainer(my_model) # calculate shap values. This is what we will plot. shap_values = explainer.shap_values(X) # make plot. shap.dependence_plot('Ball Possession %', shap_values[1], X, interaction_index=\"Goal Scored\") If you don’t supply an argument for interaction_index, Shapley uses some logic to pick one that may be interesting!\n","wordCount":"2712","inLanguage":"vi","datePublished":"2023-08-26T00:00:00Z","dateModified":"2023-08-26T00:00:00Z","author":{"@type":"Person","name":"Kean Teng Blog"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/2023/2023-08-26-ml-explainability/"},"publisher":{"@type":"Organization","name":"The Financial Engineer","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="The Financial Engineer (Alt + H)">The Financial Engineer</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=About><span>About</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Machine Learning Explainability</h1><div class=post-description>My Kaggle Learning Note</div><div class=post-meta><span title='2023-08-26 00:00:00 +0000 UTC'>tháng 8 26, 2023</span>&nbsp;·&nbsp;13 phút&nbsp;·&nbsp;Kean Teng Blog</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Mục lục</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-permutation-importance>1. Permutation Importance</a><ul><li><a href=#11-example>1.1 Example</a></li></ul></li><li><a href=#2-partial-plots>2. Partial Plots</a><ul><li><a href=#21-example>2.1 Example</a></li><li><a href=#22-2d-partial-dependence-plots>2.2 2D Partial Dependence Plots</a></li></ul></li><li><a href=#3-shap-value>3. SHAP Value</a><ul><li><a href=#how-to-do-that-in-code>How to Do That in Code</a></li></ul></li><li><a href=#4-advanced-uses-of-shap-value>4. Advanced Uses of SHAP Value</a><ul><li><a href=#how-to-do-that-in-code-1>How to Do That in Code</a></li><li><a href=#shap-dependence-contributions-plots>SHAP Dependence Contributions Plots</a></li><li><a href=#how-to-do-that-in-code-2>How to Do That in Code</a></li></ul></li></ul></nav></div></details></div><div class=post-content><blockquote><p><em>Disclaimer: This article is my learning note from the courses I took from Kaggle.</em></p></blockquote><p>Many people tend to say that machine learning models are &ldquo;black boxes&rdquo; because they can make good predictions but cannot understand the logic behind those predictions.</p><p>In this course, we will learn on methods to extract insights from model:</p><ul><li>What features in the data the model think as the most important?</li><li>How each feature affect the prediction?</li></ul><p>So, why do we need model insights? Model insights can be useful in a couple of ways:</p><p><strong>Debugging</strong></p><p>Given the frequency and potentially disastrous consequences of bugs, debugging is one of the most valuable skills in data science. Understanding the patterns a model is finding will help you identify when those are at odds with your knowledge of the real world, and this is typically the first step in tracking down bugs.</p><p><strong>Informing Feature Engineering</strong></p><p>Feature engineering is usually the most effective way to improve model accuracy. Feature engineering usually involves repeatedly creating new features using transformations of your raw data or features you have previously created.</p><p>Sometimes you can go through this process using nothing but intuition about the underlying topic. But you&rsquo;ll need more direction when you have 100s of raw features or when you lack background knowledge about the topic you are working on.</p><p><strong>Directing Future Data Collection</strong></p><p>You have no control over datasets you download online. But many businesses and organizations using data science have opportunities to expand what types of data they collect. Collecting new types of data can be expensive or inconvenient, so they only want to do this if they know it will be worthwhile. Model-based insights give you a good understanding of the value of features you currently have, which will help you reason about what new values may be most helpful.</p><p><strong>Informing Human Decision-Making</strong></p><p>Some decisions are made automatically by models. Amazon doesn&rsquo;t have humans (or elves) scurry to decide what to show you whenever you go to their website. But many important decisions are made by humans. For these decisions, insights can be more valuable than predictions.</p><p><strong>Building Trust</strong></p><p>Many people won&rsquo;t assume they can trust your model for important decisions without verifying some basic facts. This is a smart precaution given the frequency of data errors. In practice, showing insights that fit their general understanding of the problem will help build trust, even among people with little deep knowledge of data science.</p><h2 id=1-permutation-importance>1. Permutation Importance<a hidden class=anchor aria-hidden=true href=#1-permutation-importance>#</a></h2><p>Permutation importance is calculated after a model has been fitted. Imagine that now, we want to predict a person&rsquo;s height when they become 20 years old using only data that is available at age 10.</p><p>Now, if we randomly shuffle a single column of the validation data but all the other columns remain in place, how would the accuracy of the prediction be affected?</p><p>Of course, such an approach would reduce the model accuracy, since the data no longer corresponds to what we can observe in the real world. Model accuracy especially suffers if we shuffle a column that the model relied on heavily for predictions. In this case, shuffling height at age 10 would cause terrible predictions. If we shuffled socks owned instead, the resulting predictions wouldn&rsquo;t suffer nearly as much.</p><p>So, here is what we can do:</p><ul><li>Get a trained model</li><li>Shuffle values in a single column and make prediction from it. Calculate the difference of prediction and target value with a loss function. The performance deterioration is the importance of the variable that we shuffled</li><li>Return to step 2 until all the importance for each column is calculated</li></ul><h3 id=11-example>1.1 Example<a hidden class=anchor aria-hidden=true href=#11-example>#</a></h3><p>Here is how to calculate the importance with <code>eli5</code> library:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>import</span> eli5
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> eli5.sklearn <span style=color:#f92672>import</span> PermutationImportance
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>perm <span style=color:#f92672>=</span> PermutationImportance(my_model, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>fit(val_X, val_y)
</span></span><span style=display:flex><span>eli5<span style=color:#f92672>.</span>show_weights(perm, feature_names <span style=color:#f92672>=</span> val_X<span style=color:#f92672>.</span>columns<span style=color:#f92672>.</span>tolist())
</span></span></code></pre></td></tr></table></div></div><p>From the above figure, the values towards the top is the most important features. The first number in each row shows how much the model performance decreased with a random shuffling. There is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the ± measures how performance varied from one-reshuffling to the next.</p><p>You&rsquo;ll occasionally see negative values for permutation importance. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn&rsquo;t matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck/chance.</p><h2 id=2-partial-plots>2. Partial Plots<a hidden class=anchor aria-hidden=true href=#2-partial-plots>#</a></h2><p>Similar to permutation importance, partial dependence plots are calculated after a model has been fit. To see how partial plots separate out the effect of each feature, we start by considering a single row of data. For example, that row of data might represent a team that had the ball 50% of the time, made 100 passes, took 10 shots and scored 1 goal.</p><p>We will use the fitted model to predict our outcome (probability their player won &ldquo;man of the match&rdquo;). But we repeatedly alter the value for one variable to make a series of predictions. We could predict the outcome if the team had the ball only 40% of the time. We then predict with them having the ball 50% of the time. Then predict again for 60%. And so on. We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis).</p><h3 id=21-example>2.1 Example<a hidden class=anchor aria-hidden=true href=#21-example>#</a></h3><p>Let&rsquo;s get a decision tree from the model:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>from</span> sklearn <span style=color:#f92672>import</span> tree
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> graphviz
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tree_graph <span style=color:#f92672>=</span> tree<span style=color:#f92672>.</span>export_graphviz(tree_model, out_file<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, feature_names<span style=color:#f92672>=</span>feature_names)
</span></span><span style=display:flex><span>graphviz<span style=color:#f92672>.</span>Source(tree_graph)
</span></span></code></pre></td></tr></table></div></div><p>Produce a partial dependence plot:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>from</span> matplotlib <span style=color:#f92672>import</span> pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.inspection <span style=color:#f92672>import</span> PartialDependenceDisplay
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create and plot the data</span>
</span></span><span style=display:flex><span>disp1 <span style=color:#f92672>=</span> PartialDependenceDisplay<span style=color:#f92672>.</span>from_estimator(tree_model, val_X, [<span style=color:#e6db74>&#39;Goal Scored&#39;</span>])
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></td></tr></table></div></div><p>The y-axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.</p><p>From this particular graph, we see that scoring a goal substantially increases your chances of winning &ldquo;Man of The Match.&rdquo; But extra goals beyond that appear to have little impact on predictions.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>feature_to_plot <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;Distance Covered (Kms)&#39;</span>
</span></span><span style=display:flex><span>disp2 <span style=color:#f92672>=</span> PartialDependenceDisplay<span style=color:#f92672>.</span>from_estimator(tree_model, val_X, [feature_to_plot])
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></td></tr></table></div></div><p>This graph seems too simple to represent reality. But that&rsquo;s because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model&rsquo;s structure.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#75715e># Build Random Forest model</span>
</span></span><span style=display:flex><span>rf_model <span style=color:#f92672>=</span> RandomForestClassifier(random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>fit(train_X, train_y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>disp3 <span style=color:#f92672>=</span> PartialDependenceDisplay<span style=color:#f92672>.</span>from_estimator(rf_model, val_X, [feature_to_plot])
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></td></tr></table></div></div><blockquote><p>This model thinks you are more likely to win Man of the Match if your players run a total of 100km over the course of the game. Though running much more causes lower predictions.</p></blockquote><p>In general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model. Though this dataset is small enough that we would be careful in how we interpret any model.</p><h3 id=22-2d-partial-dependence-plots>2.2 2D Partial Dependence Plots<a hidden class=anchor aria-hidden=true href=#22-2d-partial-dependence-plots>#</a></h3><p>We will use the same datasets as of above:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>fig, ax <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>f_names <span style=color:#f92672>=</span> [(<span style=color:#e6db74>&#39;Goal Scored&#39;</span>, <span style=color:#e6db74>&#39;Distance Covered (Kms)&#39;</span>)]
</span></span><span style=display:flex><span><span style=color:#75715e># Similar to previous PDP plot except we use tuple of features instead of single feature</span>
</span></span><span style=display:flex><span>disp4 <span style=color:#f92672>=</span> PartialDependenceDisplay<span style=color:#f92672>.</span>from_estimator(tree_model, val_X, f_names, ax<span style=color:#f92672>=</span>ax)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></td></tr></table></div></div><p>From the plot above, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km. If they score 0 goals, distance covered doesn&rsquo;t matter. Can you see this by tracing through the decision tree with 0 goals?</p><p>But distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot.</p><h2 id=3-shap-value>3. SHAP Value<a hidden class=anchor aria-hidden=true href=#3-shap-value>#</a></h2><p>SHAP or <code>SHapley Additive exPlanations</code> is used to break down a prediction t0 show the impact of each feature. It interprets the impact of having a certain value for a given feature in comparison to the prediction we would make if that feature took some baseline value.</p><p>For example, consider the Man of the Match award example for previous section, we could ask questions like how much prediction driven by the fact that the team scored 3 goals?</p><p>But for each team, they are many features, so if we answer for the <code>number of goals</code>, we could repeat the process for other features too. SHAP values of all features sum up to explain why my prediction was different from the baseline.</p><pre tabindex=0><code>sum(SHAP values for all features) = pred_for_team - pred_for_baseline_values
</code></pre><p>To interpret the graph:</p><p>We predicted 0.7, whereas the base_value is 0.4979. Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature&rsquo;s effect. Feature values decreasing the prediction are in blue. The biggest impact comes from Goal Scored being 2. Though the ball possession value has a meaningful effect decreasing the prediction.</p><p>If you subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output.</p><h3 id=how-to-do-that-in-code>How to Do That in Code<a hidden class=anchor aria-hidden=true href=#how-to-do-that-in-code>#</a></h3><p>Let&rsquo;s get the model ready:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.ensemble <span style=color:#f92672>import</span> RandomForestClassifier
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#39;../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv&#39;</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> (data[<span style=color:#e6db74>&#39;Man of the Match&#39;</span>] <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;Yes&#34;</span>)  <span style=color:#75715e># Convert from string &#34;Yes&#34;/&#34;No&#34; to binary</span>
</span></span><span style=display:flex><span>feature_names <span style=color:#f92672>=</span> [i <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> data<span style=color:#f92672>.</span>columns <span style=color:#66d9ef>if</span> data[i]<span style=color:#f92672>.</span>dtype <span style=color:#f92672>in</span> [np<span style=color:#f92672>.</span>int64, np<span style=color:#f92672>.</span>int64]]
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> data[feature_names]
</span></span><span style=display:flex><span>train_X, val_X, train_y, val_y <span style=color:#f92672>=</span> train_test_split(X, y, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>my_model <span style=color:#f92672>=</span> RandomForestClassifier(random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>fit(train_X, train_y)
</span></span></code></pre></td></tr></table></div></div><p>We will look for SHAP value for a single row of the dataset. Let&rsquo;s check on the raw prediction first:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>row_to_show <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>data_for_prediction <span style=color:#f92672>=</span> val_X<span style=color:#f92672>.</span>iloc[row_to_show]  <span style=color:#75715e># use 1 row of data here. Could use multiple rows if desired</span>
</span></span><span style=display:flex><span>data_for_prediction_array <span style=color:#f92672>=</span> data_for_prediction<span style=color:#f92672>.</span>values<span style=color:#f92672>.</span>reshape(<span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>my_model<span style=color:#f92672>.</span>predict_proba(data_for_prediction_array)
</span></span></code></pre></td></tr></table></div></div><p>The output is <code>array([[0.29, 0.71]])</code>, the team is 70% likely to have a player win the award</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>import</span> shap  <span style=color:#75715e># package used to calculate Shap values</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create object that can calculate shap values</span>
</span></span><span style=display:flex><span>explainer <span style=color:#f92672>=</span> shap<span style=color:#f92672>.</span>TreeExplainer(my_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Calculate Shap values</span>
</span></span><span style=display:flex><span>shap_values <span style=color:#f92672>=</span> explainer<span style=color:#f92672>.</span>shap_values(data_for_prediction)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>shap<span style=color:#f92672>.</span>initjs()
</span></span><span style=display:flex><span>shap<span style=color:#f92672>.</span>force_plot(explainer<span style=color:#f92672>.</span>expected_value[<span style=color:#ae81ff>1</span>], shap_values[<span style=color:#ae81ff>1</span>], data_for_prediction)
</span></span></code></pre></td></tr></table></div></div><p>The <code>shap_values</code> object above is a list with two arrays. The first array is the SHAP values for a negative outcome (don&rsquo;t win the award), and the second array is the list of SHAP values for the positive outcome (wins the award). We typically think about predictions in terms of the prediction of a positive outcome, so we&rsquo;ll pull out SHAP values for positive outcomes (pulling out <code>shap_values[1]</code>).</p><p>Of course, SHAP package also has explainers for every type of model:</p><ul><li><code>shap.DeepExplainer</code> works with Deep Learning models.</li><li><code>shap.KernelExplainer</code> works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values.</li></ul><h2 id=4-advanced-uses-of-shap-value>4. Advanced Uses of SHAP Value<a hidden class=anchor aria-hidden=true href=#4-advanced-uses-of-shap-value>#</a></h2><p>Shap values show how much a given feature changed our prediction (compared to if we made that prediction at some baseline value of that feature).</p><p>Consider the equation:</p><pre tabindex=0><code>y = 4 * x1 + 2 * x2
</code></pre><p>If x1 takes the value 2, instead of a baseline value of 0, then our SHAP value for x1 would be 8 (from 4 times 2).</p><p>These are harder to calculate with the sophisticated models we use in practice. But through some algorithmic cleverness, Shap values allow us to decompose any prediction into the sum of effects of each feature value, yielding a graph like this:</p><p>In addition to this nice breakdown for each prediction, the Shap library offers great visualizations of groups of Shap values. We will focus on two of these visualizations. These visualizations have conceptual similarities to permutation importance and partial dependence plots</p><p>SHAP summary plots give us a birds-eye view of feature importance and what is driving it. We&rsquo;ll walk through an example plot for the soccer data:</p><p>This plot is made of many dots. Each dot has three characteristics:</p><ul><li>Vertical location shows what feature it is depicting</li><li>Color shows whether that feature was high or low for that row of the dataset</li><li>Horizontal location shows whether the effect of that value caused a higher or lower prediction.</li></ul><p>Some things you should be able to easily pick out:</p><ul><li>The model ignored the Red and Yellow & Red features.</li><li>Usually Yellow Card doesn&rsquo;t affect the prediction, but there is an extreme case where a high value caused a much lower prediction.</li><li>High values of Goal scored caused higher predictions, and low values caused low predictions</li></ul><h3 id=how-to-do-that-in-code-1>How to Do That in Code<a hidden class=anchor aria-hidden=true href=#how-to-do-that-in-code-1>#</a></h3><p>Get the data and model ready:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.ensemble <span style=color:#f92672>import</span> RandomForestClassifier
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#39;../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv&#39;</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> (data[<span style=color:#e6db74>&#39;Man of the Match&#39;</span>] <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;Yes&#34;</span>)  <span style=color:#75715e># Convert from string &#34;Yes&#34;/&#34;No&#34; to binary</span>
</span></span><span style=display:flex><span>feature_names <span style=color:#f92672>=</span> [i <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> data<span style=color:#f92672>.</span>columns <span style=color:#66d9ef>if</span> data[i]<span style=color:#f92672>.</span>dtype <span style=color:#f92672>in</span> [np<span style=color:#f92672>.</span>int64, np<span style=color:#f92672>.</span>int64]]
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> data[feature_names]
</span></span><span style=display:flex><span>train_X, val_X, train_y, val_y <span style=color:#f92672>=</span> train_test_split(X, y, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>my_model <span style=color:#f92672>=</span> RandomForestClassifier(random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)<span style=color:#f92672>.</span>fit(train_X, train_y)
</span></span></code></pre></td></tr></table></div></div><p>Let&rsquo;s get a SHAP summary plot:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>import</span> shap  <span style=color:#75715e># package used to calculate Shap values</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create object that can calculate shap values</span>
</span></span><span style=display:flex><span>explainer <span style=color:#f92672>=</span> shap<span style=color:#f92672>.</span>TreeExplainer(my_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># calculate shap values. This is what we will plot.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Calculate shap_values for all of val_X rather than a single row, to have more data for plot.</span>
</span></span><span style=display:flex><span>shap_values <span style=color:#f92672>=</span> explainer<span style=color:#f92672>.</span>shap_values(val_X)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Make plot. Index of [1] is explained in text below.</span>
</span></span><span style=display:flex><span>shap<span style=color:#f92672>.</span>summary_plot(shap_values[<span style=color:#ae81ff>1</span>], val_X)
</span></span></code></pre></td></tr></table></div></div><p>The code isn&rsquo;t too complex. But there are a few caveats.</p><ul><li>When plotting, we call <code>shap_values[1]</code>. For classification problems, there is a separate array of SHAP values for each possible outcome. In this case, we index in to get the SHAP values for the prediction of &ldquo;True&rdquo;.</li><li>Calculating SHAP values can be slow. It isn&rsquo;t a problem here, because this dataset is small. But you&rsquo;ll want to be careful when running these to plot with reasonably sized datasets. The exception is when using an xgboost model, which SHAP has some optimizations for and which is thus much faster.</li></ul><h3 id=shap-dependence-contributions-plots>SHAP Dependence Contributions Plots<a hidden class=anchor aria-hidden=true href=#shap-dependence-contributions-plots>#</a></h3><p>For SHAP dependence contribution plots provide a similar insight to partial dependence plot&rsquo;s, but they add a lot more detail.</p><p>Each dot represents a row of the data. The horizontal location is the actual value from the dataset, and the vertical location shows what having that value did to the prediction. The fact this slopes upward says that the more you possess the ball, the higher the model&rsquo;s prediction is for winning the Man of the Match award.</p><p>The spread suggests that other features must interact with Ball Possession %. For example, here we have highlighted two points with similar ball possession values. That value caused one prediction to increase, and it caused the other prediction to decrease.</p><p>For comparison, a simple linear regression would produce plots that are perfect lines, without this spread.</p><p>This suggests we delve into the interactions, and the plots include color coding to help do that. While the primary trend is upward, you can visually inspect whether that varies by dot color.</p><p>These two points stand out spatially as being far away from the upward trend. They are both colored purple, indicating the team scored one goal. You can interpret this to say In general, having the ball increases a team&rsquo;s chance of having their player win the award. But if they only score one goal, that trend reverses and the award judges may penalize them for having the ball so much if they score that little.</p><h3 id=how-to-do-that-in-code-2>How to Do That in Code<a hidden class=anchor aria-hidden=true href=#how-to-do-that-in-code-2>#</a></h3><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>import</span> shap  <span style=color:#75715e># package used to calculate Shap values</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create object that can calculate shap values</span>
</span></span><span style=display:flex><span>explainer <span style=color:#f92672>=</span> shap<span style=color:#f92672>.</span>TreeExplainer(my_model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># calculate shap values. This is what we will plot.</span>
</span></span><span style=display:flex><span>shap_values <span style=color:#f92672>=</span> explainer<span style=color:#f92672>.</span>shap_values(X)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># make plot.</span>
</span></span><span style=display:flex><span>shap<span style=color:#f92672>.</span>dependence_plot(<span style=color:#e6db74>&#39;Ball Possession %&#39;</span>, shap_values[<span style=color:#ae81ff>1</span>], X, interaction_index<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Goal Scored&#34;</span>)
</span></span></code></pre></td></tr></table></div></div><p>If you don&rsquo;t supply an argument for interaction_index, Shapley uses some logic to pick one that may be interesting!</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/python/>Python</a></li><li><a href=http://localhost:1313/tags/machine-learning/>“Machine Learning"</a></li><li><a href=http://localhost:1313/tags/shap/>SHAP</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/2023/2023-08-26-land-cover-elevation-and-slope/><span class=title>« Bài mới hơn</span><br><span>Land Cover, Elevation & Slope</span>
</a><a class=next href=http://localhost:1313/posts/2023/2023-08-20-feature-engineering/><span class=title>Bài cũ hơn »</span><br><span>Feature Engineering</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning Explainability on x" href="https://x.com/intent/tweet/?text=Machine%20Learning%20Explainability&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-08-26-ml-explainability%2f&amp;hashtags=Python%2c%e2%80%9cMachineLearning%22%2cSHAP"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning Explainability on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-08-26-ml-explainability%2f&amp;title=Machine%20Learning%20Explainability&amp;summary=Machine%20Learning%20Explainability&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-08-26-ml-explainability%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning Explainability on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-08-26-ml-explainability%2f&title=Machine%20Learning%20Explainability"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning Explainability on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-08-26-ml-explainability%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning Explainability on whatsapp" href="https://api.whatsapp.com/send?text=Machine%20Learning%20Explainability%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-08-26-ml-explainability%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning Explainability on telegram" href="https://telegram.me/share/url?text=Machine%20Learning%20Explainability&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-08-26-ml-explainability%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Machine Learning Explainability on ycombinator" href="https://news.ycombinator.com/submitlink?t=Machine%20Learning%20Explainability&u=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-08-26-ml-explainability%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>The Financial Engineer</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="Sao chép";function s(){t.innerHTML="Đã sao chép!",setTimeout(()=>{t.innerHTML="Sao chép"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>