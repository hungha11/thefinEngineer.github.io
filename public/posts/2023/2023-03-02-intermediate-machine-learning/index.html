<!doctype html><html lang=vi dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Intermediate Machine Learning | The Financial Engineer</title>
<meta name=keywords content="Python,Data Analysis,Data Science,Machine Learning,Kaggle"><meta name=description content="My Kaggle Learning Note 3"><meta name=author content="Kean Teng Blog"><link rel=canonical href=http://localhost:1313/posts/2023/2023-03-02-intermediate-machine-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=vi href=http://localhost:1313/posts/2023/2023-03-02-intermediate-machine-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Intermediate Machine Learning"><meta property="og:description" content="My Kaggle Learning Note 3"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/2023/2023-03-02-intermediate-machine-learning/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-02T00:00:00+00:00"><meta property="article:modified_time" content="2023-03-02T00:00:00+00:00"><meta property="og:site_name" content="QHung's Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Intermediate Machine Learning"><meta name=twitter:description content="My Kaggle Learning Note 3"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Intermediate Machine Learning","item":"http://localhost:1313/posts/2023/2023-03-02-intermediate-machine-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Intermediate Machine Learning","name":"Intermediate Machine Learning","description":"My Kaggle Learning Note 3","keywords":["Python","Data Analysis","Data Science","Machine Learning","Kaggle"],"articleBody":" Disclaimer: This article is my learning note from the courses I took from Kaggle.\nUsing machine learning in data analysis is a rather procedural approach. As we can notice from the approach, we will start by Preparing data » Defining a model » Model diagnostic checking » Model prediction to complete our workflow. This workflow is often and commonly practiced when we are doing data analysis work. In this course, we will learn some important and useful technique that can be used in our work to achieve a better model.\nMain learnings:\nApproaches towards missing data values and categorical variables (non-numeric) Construct pipeline to improve our workflow and code flow. Cross-validation technique Build state-of-the-art model such as XGBoost Approaches to avoid data leakage. 1. Missing Values \u0026 Categorial Variables 1.1 Missing values We can deal with missing values with the following three ways:\nDrop the columns containing missing values (not recommended, might loss access to important information) Imputation to fill the empty cells with some number. Imputation, then add a new column that shows the missing entries. Imputation will perform better than dropping the entire columns. This is how we can do that in code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## 1. drop columns with missing values # get the columns name cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()] # perform drop reduced_X_train = X_train.drop(cols_with_missing, axis = 1) ## 2. imputation from sklearn.impute import SimpleImputer my_imputer = SimpleImputer() imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train)) # imputation removed column names, put back imputed_X_train.columns = X_train.columns ## 3. extended imputation X_train_plus = X_train.copy() for col in cols_with_missing: X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull() my_imputer = SimpleImputer() imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus)) imputed_X_train_plus.columns = X_train_plus.columns 1.2 Categorical variables There are three ways to deal with categorical variables (non-numeric data):\nDropping the categorical variables (if the columns does not provide any useful information) Ordinal encoding — assign a unique value in the dataset to a different integer. One-hot encoding — create new columns to indicate the presence of each possible value in the original data For One-hot encoding, it means that if a column with 100 rows contains 100 unique values, it will create an extra (100 rows *100 unique values –100 original rows) new entries.\nWe can apply the 3 approaches with the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## 1. droppping categorical variables dorp_X_train = X_train.select_dtypes(exclude= ['object']) ## 2. ordinal encoding from sklearn.preprocessing import OrdinalEncoder label_X_train = X_train.copy() ordinal_encoder = OrdinalEncoder() label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols]) ## 3. one hot encoding from sklearn.preprocessing import OneHotEncoder # one hot encode categorical data OH_encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False) OH_cols_train = pd.DataFrame(OH_encoder.fit_transform[object_cols]) # add back removed index OH_cols_train.index = X_train.index # remove categorical column and add back the one hot encoded columns num_X_train = X_train.drop(object_cols, axis = 1) OH_X_train = pd.concat([num_X_train, OH_cols_train], axis = 1) 2. Pipelines Pipeline is a way to bundle our preprocessing and modelling steps to keep our code organized. The benefits of using pipeline are it gives a cleaner code, reduces bugs and make our model easier to be implemented.\nHere’s how we can apply pipeline to impute missing numerical entries and one-hot encode missing categorical entries:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder # preprocess numerical data numerical_transformer = SimpleImputer(strategy = 'constant') # preprocess categorical data categorical_transformer = Pipeline(steps = [ ('imputer', SimpleImputer(strategy = 'most frequent')), ('onehot', OneHotEncoder(handle_unknown = 'ignore')) ]) # bundle the two preprocesses preprocessor = ColumnTransformer( transformers = [ ('num', numerical_transformer, numerical_cols), ('cat', categorical_transformer, categorical_cols) ]) # bundle preprocessing and modelling my_pipeline = Pipeline(steps = [ ('preprocessor', preprocessor), ('model', model) ]) # model evaluation my_pipeline.fit(X)train, y_train) preds = my_pipeline.predict(X_valid) mean_absolute_error(y_valid, preds) 3. Cross-validation Cross-validation means we run our modelling process on different subsets of the data to get several measures of our model quality. Although this technique gives a more accurate measure of model quality, it can take some time to run as it need to estimate multiple models as we can see from the below images.\nIt is recommended to run cross-validation for smaller datasets while for larger datasets, a single validation if often suffice.\nHere’s how we can apply cross-validation in Python together with pipeline which we learned earlier on:\n1 2 3 4 5 6 from sklearn.model_selection import cross_val_score # split the data to 5 sets for validation scores = -1 * cross_val_score(my_pipeline, X, y, cv = 5, scoring = 'neg_mean_absolute_error') print(scores.mean()) It is surprising to see that negative mean absolute error is used in the code. This is because “sklearn” has a convention where all metrics are defined so a high number is better. Thus, the use of negatives allows convention consistency.\n4. XGBoost — Gradient Boosting In the random forest method, we improve a model prediction by averaging the prediction of many decision trees. Random forest method is one of an ensemble method where we combine the prediction of several models. A state-of-the-art method would be to apply gradient boosting where we perform iterative cycles to add models into an ensemble to result in better prediction.\nConcepts:\nUse the current ensemble to generate predictions for each observation in the dataset. Use the prediction to calculate a loss function. Use the loss function to fit a new model that will be added to the ensemble which will reduce the loss. Add this new model to the ensemble. Repeat Here’s how to do it in code:\n1 2 3 4 5 6 7 8 from xgboost import XGBRegressor from sklearn.metrics import mean_absolute_error my_model = XGBRegressor() my_model.fit(X_train, y_train) predictions = my_model.predict(X_valid) mean_absolute_error(predictions, y_valid) There are a few parameters in the xgregressor function that might affect the accuracy of our result:\nn_estimators which means how many times to go through the modelling cycle (concepts above), value too high or too low might result in overfitting or underfitting respectively early_stopping_rounds which means stopping the model when the validation score stops improving with imposed criteria learning_rate which means multiply the predictions from each model by a small number before adding up the prediction from each component model n_jobs which aims to improve model’s runtime. We can set the number equal to the cores of our machine 1 2 3 4 5 6 model = XGBRegressor(n_estimators = 1000, learning_rate = 0.05, n_jobs = 4) model.fit(X_train,y_train, early_stopping_rounds = 5, eval_set = [(X_valid, y_valid)], verbose = False) 5. Data Leakage Data leakage happens when training data contains information about the target, but similar data will not be available when we used the model to perform prediction. The two main types of data leakage are target leakage and train-test contamination.\n5.1 Target leakage Target leakage occurs when predictors include data that will not be available at the time when predictions is made. A way to overcome this issue is to think about timing or chronological order that data becomes available rather than whether a feature will help to make good predictions.\n5.2 Train-test contamination Train-test contamination happens when validation data affects the preprocessing behavior as the validation process is corrupted.\nFor example, imagine you run preprocessing (like fitting an imputer for missing values) before calling train_test_split(). The end result? Your model may get good validation scores, giving you great confidence in it, but perform poorly when you deploy it to make decisions. In the courses, there are several interesting case studies on data leakage that worth looking to improve our acumen when interpreting results from work.\nGuide:\nThis is tricky, and it depends on details of how data is collected (which is common when thinking about leakage). Would you at the beginning of the month decide how much leather will be used that month? If so, this is ok. But if that is determined during the month, you would not have access to it when you make the prediction. If you have a guess at the beginning of the month, and it is subsequently changed during the month, the actual amount used during the month cannot be used as a feature (because it causes leakage).\n","wordCount":"1399","inLanguage":"vi","datePublished":"2023-03-02T00:00:00Z","dateModified":"2023-03-02T00:00:00Z","author":{"@type":"Person","name":"Kean Teng Blog"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/2023/2023-03-02-intermediate-machine-learning/"},"publisher":{"@type":"Organization","name":"The Financial Engineer","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="The Financial Engineer (Alt + H)">The Financial Engineer</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=About><span>About</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Intermediate Machine Learning</h1><div class=post-description>My Kaggle Learning Note 3</div><div class=post-meta><span title='2023-03-02 00:00:00 +0000 UTC'>tháng 3 2, 2023</span>&nbsp;·&nbsp;7 phút&nbsp;·&nbsp;Kean Teng Blog</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Mục lục</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-missing-values--categorial-variables>1. Missing Values & Categorial Variables</a><ul><li><a href=#11-missing-values>1.1 Missing values</a></li><li><a href=#12-categorical-variables>1.2 Categorical variables</a></li></ul></li><li><a href=#2-pipelines>2. Pipelines</a></li><li><a href=#3-cross-validation>3. Cross-validation</a></li><li><a href=#4-xgboost--gradient-boosting>4. XGBoost — Gradient Boosting</a></li><li><a href=#5-data-leakage>5. Data Leakage</a><ul><li><a href=#51-target-leakage>5.1 Target leakage</a></li><li><a href=#52-train-test-contamination>5.2 Train-test contamination</a></li></ul></li></ul></nav></div></details></div><div class=post-content><blockquote><p><em>Disclaimer: This article is my learning note from the courses I took from Kaggle.</em></p></blockquote><p>Using machine learning in data analysis is a rather procedural approach. As we can notice from the approach, we will start by <em>Preparing data &#187; Defining a model &#187; Model diagnostic checking &#187; Model prediction</em> to complete our workflow. This workflow is often and commonly practiced when we are doing data analysis work. In this course, we will learn some important and useful technique that can be used in our work to achieve a better model.</p><p><strong>Main learnings:</strong></p><ul><li>Approaches towards missing data values and categorical variables (non-numeric)</li><li>Construct pipeline to improve our workflow and code flow.</li><li>Cross-validation technique</li><li>Build state-of-the-art model such as <code>XGBoost</code></li><li>Approaches to avoid data leakage.</li></ul><h2 id=1-missing-values--categorial-variables>1. Missing Values & Categorial Variables<a hidden class=anchor aria-hidden=true href=#1-missing-values--categorial-variables>#</a></h2><h3 id=11-missing-values>1.1 Missing values<a hidden class=anchor aria-hidden=true href=#11-missing-values>#</a></h3><p>We can deal with missing values with the following three ways:</p><ul><li>Drop the columns containing missing values (not recommended, might loss access to important information)</li><li>Imputation to fill the empty cells with some number.</li><li>Imputation, then add a new column that shows the missing entries.</li></ul><p>Imputation will perform better than dropping the entire columns. This is how we can do that in code:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#75715e>## 1. drop columns with missing values</span>
</span></span><span style=display:flex><span><span style=color:#75715e># get the columns name</span>
</span></span><span style=display:flex><span>cols_with_missing <span style=color:#f92672>=</span> [col <span style=color:#66d9ef>for</span> col <span style=color:#f92672>in</span> X_train<span style=color:#f92672>.</span>columns
</span></span><span style=display:flex><span>                      <span style=color:#66d9ef>if</span> X_train[col]<span style=color:#f92672>.</span>isnull()<span style=color:#f92672>.</span>any()]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># perform drop</span>
</span></span><span style=display:flex><span>reduced_X_train <span style=color:#f92672>=</span> X_train<span style=color:#f92672>.</span>drop(cols_with_missing, axis <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## 2. imputation</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.impute <span style=color:#f92672>import</span> SimpleImputer
</span></span><span style=display:flex><span>my_imputer <span style=color:#f92672>=</span> SimpleImputer()
</span></span><span style=display:flex><span>imputed_X_train <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(my_imputer<span style=color:#f92672>.</span>fit_transform(X_train))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># imputation removed column names, put back</span>
</span></span><span style=display:flex><span>imputed_X_train<span style=color:#f92672>.</span>columns <span style=color:#f92672>=</span> X_train<span style=color:#f92672>.</span>columns
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## 3. extended imputation</span>
</span></span><span style=display:flex><span>X_train_plus <span style=color:#f92672>=</span> X_train<span style=color:#f92672>.</span>copy()
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> col <span style=color:#f92672>in</span> cols_with_missing:
</span></span><span style=display:flex><span>  X_train_plus[col <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;_was_missing&#39;</span>] <span style=color:#f92672>=</span> X_train_plus[col]<span style=color:#f92672>.</span>isnull()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>my_imputer <span style=color:#f92672>=</span> SimpleImputer()
</span></span><span style=display:flex><span>imputed_X_train_plus <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(my_imputer<span style=color:#f92672>.</span>fit_transform(X_train_plus))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>imputed_X_train_plus<span style=color:#f92672>.</span>columns <span style=color:#f92672>=</span> X_train_plus<span style=color:#f92672>.</span>columns
</span></span></code></pre></td></tr></table></div></div><h3 id=12-categorical-variables>1.2 Categorical variables<a hidden class=anchor aria-hidden=true href=#12-categorical-variables>#</a></h3><p>There are three ways to deal with categorical variables (non-numeric data):</p><ul><li>Dropping the categorical variables (if the columns does not provide any useful information)</li><li>Ordinal encoding — assign a unique value in the dataset to a different integer.</li><li>One-hot encoding — create new columns to indicate the presence of each possible value in the original data</li></ul><p>For One-hot encoding, it means that if a column with 100 rows contains 100 unique values, it will create an extra <code>(100 rows *100 unique values –100 original rows)</code> new entries.</p><p>We can apply the 3 approaches with the following code:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#75715e>## 1. droppping categorical variables</span>
</span></span><span style=display:flex><span>dorp_X_train <span style=color:#f92672>=</span> X_train<span style=color:#f92672>.</span>select_dtypes(exclude<span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;object&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## 2. ordinal encoding</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> OrdinalEncoder
</span></span><span style=display:flex><span>label_X_train <span style=color:#f92672>=</span> X_train<span style=color:#f92672>.</span>copy()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ordinal_encoder <span style=color:#f92672>=</span> OrdinalEncoder()
</span></span><span style=display:flex><span>label_X_train[object_cols] <span style=color:#f92672>=</span> ordinal_encoder<span style=color:#f92672>.</span>fit_transform(X_train[object_cols])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## 3. one hot encoding</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> OneHotEncoder
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># one hot encode categorical data</span>
</span></span><span style=display:flex><span>OH_encoder <span style=color:#f92672>=</span> OneHotEncoder(handle_unknown <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;ignore&#39;</span>, sparse <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>OH_cols_train <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(OH_encoder<span style=color:#f92672>.</span>fit_transform[object_cols])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># add back removed index</span>
</span></span><span style=display:flex><span>OH_cols_train<span style=color:#f92672>.</span>index <span style=color:#f92672>=</span> X_train<span style=color:#f92672>.</span>index
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># remove categorical column and add back the one hot encoded columns</span>
</span></span><span style=display:flex><span>num_X_train <span style=color:#f92672>=</span> X_train<span style=color:#f92672>.</span>drop(object_cols, axis <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>OH_X_train <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>concat([num_X_train, OH_cols_train], axis <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>)
</span></span></code></pre></td></tr></table></div></div><h2 id=2-pipelines>2. Pipelines<a hidden class=anchor aria-hidden=true href=#2-pipelines>#</a></h2><p>Pipeline is a way to bundle our preprocessing and modelling steps to keep our code organized. The benefits of using pipeline are it gives a cleaner code, reduces bugs and make our model easier to be implemented.</p><p>Here’s how we can apply pipeline to impute missing numerical entries and one-hot encode missing categorical entries:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.compose <span style=color:#f92672>import</span> ColumnTransformer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.pipeline <span style=color:#f92672>import</span> Pipeline
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.impute <span style=color:#f92672>import</span> SimpleImputer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> OneHotEncoder
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># preprocess numerical data</span>
</span></span><span style=display:flex><span>numerical_transformer <span style=color:#f92672>=</span> SimpleImputer(strategy <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;constant&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># preprocess categorical data</span>
</span></span><span style=display:flex><span>categorical_transformer <span style=color:#f92672>=</span> Pipeline(steps <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>  (<span style=color:#e6db74>&#39;imputer&#39;</span>, SimpleImputer(strategy <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;most frequent&#39;</span>)),
</span></span><span style=display:flex><span>  (<span style=color:#e6db74>&#39;onehot&#39;</span>, OneHotEncoder(handle_unknown <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;ignore&#39;</span>))
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># bundle the two preprocesses</span>
</span></span><span style=display:flex><span>preprocessor <span style=color:#f92672>=</span> ColumnTransformer(
</span></span><span style=display:flex><span>  transformers <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    (<span style=color:#e6db74>&#39;num&#39;</span>, numerical_transformer, numerical_cols),
</span></span><span style=display:flex><span>    (<span style=color:#e6db74>&#39;cat&#39;</span>, categorical_transformer, categorical_cols)  
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># bundle preprocessing and modelling</span>
</span></span><span style=display:flex><span>my_pipeline <span style=color:#f92672>=</span> Pipeline(steps <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>  (<span style=color:#e6db74>&#39;preprocessor&#39;</span>, preprocessor),
</span></span><span style=display:flex><span>  (<span style=color:#e6db74>&#39;model&#39;</span>, model)
</span></span><span style=display:flex><span>  ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># model evaluation</span>
</span></span><span style=display:flex><span>my_pipeline<span style=color:#f92672>.</span>fit(X)train, y_train)
</span></span><span style=display:flex><span>preds <span style=color:#f92672>=</span> my_pipeline<span style=color:#f92672>.</span>predict(X_valid)
</span></span><span style=display:flex><span>mean_absolute_error(y_valid, preds)
</span></span></code></pre></td></tr></table></div></div><h2 id=3-cross-validation>3. Cross-validation<a hidden class=anchor aria-hidden=true href=#3-cross-validation>#</a></h2><p>Cross-validation means we run our modelling process on different subsets of the data to get several measures of our model quality. Although this technique gives a more accurate measure of model quality, it can take some time to run as it need to estimate multiple models as we can see from the below images.</p><p>It is recommended to run cross-validation for smaller datasets while for larger datasets, a single validation if often suffice.</p><p>Here’s how we can apply cross-validation in Python together with pipeline which we learned earlier on:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> cross_val_score
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># split the data to 5 sets for validation</span>
</span></span><span style=display:flex><span>scores <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span> <span style=color:#f92672>*</span> cross_val_score(my_pipeline, X, y,
</span></span><span style=display:flex><span>  cv <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>, scoring <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;neg_mean_absolute_error&#39;</span>)
</span></span><span style=display:flex><span>print(scores<span style=color:#f92672>.</span>mean())
</span></span></code></pre></td></tr></table></div></div><p>It is surprising to see that negative mean absolute error is used in the code. This is because “sklearn” has a convention where all metrics are defined so a high number is better. Thus, the use of negatives allows convention consistency.</p><h2 id=4-xgboost--gradient-boosting>4. XGBoost — Gradient Boosting<a hidden class=anchor aria-hidden=true href=#4-xgboost--gradient-boosting>#</a></h2><p>In the random forest method, we improve a model prediction by averaging the prediction of many decision trees. Random forest method is one of an ensemble method where we combine the prediction of several models. A state-of-the-art method would be to apply gradient boosting where we perform iterative cycles to add models into an ensemble to result in better prediction.</p><p><strong>Concepts:</strong></p><ul><li>Use the current ensemble to generate predictions for each observation in the dataset.</li><li>Use the prediction to calculate a loss function.</li><li>Use the loss function to fit a new model that will be added to the ensemble which will reduce the loss.</li><li>Add this new model to the ensemble.</li><li>Repeat</li></ul><p>Here’s how to do it in code:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">8
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>from</span> xgboost <span style=color:#f92672>import</span> XGBRegressor
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> mean_absolute_error
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>my_model <span style=color:#f92672>=</span> XGBRegressor()
</span></span><span style=display:flex><span>my_model<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>predictions <span style=color:#f92672>=</span> my_model<span style=color:#f92672>.</span>predict(X_valid)
</span></span><span style=display:flex><span>mean_absolute_error(predictions, y_valid)
</span></span></code></pre></td></tr></table></div></div><p>There are a few parameters in the <code>xgregressor</code> function that might affect the accuracy of our result:</p><ul><li><code>n_estimators</code> which means how many times to go through the modelling cycle (concepts above), value too high or too low might result in overfitting or underfitting respectively</li><li><code>early_stopping_rounds</code> which means stopping the model when the validation score stops improving with imposed criteria</li><li><code>learning_rate</code> which means multiply the predictions from each model by a small number before adding up the prediction from each component model</li><li><code>n_jobs</code> which aims to improve model’s runtime. We can set the number equal to the cores of our machine</li></ul><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>model <span style=color:#f92672>=</span> XGBRegressor(n_estimators <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>, learning_rate <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.05</span>,
</span></span><span style=display:flex><span>  n_jobs <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>fit(X_train,y_train, early_stopping_rounds <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>,
</span></span><span style=display:flex><span>  eval_set <span style=color:#f92672>=</span> [(X_valid, y_valid)],
</span></span><span style=display:flex><span>  verbose <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>)
</span></span></code></pre></td></tr></table></div></div><h2 id=5-data-leakage>5. Data Leakage<a hidden class=anchor aria-hidden=true href=#5-data-leakage>#</a></h2><p>Data leakage happens when training data contains information about the target, but similar data will not be available when we used the model to perform prediction. The two main types of data leakage are target leakage and train-test contamination.</p><h3 id=51-target-leakage>5.1 Target leakage<a hidden class=anchor aria-hidden=true href=#51-target-leakage>#</a></h3><p>Target leakage occurs when predictors include data that will not be available at the time when predictions is made. A way to overcome this issue is to think about timing or chronological order that data becomes available rather than whether a feature will help to make good predictions.</p><h3 id=52-train-test-contamination>5.2 Train-test contamination<a hidden class=anchor aria-hidden=true href=#52-train-test-contamination>#</a></h3><p>Train-test contamination happens when validation data affects the preprocessing behavior as the validation process is corrupted.</p><pre tabindex=0><code>For example, imagine you run preprocessing (like fitting an imputer for 
missing values) before calling train_test_split(). The end result? 
Your model may get good validation scores, giving you great confidence 
in it, but perform poorly when you deploy it to make decisions.
</code></pre><p>In the courses, there are several interesting case studies on data leakage that worth looking to improve our acumen when interpreting results from work.</p><p><strong>Guide:</strong></p><p>This is tricky, and it depends on details of how data is collected (which is common when thinking about leakage). Would you at the beginning of the month decide how much leather will be used that month? If so, this is ok. But if that is determined during the month, you would not have access to it when you make the prediction. If you have a guess at the beginning of the month, and it is subsequently changed during the month, the actual amount used during the month cannot be used as a feature (because it causes leakage).</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/python/>Python</a></li><li><a href=http://localhost:1313/tags/data-analysis/>Data Analysis</a></li><li><a href=http://localhost:1313/tags/data-science/>Data Science</a></li><li><a href=http://localhost:1313/tags/machine-learning/>Machine Learning</a></li><li><a href=http://localhost:1313/tags/kaggle/>Kaggle</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/2023/2023-03-15-intro-to-sql/><span class=title>« Bài mới hơn</span><br><span>Intro to SQL</span>
</a><a class=next href=http://localhost:1313/posts/2023/2023-02-25-stable-diffusion-webui-with-civitai-loras/><span class=title>Bài cũ hơn »</span><br><span>Stable Diffusion WebUI with Civitai LORAs</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Intermediate Machine Learning on x" href="https://x.com/intent/tweet/?text=Intermediate%20Machine%20Learning&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-03-02-intermediate-machine-learning%2f&amp;hashtags=Python%2cDataAnalysis%2cDataScience%2cMachineLearning%2cKaggle"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Intermediate Machine Learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-03-02-intermediate-machine-learning%2f&amp;title=Intermediate%20Machine%20Learning&amp;summary=Intermediate%20Machine%20Learning&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-03-02-intermediate-machine-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Intermediate Machine Learning on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-03-02-intermediate-machine-learning%2f&title=Intermediate%20Machine%20Learning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Intermediate Machine Learning on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-03-02-intermediate-machine-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Intermediate Machine Learning on whatsapp" href="https://api.whatsapp.com/send?text=Intermediate%20Machine%20Learning%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-03-02-intermediate-machine-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Intermediate Machine Learning on telegram" href="https://telegram.me/share/url?text=Intermediate%20Machine%20Learning&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-03-02-intermediate-machine-learning%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Intermediate Machine Learning on ycombinator" href="https://news.ycombinator.com/submitlink?t=Intermediate%20Machine%20Learning&u=http%3a%2f%2flocalhost%3a1313%2fposts%2f2023%2f2023-03-02-intermediate-machine-learning%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>The Financial Engineer</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="Sao chép";function s(){t.innerHTML="Đã sao chép!",setTimeout(()=>{t.innerHTML="Sao chép"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>