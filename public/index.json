[{"content":" Để lấy file code và dữ liệu, vui lòng gửi email tới hung.ha@miquant.vn\nConcept Trong thị trường tài chính đầy biến động, mỗi điểm dữ liệu không chỉ chứa giá trị tức thời mà còn ẩn chứa các mối liên hệ ngầm hình thành từ quy luật biến động theo thời gian. Các mô hình thống kê chuỗi thời gian (time series) truyền thống—với giả định tuyến tính và tính dừng— khó phát hiện chuyển đổi giữa các “regime” với nhau hay các phụ thuộc phi tuyến tính (non-linear) ẩn sâu trong dữ liệu chuỗi thời gian. Nhằm khắc phục những vấn đề này, Visibility Graph được phát triển, chuyển bài toán phân tích chuỗi thời gian (time series) thành phân tích mạng (network), từ đó tận dụng các công cụ của khoa học mạng (network science) để khai thác thông tin động lực và cấu trúc tiềm ẩn.\nTime Series to Visibility Graph? Time series Chuỗi thời gian là một dãy các điểm dữ liệu được liệt kê theo thứ tự thời gian, thường được thu thập tại các khoảng thời gian đều nhau. Nói cách khác, chuỗi thời gian là bất kỳ dãy dữ liệu nào được lấy ở các thời điểm rời rạc (discrete). Chuỗi thời gian thường được trực quan hóa bằng đồ thị đường, đồ thị diện tích hoặc đồ thị cột, trong đó trục hoành (x) tương ứng với thời gian, còn chiều cao (trục tung, y) tương ứng với giá trị dữ liệu tại thời điểm đó.\nMặc dù biểu đồ đường, diện tích hay cột cho ta cái nhìn tổng quan về xu hướng và biến động, nhưng rất khó để nhận diện:\nCác mối liên kết phi tuyến giữa hai điểm thời gian không kề nhau Những motif (pattern) lặp lại hay sự tự tương quan có thể tới từ những khoảng cách lớn trong dữ liệu Điểm gãy (change point) tiềm ẩn khi cấu trúc phụ thuộc giữa các giá thay đổi đột ngột Hay các trạng thái khác nhau của những giai đoạn thời gian khác nhau. Chính vì những khó khăn này của phương pháp pháp chuỗi thời gian, trong 2 thập kỷ trở lại đây, các nhà nghiên cứu bắt đầu chuyển hướng sang việc sử dụng các phương pháp “complex network” để mô tả các thông tin đầy biến động và phụ thuộc lẫn nhau của time series! Các phương pháp có thể kể đến như: proximity networks, transition networks, and visibility graphs. Bài viết sẽ tiến hành thử nghiệm với visibility graph.\nVisibility graph Visibility graph ra đời vào năm 2008, trong đó các nút (node) tương ứng với các điểm dữ liệu của chuỗi thời gian, và cạnh (edge) được nối giữa hai nút nếu chúng “nhìn thấy” nhau. Thuật toán này có thể ánh xạ mọi loại chuỗi thời gian thành mạng: chuyển chuỗi tuần hoàn thành đồ thị đều, chuỗi ngẫu nhiên thành đồ thị ngẫu nhiên và dễ dàng scale với mọi độ lớn của dữ liệu. Ngoài ra, có nhiều cách xây dựng visibility graph như: natural, horizontal và limited penetrable visibility graph. Bài viết thử nghiệm với natural visibility graph và mọi thử nghiệm luôn ngầm định là natural.\nViệc xây dựng visibility graph rất dễ, hai điểm dữ liệu được nối với nhau khi và chỉ khi đường thẳng nối chúng không bị bất kỳ điểm nào nằm giữa “che khuất” – tức là mọi giá trị tại các điểm giữa đều nằm dưới đường thẳng đó.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 from ts2vg import NaturalVG import networkx as nx import numpy as np import matplotlib.pyplot as plt # 1. Generate random time series (Brownian motion) rng = np.random.default_rng(110) ts = rng.standard_normal(size=8) ts = np.cumsum(ts) # 2. Build visibility graph g = NaturalVG(directed=None).build(ts) nxg = g.as_networkx() # 3. Make plots fig, [ax0, ax1, ax2] = plt.subplots(ncols=3, figsize=(12, 3.5),dpi=200) ax0.plot(ts) ax0.set_title(\u0026#34;Time Series\u0026#34;) ## bar plot for time series ax0.bar(range(len(ts)), ts, color=\u0026#39;lightblue\u0026#39;, alpha=0.7) ax0.set_xlabel(\u0026#34;Time\u0026#34;) # ax0.set_ylabel(\u0026#34;Value\u0026#34;) ax0.set_title(\u0026#34;Time Series Bar Plot\u0026#34;) graph_plot_options = { \u0026#34;with_labels\u0026#34;: False, \u0026#34;node_size\u0026#34;: 2, \u0026#34;node_color\u0026#34;: [(0, 0, 0, 1)], \u0026#34;edge_color\u0026#34;: [(0, 0, 0, 0.15)], } nx.draw_networkx(nxg, ax=ax1, pos=g.node_positions(), **graph_plot_options) ax1.tick_params(bottom=True, labelbottom=True) ax1.plot(ts) ax1.set_title(\u0026#34;Visibility Graph\u0026#34;) ## adding bar plot for visibility graph ax1.bar(range(len(ts)), ts, color=\u0026#39;lightblue\u0026#39;, alpha=0.7) ax1.set_xlabel(\u0026#34;Time\u0026#34;) # ax1.set_ylabel(\u0026#34;Value\u0026#34;) nx.draw_networkx(nxg, ax=ax2, pos=nx.kamada_kawai_layout(nxg), **graph_plot_options) ax2.set_title(\u0026#34;Visibility Graph\u0026#34;) # ax2.tick_params(bottom=True, labelbottom=True) ax2.set_xlabel(\u0026#39;Graph\u0026#39;) Về mặt toán học, phương trình xây dựng mô hình sẽ như sau.\nQuan sát dữ liệu, ta dễ dàng nhận thấy một hiện tượng thú vị: các điểm giá có xu hướng tự gom thành các cụm. Trong bài viết trước, chúng ta đã giới thiệu phương pháp xây dựng mạng và phân cộng đồng (community detection) để nhóm các nút tương đồng (link). Ở đây, phương pháp đó sẽ tiếp tục được áp dụng nhằm phân nhóm các ngày dựa trên mức độ tương đồng trong biến động giá, từ đó giúp xác định các “regime” khác nhau của chuỗi thời gian giá cổ phiếu. Ngoài ra, bài viết cũng thử nghiệm trích xuất thông tin “alternative” từ dữ liệu mạng như là chỉ báo để quản trị rủi ro cho tiến lược đầu tư.\nExperiment: Regime analysis \u0026amp; Risk management indicator Regime detection Trước tiên, chúng tiến hành thử nghiệm để đi tìm kiếm các trạng thái “regime” của cổ phiếu và VNINDEX. Để làm được điều đó, chúng ta sử dụng thuật toán natural visibility để xây dựng network, sau đó áp dụng thuật toán Louvain để xác định cộng đồng (community).\nSơ lược về bài toán regime detection, đây có thể xem như một dạng học không giám sát (unsupervised learning), trong đó mục tiêu là tự động phân nhóm các khoảng thời gian trong chuỗi giá dựa trên đặc trưng thống kê hoặc cấu trúc động lực. Mỗi regime tương ứng với một trạng thái thị trường khác nhau—chẳng hạn giai đoạn biến động thấp, biến động cao hay khủng hoảng—và không cần nhãn trước. Việc phát hiện chính xác các regime giúp tối ưu hóa chiến lược giao dịch, thiết lập bộ lọc rủi ro và hiểu rõ hơn về cấu trúc động lực của thị trường. Việc gom nhóm sẽ được xác định bằng thuật toán Louvain.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def graph_visibility_construction(df): \u0026#34;\u0026#34;\u0026#34; Construct a Natural Visibility Graph from a price series and detect communities. \u0026#34;\u0026#34;\u0026#34; symbol = df.columns[0] ts = df[symbol].values dates = df.index # Build visibility graph vg = NaturalVG(directed=None) g = vg.build(ts) nxg = g.as_networkx() # nxg = g.as_igraph() ## We can also use igraph if needed # Detect Louvain communities communities = nx.algorithms.community.louvain_communities(nxg) # communities = nxg.community_infomap() ## Using Infomap for community detection # Assign colors COLORS = [\u0026#34;#4C72B0\u0026#34;, \u0026#34;#DD8452\u0026#34;, \u0026#34;#55A868\u0026#34;, \u0026#34;#C44E52\u0026#34;, \u0026#34;#8172B3\u0026#34;, \u0026#34;#937860\u0026#34;, \u0026#34;#DA8BC3\u0026#34;, \u0026#34;#CCB974\u0026#34;, \u0026#34;#64B5CD\u0026#34;, \u0026#34;#E24A33\u0026#34;, \u0026#34;#8C72B0\u0026#34;, \u0026#34;#FFB347\u0026#34;, \u0026#34;#6BA368\u0026#34;, \u0026#34;#D9534F\u0026#34;, \u0026#34;#7E57C2\u0026#34;, \u0026#34;#A1887F\u0026#34;, \u0026#34;#E18ECF\u0026#34;, \u0026#34;#C7C05D\u0026#34;, \u0026#34;#4FAFBD\u0026#34;, \u0026#34;#F28E2B\u0026#34;] node_colors = [\u0026#34;#000000\u0026#34;] * len(ts) for cid, nodes in enumerate(communities): for n in nodes: node_colors[n] = COLORS[cid % len(COLORS)] return { \u0026#39;symbol\u0026#39;: symbol, \u0026#39;ts\u0026#39;: ts, \u0026#39;dates\u0026#39;: dates, \u0026#39;nxg\u0026#39;: nxg, \u0026#39;vg\u0026#39;: vg, \u0026#39;communities\u0026#39;: communities, \u0026#39;node_colors\u0026#39;: node_colors } Ta thu được kết quả như sau, thử nghiệm với FPT, HPG và VNINDEX\nChuyển đổi ngược trở lại thành time series thì sẽ trông như sau\nPhương pháp bắt tương đối chuẩn các thay đổi của giá cổ phiếu FPT. Với VNINDEX, việc xác định regime cũng tương đối chính xác và ta có thể nhận thấy Regime thứ 7 xuất hiện 3 lần. Việc xác định regime về mặt cảm quan là tương đối chính xác, tuy nhiên để thực sự đưa được regime này vào trong chiến lược đầu tư thì sẽ cần nhiều tính toán chi tiết hơn như: các thông tin mô tả mạng như (average shortest path, diameter, density, clustering coefficient,…). Viết phát triển tiếp tục mô hình regime detection này sẽ được nhường lại cho bạn đọc, hì!\nSố lượng ngày giao dịch trong từng communtiy (hay regime). Ta có thể thấy giao động từ khoảng 15, 20, 40 và 60 tương ứng với regime ngắn thường giao động khoảng 3 tuần tới 1 tháng và regime dài có thể giao động từ 2-3 tháng.\nRisk management indicator Một trong những ứng dụng khác của mô hình này là xây dựng các chỉ báo nhằm phục vụ cho việc quản trị rủi ro. Bài viết sử dụng 2 chỉ báo\nDiameter: Trong graph theory, diameter là độ dài lớn nhất trong tất cả các quảng đường ngắn nhất giữa 2 node trong network. Nôm na có thể tưởng tượng là tìm ra tuyến đường ngắn nhất giữa mọi cặp thành phố có thể có trên bản đồ, sau đó chọn tuyến đường dài nhất trong số các tuyến đường ngắn nhất đó – đó chính là diamter. Diameter lớn cho thấy thị trường kém kết nối hơn, khi đó thông tin sẽ cần nhiều bước trung gian hơn để lan truyền. Điều này có thể báo hiệu rủi ro tăng, bởi thị trường đang phân mảnh.\nBetweenness Centrality Variance: Betweenness centrality đo tần suất một node nằm trên đường đi đường ngắn nhất (shortest path) giữa các node khác. Betweenness centrality có nghĩa là node có ảnh hưởng rất lớn. Bằng cách lấy phương sai của các giá trị này, ta đánh giá sự phân tán của mức độ ảnh hưởng: Khi variance cao cảnh báo rằng chỉ một vài node giữ phần lớn vai trò kết nối – khi những điểm này biến động, toàn bộ cấu trúc mạng dễ bị tác động mạnh. Từ đó biến động lớn của betweenness variance chính là tín hiệu cảnh báo rủi ro tập trung, giúp ta phát hiện sớm các turning point và giảm tỷ trọng vị thế kịp thời.\nTừ đó, việc sử dụng 2 chỉ báo này như một công cụ quản lý rủi ro nhằm đánh giá mức độ rủi ro của cổ phiếu. Bài viết sẽ thiết kế một hàm “activation” như là một công tắc để giảm hoặc bán hết danh mục khi thị trường vượt một mức rủi ro nhất định diễn tả thông qua 2 chỉ báo trên. Công cụ này sẽ được kết hợp với chiến lược TSMOM (đọc tại đây) như là công cụ quản lý rủi ro cho chiến lược này.\nXây dựng chỉ báo\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def rolling_graph_features(prices: pd.Series, window: int = 60): \u0026#34;\u0026#34;\u0026#34; Daily rolling visibility-graph features: - diameter (on largest component) - betw_var (variance of betweenness centrality) \u0026#34;\u0026#34;\u0026#34; dates = prices.index[window:] vg = NaturalVG(directed=None) data = {\u0026#39;diameter\u0026#39;: [], \u0026#39;betw_var\u0026#39;: []} for dt in dates: vals = prices.loc[:dt].iloc[-window:].values G = vg.build(vals).as_networkx() # diameter if nx.is_connected(G): d = nx.diameter(G) else: comp = max(nx.connected_components(G), key=len) d = nx.diameter(G.subgraph(comp)) data[\u0026#39;diameter\u0026#39;].append(d) # betweenness variance bc = np.array(list(nx.betweenness_centrality(G).values())) data[\u0026#39;betw_var\u0026#39;].append(bc.var()) return pd.DataFrame(data, index=dates) Tích hợp công cụ quản trị rủi ro cho chiến lược.\nChiến lược TSMOM cơ bản:\n$$ r_{t,t+1}^{TSMOM} =\\text{signal} *\\text{position} *\\text{return} - \\text{cost}= sign(r_{t-6, t})*\\frac{0.4}{\\sigma_t}*r_{t,t+1} - \\text{cost} $$Chiến lược TSMOM có cơ chế quản trị rủi ro\nTrong đó hệ số cảnh báo đồ thị $s_t$ được định nghĩa theo từng đoạn với các ngưỡng $z_{\\rm th}=1.645$, $z_{\\max}=2.576$ và có giả trị tối thiểu là $s_{\\min}$, $z_t^D , z_t^B$ là chỉ báo Dimater và Betweenness Centrality Variance được normalize.\n$$ s_t = \\begin{cases} 1, \u0026 z_t \\le z_{\\rm th},\\\\[6pt] 1 - \\dfrac{z_t - z_{\\rm th}}{\\,z_{\\max}-z_{\\rm th}\\,}\\,(1 - s_{\\min}), \u0026 z_{\\rm th} \u003c z_t \u003c z_{\\max},\\\\[8pt] s_{\\min}, \u0026 z_t \\ge z_{\\max}, \\end{cases} \\quad z_t = \\max\\{0,\\;z^D_t,\\;z^B_t\\}. $$Ngoài ra lấy max để\nQuan tâm kịch bản xấu nhất: bằng cách lấy giá trị lớn nhất, bạn để chỉ báo nào (Diamater hay Betweenness centrality variance) “cực đoan” hơn sẽ chi phối tín hiệu rủi ro. Tính đơn điệu (monocity): nếu một trong hai chỉ báo trở nên rủi ro hơn, $z_t$ chỉ tăng (không bao giờ giảm), đảm bảo bạn không bao giờ đánh giá thấp mức độ rủi ro. Ngoài ra, hàm quản trị rủi ro chỉ được kích hoạt khi các chỉ báo vượt trên mức 90% và giảm vị thế tối thiểu khi chỉ báo vượt mức 99%.\nDưới 90% → diễn biến còn trong vùng “bình thường” → giữ nguyên vị thế. Trên 90% → bắt đầu “penalize” dần, càng lên tới 99% càng giảm mạnh. Trên 99% → khẩn cấp → giảm ngay về mức tối thiểu. Giữa 90–99%, nội suy tuyến tính từ 1 → mức tối thiểu Kết quả là một chỉ báo động: giữ nguyên khi thị trường bình thường, giảm dần khi căng thẳng tăng, và “giảm tối đa” khi cực đoan—tất cả dựa trên những mức cắt 90% và 99% quen thuộc trong quản trị rủi ro.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # === graph early-warning scaling === # (a) daily graph features → weekly gf_daily = rolling_graph_features(prices, window=graph_window) gf_weekly = (gf_daily .resample(\u0026#39;W-FRI\u0026#39;).last() .reindex(weekly_ret.index, method=\u0026#39;ffill\u0026#39;)) # (b) compute threshold flags alpha = 2.0 / (warn_lookback_weeks + 1) mu_diam = gf_weekly[\u0026#39;diameter\u0026#39;].ewm(alpha=alpha, min_periods=warn_lookback_weeks//2).mean() sd_diam = gf_weekly[\u0026#39;diameter\u0026#39;].ewm(alpha=alpha, min_periods=warn_lookback_weeks//2).std() mu_betw = gf_weekly[\u0026#39;betw_var\u0026#39;].ewm(alpha=alpha, min_periods=warn_lookback_weeks//2).mean() sd_betw = gf_weekly[\u0026#39;betw_var\u0026#39;].ewm(alpha=alpha, min_periods=warn_lookback_weeks//2).std() z_diam = (gf_weekly[\u0026#39;diameter\u0026#39;] - mu_diam) / sd_diam z_betw = (gf_weekly[\u0026#39;betw_var\u0026#39;] - mu_betw) / sd_betw z_spike = pd.concat([z_diam, z_betw], axis=1).clip(lower=0).max(axis=1) # assume z_spike is a pd.Series of your max‐z scores z_th = 1.645 z_max = 2.576 # piecewise linear # a) between z_th and z_max, linearly ramp down ramp = 1 - ((z_spike - z_th) / (z_max - z_th)) * (1 - s_min) # b) combine into one Series risk_scale = pd.Series(1.0, index=z_spike.index) # when above threshold, use ramp but cap at s_min mask = z_spike \u0026gt; z_th risk_scale[mask] = ramp[mask].clip(lower=s_min) position_warn = signal * pos_size * risk_scale trade_cost_warn = 2 * commission * pd.Series(position_warn).diff().abs().shift(-1) strat_ret_warn = position_warn * weekly_ret.shift(-1) - trade_cost_warn Kết quả thu được như sau cho cổ phiếu HPG\nMetric Buy \u0026amp; Hold TSMOM Traditional TSMOM + Graph Warning Total Return 656.45% 852.83% 1000.08% Annualized Return 24.51% 27.66% 29.66% Annualized Volatility 33.91% 29.95% 29.14% Sharpe Ratio 0.82 0.97 1.04 Sortino Ratio 1.28 1.56 1.74 Max Drawdown –71.62% –41.70% –41.70% 95% VaR –6.59% –5.31% –5.24% Kết quả cho cổ phiếu HPG cho thấy:\nSo với TSMOM truyền thống, việc thêm hệ số cảnh báo đồ thị giúp tăng lợi nhuận trung bình thêm 2% (từ 27.66% lên 29.66%) trong khi giảm nhẹ độ biến động (từ 29.95% xuống 29.14%) từ đó Sharpe Ratio được đẩy lên từ 0.97 lên 1.04. Một mức tối ưu ổn 🙂 tuy nhiên không phải là xuất sắc khi max drawdown vẫn giữ nguyên. Điều này có thể đến từ việc sử dụng dữ liệu hoặc thiết kế của công cụ quản trị rủi ro chưa bắt được đúng những pha giảm mạnh của thị trường. Việc tối ưu hoặc xây dựng một cơ chế đo lường rủi ro khác có thể được dễ dàng thực hiện cho bạn đọc.\nTuy nhiên, việc tách bạch cơ chế giữa chiến lược mua/bán cổ phiếu với cơ chế quản trị rủi ro có thể giúp trader dễ dàng cân chỉnh được phần hiệu suất của chiến lược hay cơ chế quản trị rủi ro từ đó giúp cho hệ thống minh bạch, dễ kiểm soát và đáng tin hơn.\nKết luận Visibility Graph mở ra một lăng kính mới trong phân tích chuỗi thời gian tài chính, giúp chuyển hóa những quy luật ẩn sâu trong chuyển động giá thành cấu trúc mạng có thể khai thác bằng các công cụ của network science. Thay vì chỉ dựa vào giả định tuyến tính và tính dừng của mô hình thống kê truyền thống, phương pháp này tận dụng được các phụ thuộc phi tuyến và khả năng nhận diện các “regime” thị trường thông qua phân cộng đồng. Việc áp dụng Natural Visibility Graph cùng thuật toán Louvain không chỉ cho phép phát hiện trực quan các giai đoạn của giá cổ phiếu nói riêng và time series nói chung mà còn cung cấp nền tảng để xây dựng các chỉ báo quản trị rủi ro dựa trên cấu trúc mạng.\nNgoài ra, phương pháp này cũng là một cách tiếp cận đầy tiềm năng trong việc xây dựng các công cụ quản trị rủi ro thông qua khai thác thêm các đặc trưng mạng để xác định các giai đoạn stress của thị trường, rủi ro phân mảnh của cổ phiếu hay chuyển đổi regime để từ đó tích hợp vào trong từng chiến lược đầu tư.\nTóm lại, visibility graph không chỉ là công cụ hỗ trợ phân tích chuỗi thời gian mà còn là nền tảng để xây dựng các chỉ báo rủi ro mang tính cấu trúc và động lực. Việc tiếp tục khai thác các đặc trưng mạng chuyên sâu và tinh chỉnh cơ chế kích hoạt sẽ giúp hoàn thiện mô hình, đồng thời đem lại lợi thế cạnh tranh trong quản trị rủi ro và tối ưu hóa chiến lược đầu tư.\nĐể lấy file code và dữ liệu, vui lòng gửi email tới hung.ha@miquant.vn\nReference [1] https://cbergillos.com/ts2vg/examples.html\n[2] https://www.cs.cornell.edu/home/kleinber/networks-book/\n[3] https://onlinelibrary.wiley.com/doi/epdf/10.1155/2019/5320686\n[4] https://vnquant.vn/posts/2025/2025-05-17-stock-market-network/\n[5] https://vnquant.vn/posts/2025/2025-03-24-tsmom-vietnam/\n[6] https://arxiv.org/abs/0810.0920\n","permalink":"http://localhost:1313/posts/2025/2025-06-23-graph-visibility-regime-and-risk-management/","summary":"Nghiên cứu phân tích thị trường nhằm xây dựng được cơ chế xác định pha (regime) của thị trường cũng như cổ phiếu thông qua các thuật toán liên quan tới graph visibility. Ngoài ra, bài viết cũng xây dựng cơ chế quản trị rủi ro cho chiến lược đầu tư thông qua các thuật toán liên quan tới network science.","title":"Graph visibility for regime detection and risk management"},{"content":" Bài viết này thử nghiệm bài toán “Community detection” cho thị trường chứng khoán Việt Nam (VN100) và backtest 1 chiến lược dựa trên thuật toán này.\nBài viết được lấy ý tưởng từ: https://arxiv.org/pdf/2112.13383\nĐể lấy file code và dữ liệu, vui lòng gửi email tới hung.ha@miquant.vn\nConcept Trong một môi trường có nhiều yếu tố tương tác với nhau, sự tương quan (thể hiện qua xu hướng biến động giống nhau) là điều tất yếu, và thị trường chứng khoán cũng không ngoại lệ. Mặc dù mối tương quan giữa các cổ phiếu đã được biết đến từ lâu, nhưng phải đến khi Harry Markowitz giới thiệu Lý thuyết Danh mục Đầu tư Hiện đại vào năm 1952, nó mới được áp dụng rộng rãi. Lý thuyết này đề xuất phương pháp định lượng để tối ưu hóa danh mục đầu tư dựa trên mối tương quan giữa các tài sản. Kể từ đó, việc nghiên cứu mối liên hệ giữa các cổ phiếu đã phát triển theo nhiều hướng, trong đó việc áp dụng lý thuyết mạng (Network Theory) nổi lên như một phương pháp hiệu quả và đang được đặc biệt quan tâm trong giai đoạn đầy biến động này.\nStarting simple, correlation matrix Correlation: what is it? Tương quan (correlation) là một chỉ số thống kê đo lường mức độ biến động đồng thời giữa hai biến số. Trong thị trường chứng khoán, tương quan thường được sử dụng để đánh giá mối liên hệ giữa giá của các cổ phiếu.\nCông thức tính hệ số tương quan Pearson:\n$$ r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}} $$Nói một cách đơn giản, tương quan giữa các cổ phiếu thể hiện mức độ \u0026ldquo;đồng hành\u0026rdquo; của chúng. Khi hai cổ phiếu có tương quan cao (gần 1), chúng thường tăng giảm cùng nhau—ví dụ, khi một cổ phiếu tăng 5%, cổ phiếu kia cũng có xu hướng tăng tương tự. Ngược lại, tương quan âm (gần -1) nghĩa là khi cổ phiếu này tăng, cổ phiếu kia thường giảm. Tương quan bằng 0 không đồng nghĩa với việc hai cổ phiếu không liên quan đến nhau, mà chỉ cho thấy không có mối liên hệ tuyến tính rõ ràng trong biến động giá của chúng.\nBài viết minh họa điều này qua việc so sánh tương quan giữa HPG với HSG và BWE. Với hệ số tương quan 0,8 (tính trên hiệu suất tuần), HPG và HSG có mối tương quan mạnh. Điều này dễ hiểu bởi cả hai đều là doanh nghiệp sản xuất thép, chịu tác động từ cùng các yếu tố thị trường như giá nguyên liệu đầu vào và nhu cầu xây dựng. Ngược lại, HPG và BWE chỉ có hệ số tương quan 0,2, cho thấy sự khác biệt trong mô hình kinh doanh và các yếu tố ảnh hưởng đến giá cổ phiếu của hai doanh nghiệp.\nQua đó có thể thấy, các cổ phiếu cùng ngành (hay rộng hơn là có chung các yếu tố cơ bản) thường phản ánh xu hướng biến động giá và rủi ro tương tự nhau, do nhà đầu tư thường định giá và quyết định dựa trên những yếu tố tương đồng. Hiện tượng này tạo ra hiệu ứng \u0026ldquo;cộng hưởng\u0026rdquo; trong biến động giá, khiến các cổ phiếu cùng nhóm ngành thường di chuyển đồng thời với nhau. Đây chính là nguồn gốc của rủi ro hệ thống (systematic risk).\nKhi mở rộng phân tích từ một vài cổ phiếu lên hàng chục, thậm chí hàng trăm mã (ví dụ như VN100), phương pháp chỉ dựa vào ma trận tương quan thuần túy sẽ gặp phải nhiều hạn chế sau:\nCurse of dimensionality Với $N$ tài sản, ta phải tính tới $\\tfrac{N(N-1)}{2}$ cặp tương quan. Khi $\\tfrac{N(N-1)}{2}$, con số này lên tới 4.950 cặp — vượt ra ngoài khả năng trực quan hóa và so sánh thủ công, dẫn đến “quá tải thông tin”. Noise và sai số ước lượng lớn Dữ liệu giá luôn chứa nhiều nhiễu và có tính không ổn định theo thời gian. Khi số tài sản tăng, sai số ước lượng hệ số tương quan (nhất là với mẫu nhỏ) càng lớn, dễ sinh ra các giá trị tương quan “giả” (spurious), từ đó gây hiểu nhầm về mối quan hệ giữa các cổ phiếu. Khó trích xuất cấu trúc Ma trận tương quan thuần túy thường rất “đậm đặc” (dense) — hầu như mọi cặp đều có tương quan khác 0. Điều này khiến khó khăn trong việc phân nhóm (clustering) hay tìm kiếm các cấu trúc ẩn (hidden pattern), vì không có điểm nhấn rõ nét để tách biệt các khối tài sản liên quan chặt chẽ. From correlation to network and community Con người vốn quen mô tả thế giới qua các thực thể (node) và mối quan hệ (edge): trong mạng xã hội, mỗi người là một nút, các liên hệ \u0026ldquo;kết bạn\u0026rdquo;, \u0026ldquo;theo dõi\u0026rdquo; tạo thành cạnh, và nhờ đó ta dễ dàng nhận ra gia đình, nhóm bạn, hay những nhân vật trung tâm chỉ bằng cách quan sát cấu trúc kết nối.\nTư duy trực quan này được áp dụng cho thị trường chứng khoán: thay vì xem xét từng cặp hệ số tương quan riêng lẻ, ta biến mỗi cổ phiếu thành một nút (node) và dùng đường nối (edge) thể hiện mức độ liên hệ về giá. Kết quả là một \u0026ldquo;bản đồ\u0026rdquo; mạng lưới, nơi các nhóm cổ phiếu cùng ngành hay cùng chu kỳ kinh doanh tự động hiện ra như những cộng đồng trong mạng xã hội. Điều này giúp nhà đầu tư nhanh chóng nắm bắt được các mối liên hệ, xác định được những cổ phiếu đóng vai trò \u0026ldquo;cầu nối\u0026rdquo; và nhận diện được các tâm điểm rủi ro – cơ hội mà không cần đi sâu vào khối dữ liệu đồ sộ của ma trận tương quan.\nTheory \u0026amp; Mathematical model Để biến đổi từ correlation matrix thành 1 network, bước 1 ta cần biến đổi từ cấu trúc matrix thành cấu trúc network. Sau đó, từ network tạo ra, ta tiến hành gom nhóm các cổ phiếu giống nhau lại.\nĐể xây dựng 1 network, 2 thuật toán nổi tiếng là Planar Maximally Filtered Graph và Minimum Spanning Tree.\nPlanar Maximally Filtered Graph (PMFG) PMFG giữ lại các liên kết quan trọng nhất trong khi buộc đồ thị phải duy trì tính phẳng (planar) (có thể vẽ trên một tờ giấy mà không có cạnh nào cắt nhau). Vì một đồ thị phẳng chỉ có thể có tối đa $E_{\\text{PMFG}} = 3(N-2)$ cạnh, bộ lọc thu gọn mạng lưới một cách đáng kể (nhưng vẫn bảo toàn được gấp ba lần thông tin so với Minimum Spanning Tree, vốn chỉ có $N-1$ cạnh) (đọc kĩ hơn tại đây: https://gmarti.gitlab.io/networks/2018/06/03/pmfg-algorithm.html).\nKết quả thu được sẽ giữ lại cấu trúc cốt lõi của các mối quan hệ mạnh nhất :). In a simple way, thay vì giữ toàn bộ N*(N-1)/2 thì giờ chỉ còn tối đa 3(N-2) cạnh (mối quan hệ), tương đương với 3*(100-2) = 294 cạnh so với 4950 (giảm 16 lần)\nInput của PMFG hay MST trong trường hợp này đều là correlation matrix.\nCommunity detection Sau khi đã có mạng PMFG tinh gọn, bước tiếp theo là chia mạng thành những “cụm” (community) cổ phiếu giống nhau. Hai cách phổ biến nhất là Louvain (tối ưu modularity) và Infomap (tối ưu dòng ngẫu nhiên).\nLouvain Louvain chia mạng thành cộng đồng bằng cách tối đa hóa chỉ số modularity Q – thước đo so sánh mật độ liên kết trong cộng đồng với mật độ mong đợi nếu các cạnh được nối ngẫu nhiên.\n$$ Q = \\frac{1}{2m}\\sum_{i,j}\\Bigl(A_{ij} - \\frac{k_i k_j}{2m}\\Bigr)\\delta(c_i,c_j), $$trong đó:\nAij = trọng số của cạnh giữa i và j, ki = tổng trọng số gắn với nút i, m = tổng trọng số cạnh, δ(ci,cj) = 1 nếu các nút cùng cộng đồng. Thuật toán Louvain hoạt động lặp đi lặp lại. Ban đầu, mỗi cổ phiếu được coi là một cộng đồng riêng. Sau đó, thuật toán kiểm tra từng cổ phiếu và phân bổ lại nó vào cộng đồng lân cận mà việc di chuyển đó mang lại sự gia tăng lớn nhất về modularity – một thước đo mật độ các liên kết bên trong các cộng đồng so với các liên kết giữa các cộng đồng. Quá trình này được lặp lại cho đến khi không còn sự di chuyển nào cải thiện modularity. Khi giai đoạn này hoàn thành, mỗi cộng đồng được nén lại thành một \u0026ldquo;siêu nút\u0026rdquo; duy nhất, và quá trình phân bổ lại các siêu nút này được lặp lại.\nViệc tinh chỉnh và nén lặp đi lặp lại này tiếp tục cho đến khi cấu trúc cộng đồng ổn định. Trên thực tế, các cổ phiếu có tương quan cặp đôi cao, chẳng hạn như HPG, HSG và NKG, có xu hướng nhóm lại với nhau trong một cộng đồng \u0026ldquo;thép\u0026rdquo;, trong khi VCB, BID và CTG hợp nhất thành một cộng đồng \u0026ldquo;ngân hàng\u0026rdquo;. Sau vài lần lặp, Louvain thường hội tụ về một phân vùng ổn định của mạng lưới thành các cộng đồng dày đặc, được tách biệt rõ ràng, thường tương ứng với các lĩnh vực kinh tế rõ ràng như thép, ngân hàng hoặc công nghệ/bán lẻ (ví dụ: FPT, MWG), mà không yêu cầu các nhãn ngành được xác định trước.\nInfoMap Infomap xem mạng lưới như một quá trình đi ngẫu nhiên (random walk) qua các nút và cạnh và đi tối thiểu hoá độ không chắc chắn - uncertainty (entropy) của chuỗi di chuyển này.\n$$ L(M) = q_{\\curvearrowright}H(Q) + \\sum_{i=1}^{m} p_{\\circlearrowright}^{\\,i} \\, H(P^{i}) $$ $q_{\\curvearrowright}$: xác suất thoát khỏi một module H(Q): entropy của mã thoát $p_{\\circlearrowright}$: xác suất ở lại trong module i H(Pi): entropy của các di chuyển trong module Thuật toán Infomap tiếp cận việc phát hiện cộng đồng bằng cách mô phỏng một random walk (bước đi ngẫu nhiên) trên cùng network. Ý tưởng cốt lõi là một người đi ngẫu nhiên (random walker) có khả năng ở lại trong các vùng được kết nối dày đặc (dense) trong thời gian dài hơn. Bằng cách gán mã ngắn hơn cho các phân đoạn đường đi bên trong các cộng đồng này và mã dài hơn cho các chuyển đổi giữa các cộng đồng, thuật toán xác định hiệu quả các module.\nHãy hình dung random walker này có nhiều khả năng di chuyển giữa các cổ phiếu có tương quan cao. Thuật toán Infomap tìm cách chia mạng lưới thành các nhóm (cộng đồng) sao cho người đi bộ ngẫu nhiên có xu hướng ở lại trong cùng một nhóm trong thời gian dài. Khi người đi bộ di chuyển từ nhóm này sang nhóm khác, điều đó giống như việc cần \u0026ldquo;thông báo\u0026rdquo; rằng họ đã rời khỏi nhóm hiện tại và vào một nhóm mới. Infomap tìm cách phân chia mạng lưới sao cho số lần \u0026ldquo;thông báo\u0026rdquo; này (số lần di chuyển giữa các nhóm) được giảm thiểu. Các vùng mà người đi bộ dành phần lớn thời gian của họ - di chuyển qua lại giữa các cổ phiếu có liên quan chặt chẽ - được xác định là các module hoặc cộng đồng. Bởi vì người đi bộ mô phỏng có xu hướng \u0026ldquo;dừng chân\u0026rdquo; lâu hơn giữa các cổ phiếu tương quan cao, thuật toán này cũng có xu hướng nhóm HPG, HSG và NKG lại với nhau, và giữ bộ ba ngân hàng VCB, BID, CTG trong cùng một module.\nThuật toán Tối ưu tiêu chí Cách tiếp cận Ưu điểm Nhược điểm Louvain Modularity Q Greedy optimization Rất nhanh, hiệu quả, có thể áp dụng cho mạng lớn Có thể mắc kẹt ở tối ưu cục bộ, khó phát hiện cộng đồng nhỏ hoặc chồng lấn Infomap Entropy (Description Length) Random Walk Phân biệt tốt các cộng đồng nhỏ và chồng lấn, cho kết quả “kĩ” hơn Chạy chậm hơn Louvain, nhạy cảm với cấu trúc mạng và random. Đọc kĩ hơn 2 thuật toán này tại đây: https://www.statworx.com/en/content-hub/blog/community-detection-with-louvain-and-infomap\nTóm gọn lại, ta sẽ xây dựng mạng và sử dụng thuật toán community detection như sau\nB1: Xây dựng mạng (network) - Sử dụng PMFG\nB2: Tiến hành gom nhóm (coloring graph): Louvain hoặc InfoMap. 2 thuật toán có 2 cách tiếp cận khác nhau.\nMặc dù lý thuyết rất phức tạp, nhưng implement bằng python thì rất đơn giản vì rất nhiều thư viện hỗ trợ rồi! So lucky to be living in the 21st century.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import igraph as ig import community as louvain import networkx as nx def build_pmfg(corr): tickers = corr.columns.tolist() N = len(tickers) dists = np.sqrt(2 * (1 - corr.values)) edges = [(i, j, dists[i, j]) for i in range(N) for j in range(i)] edges.sort(key=lambda x: x[2]) G = nx.Graph() G.add_nodes_from(range(N)) max_edges = 3 * (N - 2) for u, v, w in edges: if G.number_of_edges() \u0026gt;= max_edges: break G.add_edge(u, v, weight=w) if not nx.check_planarity(G, False)[0]: G.remove_edge(u, v) return nx.relabel_nodes(G, dict(enumerate(tickers))) def detect_communities_infomap(G): tickers = list(G.nodes) g = ig.Graph() g.add_vertices(len(tickers)) g.vs[\u0026#34;name\u0026#34;] = tickers edges = [(tickers.index(u), tickers.index(v)) for u, v in G.edges()] weights = [1.0 / G[u][v][\u0026#34;weight\u0026#34;] for u, v in G.edges()] g.add_edges(edges) g.es[\u0026#34;weight\u0026#34;] = weights communities = g.community_infomap(edge_weights=\u0026#34;weight\u0026#34;, trials=10) return [[g.vs[idx][\u0026#34;name\u0026#34;] for idx in comm] for comm in communities] def detect_communities_louvain(G): for u, v, d in G.edges(data=True): d[\u0026#39;weight\u0026#39;] = 1.0 / d[\u0026#39;weight\u0026#39;] partition = louvain.best_partition(G, weight=\u0026#39;weight\u0026#39;) comms = {} for node, cid in partition.items(): comms.setdefault(cid, []).append(node) return list(comms.values()) Experiments: VN100 Sau khi ta đã phân tích được các community của các cổ phiếu , ta thử nghiệm 1 chiến lược đơn giản: “Mua cổ phiếu có Sharpe cao nhất trong 300 ngày vừa qua của mỗi cụm và nắm giữ trong vòng 60 ngày tiếp theo”. Tỷ trọng sẽ được phân bổ đều (equal weighted) cho các cổ phiếu được chọn. Việc tái cân bằng danh mục được thực hiện sau mỗi 60 ngày.\nChiến lược này không nhằm “dự đoán giá cổ phiếu”, mà cung cấp một bộ lọc \u0026amp; phân bổ danh mục hợp lý dựa trên cấu trúc thị trường.\nSample danh mục hiện tại (2025-01-03)\nSố lượng Infomap Louvain 1 BMP BMP 2 LPB LPB 3 FPT FPT 4 CTD DBC 5 DBC VPI 6 IMP IMP 7 HCM KDH 8 KDH 9 TLG 2 chiến lược chỉ khác nhau phương pháp “community detection” mà sự khác biệt đã khác lớn đúng không!\nChiến lược Lợi nhuận hằng năm Biến động hằng năm Sharpe Max drawdown InfoMap 22.07 24.5 0.90 -42.85 Louvain 16.71 26.21 0.64 -47.26 VNINDEX 12.53 19.83 0.63 -40.34 Từ kết quả thử nghiệm trên, ta có thể thấy:\nChiến lược sử dụng InfoMap cho kết quả tốt hơn với Sharpe ratio 0.90, cao hơn đáng kể so với Louvain (0.64) và VNINDEX (0.63) Lợi nhuận hằng năm của InfoMap (22.07%) cũng vượt trội hơn Louvain (16.71%) và VNINDEX (12.53%) Tuy nhiên, biến động của cả hai chiến lược (24.5% và 26.21%) đều cao hơn VNINDEX (19.83%). Drawdown tối đa của hai chiến lược (-42.85% và -47.26%) cũng lớn hơn so với VNINDEX (-40.34%), phản ánh rủi ro giảm giá lớn hơn Tuy nhiên, rủi ro có thể được hạn chế nếu sử dụng các phương pháp tối ưu hoá khác so với equal weighted như thử nghiệm này.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def select_portfolio(returns_window, communities): sharpe = returns_window.mean() / returns_window.std() K = len(communities) weights = {} for comm in communities: best = sharpe[comm].idxmax() weights[best] = 1.0 / K return weights def backtest_and_save(df, method=\u0026#34;infomap\u0026#34;, window=300, step=60): df = df.ffill().dropna(axis=1) returns = compute_returns(df) portfolios, metrics, all_returns = {}, [], [] for t in range(window, len(returns), step): if t + step \u0026gt; len(returns): break label = returns.index[t].strftime(\u0026#34;%Y-%m-%d\u0026#34;) windowed = returns.iloc[t-window:t] out = returns.iloc[t:t+step] G = build_pmfg(windowed.corr()) if method == \u0026#34;infomap\u0026#34;: comms = detect_communities_infomap(G) elif method == \u0026#34;louvain\u0026#34;: comms = detect_communities_louvain(G) else: raise ValueError(\u0026#34;Unknown method\u0026#34;) weights = select_portfolio(windowed, comms) portfolios[label] = weights visualize_and_save(G, comms, label, method) port_series = out[list(weights)].dot(pd.Series(weights)).rename(\u0026#34;rets\u0026#34;) port_series = port_series.reset_index().rename(columns={\u0026#34;index\u0026#34;: \u0026#34;timestamps\u0026#34;}) all_returns.append(port_series) cum = (1+port_series[\u0026#34;rets\u0026#34;]).prod()-1 vol = port_series[\u0026#34;rets\u0026#34;].std()*np.sqrt(252) sr = port_series[\u0026#34;rets\u0026#34;].mean()/port_series[\u0026#34;rets\u0026#34;].std()*np.sqrt(252) metrics.append({\u0026#34;period\u0026#34;: label, \u0026#34;CumReturn\u0026#34;: cum, \u0026#34;Volatility\u0026#34;: vol, \u0026#34;Sharpe\u0026#34;: sr}) final_df = pd.concat(all_returns, axis=0).reset_index(drop=True) final_df.to_csv(f\u0026#34;data/{method}/raw_returns.csv\u0026#34;, index=False) pd.DataFrame(metrics).set_index(\u0026#34;period\u0026#34;).to_csv(f\u0026#34;data/{method}/metrics.csv\u0026#34;) with open(f\u0026#34;data/{method}/portfolios.json\u0026#34;, \u0026#34;w\u0026#34;) as f: json.dump(portfolios, f, indent=2) Ngoài ra, InfoMap có hiệu suất tốt hơn trong trường hợp này. Louvain thường có xu hướng gom các nhóm lại thành cộng đồng lớn, do thuật toán ưu tiên tối đa hóa modularity – một đại lượng có thể bị \u0026ldquo;bẫy tối ưu cục bộ\u0026rdquo;, dẫn đến cộng đồng bị trộn lẫn (ví dụ: nhóm cổ phiếu ngân hàng có thể bị gom chung với cổ phiếu tài chính hoặc bảo hiểm). Trong khi đó, Infomap, nhờ đặc tính tối ưu “entropy”, thường tách rời được các nhóm hoạt động chặt chẽ hơn, như nhóm \u0026ldquo;thép\u0026rdquo;, \u0026ldquo;ngân hàng\u0026rdquo;, hay \u0026ldquo;bán lẻ - công nghệ\u0026rdquo; mà không bị pha tạp.\nNgoài ra, còn rất nhiều thuật toán community detection khác như : Walktrap, Clique Percolation Method, Leiden,.. mà bạn đọc có thể thử nghiệm.\nThuật toán Tối ưu tiêu chí Cách tiếp cận Ưu điểm Nhược điểm Louvain Modularity Q Heuristic Rất nhanh, hiệu quả, có thể áp dụng cho mạng lớn Có thể mắc kẹt ở tối ưu cục bộ, khó phát hiện cộng đồng nhỏ hoặc chồng lấn Infomap Entropy (Description Length) Random Walk Phân biệt tốt các cộng đồng nhỏ và chồng lấn, cho kết quả sắc nét hơn Chạy chậm hơn Louvain, nhạy cảm với cấu trúc mạng và ngẫu nhiên hóa Conclusion Nghiên cứu này thử nghiệm việc áp dụng network theory và community detection trong thị trường chứng khoán Việt Nam (VN100). Kết quả cho thấy:\nPhương pháp phát hiện cộng đồng có thể giúp hiểu rõ hơn về cấu trúc thị trường và mối quan hệ giữa các cổ phiếu Chiến lược đầu tư dựa trên InfoMap cho kết quả vượt trội hơn so với Louvain và VNINDEX về mặt Sharpe ratio và lợi nhuận Tuy nhiên, các chiến lược này cũng đi kèm với rủi ro cao hơn thể hiện qua biến động và drawdown lớn hơn Nghiên cứu này mở ra hướng phát triển tiếp theo bằng cách:\nThử nghiệm các thuật toán phát hiện cộng đồng khác Áp dụng các phương pháp tối ưu hóa danh mục đầu tư thay vì phân bổ đều Kết hợp với các chỉ báo kỹ thuật khác để cải thiện hiệu suất Cuối cùng, phương pháp này có thể được sử dụng như một công cụ hỗ trợ trong việc ra quyết định đầu tư, không nên được sử dụng một cách độc lập mà nên kết hợp với các phương pháp phân tích khác.\nReference [1] https://arxiv.org/pdf/2112.13383\n[2] https://gmarti.gitlab.io/networks/2018/06/03/pmfg-algorithm.html\n[3] https://www.statworx.com/en/content-hub/blog/community-detection-with-louvain-and-infomap\n[4] ChatGPT o3 :)\n","permalink":"http://localhost:1313/posts/2025/2025-05-17-stock-market-network/","summary":"Nghiên cứu phân tích thị trường chứng khoán VN100 bằng phương pháp phát hiện cộng đồng (Community detection) thông qua PMFG kết hợp với InfoMap/Louvain. Kết quả cho thấy chiến lược đầu tư dựa trên network này mang lại hiệu suất cao hơn VNINDEX.","title":"Community detection for VN100"},{"content":" Time Series Momentum - TSMOM, được Moskowitz, Ooi và Pedersen (2012), là một trong những chiến lược factor investing nổi bật và được nhiều nhà phân tích, nhà nghiên cứu thử nghiệm trên đa dạng thị trường. Hôm nay chúng ta sẽ tìm hiểu về chiến lược này, xem xét cách họ kiểm định các giả thuyết và xây dựng nên một chiến lược factor investing.\nConcept Khác với XSMOM (Cross sectional momentum , bạn có thể đọc tại đây), TSMOM tập trung vào khai thác động lượng của cổ phiếu đó so với quá khứ của chính nó (thay vì nhìn con nhà người ta và so sánh với họ, thì TSMOM là vượt lên chính mình là được rồi, haha). Chiến lược này dựa trên giả thuyết rằng xu hướng giá có tính liên tục (persistence) và có thể dự đoán được phần nào dựa trên lịch sử giá. Thay vì so sánh hiệu suất giữa các tài sản như XSMOM, TSMOM xem xét liệu một tài sản có duy trì được đà tăng/giảm của chính nó hay không.\nQuantopia có 1 bài viết và video rất sâu về TSMOM: link\nOK, khái niệm rất đơn giản. Giả thuyết của chúng ta là: \u0026ldquo;Cổ phiếu sẽ tiếp tục đà tăng/giảm dựa trên hiệu suất trong quá khứ\u0026rdquo;. Hãy cùng kiểm định giả thuyết này cho thị trường Việt Nam nhé.\nBài viết này sẽ cố gắng xây dựng lại các thử nghiệm và giả thuyết dựa trên:\nhttps://pages.stern.nyu.edu/~lpederse/papers/TSMOM_Slides.pdf\nRegression and evidence Họ bắt đầu với độ biến động (volatility) như một biến chuẩn hóa cho lợi nhuận vì mỗi cổ phiếu có mức độ volatile khác nhau. Trong bài viết, họ sử dụng mô hình EWMA đơn giản để tính toán độ rủi ro (volatility).\n$$ \\sigma_t^2\\;=\\;261\\sum_{i=0}^\\infty(1-\\delta)\\,\\delta^i\\bigl(r_{t-1-i} - \\overline{r}_t\\bigr)^2 $$Với:\nσ²ₜ là phương sai tại thời điểm t 261 là số ngày giao dịch trong năm → chúng ta sẽ đổi tham số này thành 252 cho thị trường Việt Nam. δ (delta) là hệ số suy giảm. Một cách tính toán để gán trọng số cao hơn cho dữ liệu gần đây và giảm dần theo thời gian. → phản ảnh biến động gần đây quan trọng hơn. Cách tính này khác độ biến động volatility thông thường khi độ biến động mọi ngày trong khoảng thời gian quan sát có độ quan trọng giống nhau. rₜ₋₁₋ᵢ là lợi nhuận tại thời điểm t-1-i r̄ₜ là lợi nhuận trung bình Volatility thu được từ công thức này sẽ được sử dụng để chuẩn hóa lợi nhuận, giúp so sánh công bằng giữa các cổ phiếu có mức độ biến động khác nhau.\nLet’s code it in python\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def exante_volatility(returns, com=60, annualization=252): \u0026#34;\u0026#34;\u0026#34; Compute the ex ante annualized volatility using an exponentially weighted variance. The formula is: σ_t^2 = 252 * Σ_{i=0}^∞ w_i (r_{t-1-i} - r̄_t)^2, Parameters: returns (pd.Series): Asset returns. com (int): Center of mass for the exponential weights (default is 60). annualization: annualization factor - default is 252 Returns: pd.Series: The ex ante volatility σ_t (annualized by multiplying the variance by 261). Volatility is computed using returns up to t-1. \u0026#34;\u0026#34;\u0026#34; ewm_variance = returns.ewm(com=com, adjust=False,min_periods=com).var() sigma = np.sqrt(annualization * ewm_variance) return sigma Kết quả của tính toán trên sẽ trông như sau, với đường màu xanh tượng trưng cho độ biến động (volatility) và đường màu cam là giá cổ phiếu ACB.\nRegression analysis Để kiểm định giả thuyết \u0026ldquo;Cổ phiếu sẽ tiếp tục đà tăng/giảm dựa trên hiệu suất trong quá khứ\u0026rdquo;, chúng ta sẽ sử dụng phân tích hồi quy (regression analysis). Phân tích hồi quy sẽ giúp chúng ta hiểu mối quan hệ giữa lợi nhuận trong quá khứ và lợi nhuận trong tương lai, cũng như đánh giá độ tin cậy của mối quan hệ này. Cụ thể, chúng ta sẽ thực hiện hồi quy lợi nhuận trong tương lai (future returns) lên lợi nhuận trong quá khứ (past returns).\nTuy nhiên, kiểm định này được thực hiện khác với bài viết gốc. Trong khi bài viết gốc sử dụng dữ liệu tháng để chạy hồi quy, người viết muốn thử nghiệm với dữ liệu tuần (tức giao dịch theo tuần) để xem chiến lược này hoạt động như thế nào. Do đó, tất cả các thử nghiệm hồi quy và chiến lược backtest sẽ được thực hiện trên dữ liệu tuần.\nMô hình hồi quy cơ bản có dạng:\n$$ \\frac{r_{t}^s}{\\sigma_{t-1}^s} = \\alpha + \\beta_h \\frac{r_{t-h}^s}{\\sigma_{t-h-1}^s} + \\epsilon_h^s $$ Ngoài ra, không chỉ đánh giá mỗi lợi nhuận lịch sử, tác giả đánh giá khả năng dự đoán của TSMOM thông qua hồi quy tín hiệu đơn giản - sign- của lợi nhuận (sign số dương bằng 1 và số âm bằng -1)\n$$ \\frac{r_{t}^s}{\\sigma_{t-1}^s} = \\alpha + \\beta_h \\text{sign}(r_{t-h}^s) + \\epsilon_h^s $$ 2 mô hình hồi quy trên được chạy trên giá của 60 cổ phiếu có giá trị giao dịch cao nhất và niêm yết từ 2014 tới nay.\nMỗi cột biểu diễn giá trị t-statistic của βh ứng với từng độ trễ (week lag) từ 1 đến 50 tuần. Đường nét đứt màu đỏ (thường) biểu thị mức ý nghĩa thống kê ở khoảng ±1.96(tương đương kiểm định hai phía với mức ý nghĩa 5%). Nếu cột (tức t-statistic) vượt quá giá trị này về phía dương hoặc âm, ta có thể kết luận hệ số βh là có ý nghĩa thống kê tại độ trễ (lookback) tương ứng. Ở độ trễ 2 và 6, ta thấy nhiều cột t-statistic rất cao, cho thấy βh dương và có ý nghĩa thống kê. Điều này chứng tỏ có hiệu ứng động lượng (momentum) đáng kể trong ngắn hạn, tức là cổ phiếu có xu hướng duy trì đà tăng/giảm trong vài tuần đầu tiên. Tức nếu 2 hoặc 6 tuần trước giá cổ phiếu tăng thì tuần này cũng có khả năng tăng rất cao và ngược lại. Khi độ trễ tăng (từ khoảng 10 tuần trở đi), ta thấy t-stat dưới -2 cho thấy có thể có yếu tố “reversal” tức sau 10 tuần thì trend có khả năng đảo chiều. Một chút phân tích: ở khung thời gian tuần, thị trường có thể bị ảnh hưởng nhiều bởi các yếu tố tâm lý, thanh khoản, tin tức ngắn hạn… Do đó, momentum thường bộc lộ rõ hơn trong vài tuần đầu, rồi nhanh chóng phai dần hoặc đảo ngược khi nhà đầu tư điều chỉnh vị thế hoặc chốt lời (10 tuần ~ 2 tháng)\n[Vì code cho regression test khá dài nên nhờ bạn comment email ở linkedin và mình sẽ gửi đến bạn nhanh nhất]\nSau khi chạy kiểm định hồi quy, ta rút ra được rằng chiến lược TSMOM có hiệu quả đáng kể trong ngắn hạn, đặc biệt là trong khoảng 2-6 tuần đầu tiên khi cổ phiếu có xu hướng duy trì đà tăng/giảm của mình. Tuy nhiên, sau khoảng 10 tuần (tương đương 2 tháng), hiệu ứng này có dấu hiệu đảo chiều, cho thấy các nhà đầu tư nên cân nhắc điều chỉnh chiến lược giao dịch của mình sau khung thời gian này. Giờ chúng ta sẽ cùng phát triển chiến lược TSMOM dựa trên khung thời gian tuần nhé.\nTSMOM, design from scratch. Đối với bất kỳ chiến lược giao dịch nào, chúng ta cần xác định 2 yếu tố: thời điểm mua (signal) và số lượng mua (position). TSMOM cũng cần 2 bước này.\nSignal Chiến lược momentum có signal tương đối đơn giản, nếu lợi nhuận tích luỹ trong “h” tháng lớn hơn 0 thì long, nếu âm thì short, còn nếu bằng 0 thì không làm gì. Vì chúng ta đang thử nghiệm ở thị trường VN nên chiến lược được thiết kế dạng long-only.\n1 2 3 4 5 6 7 8 9 10 11 12 13 def compute_signal(returns, window=6): \u0026#34;\u0026#34;\u0026#34; Compute the trading signal based on cumulative standardized returns over a rolling window. The paper uses: signal_t = sign(r_{t-6,t}^s) where r_{t-6,t}^s is the return from t-window to t. \u0026#34;\u0026#34;\u0026#34; cum_rets = returns.rolling(window=window).apply(lambda x: np.prod(x)) ## lợi nhuận tích luỹ signal = np.sign(cum_rets).clip(lower=0) ## sign để lấy chiều, clip lower 0 để loại bỏ các tín hiệu -1 tượng trưng cho short return signal Signal sẽ trông như thế này. 1 sẽ tương ứng với mua, và 0 là không làm gì.\nPosition sizing Với chiến lược momentum, rủi ro chính là việc không thể dự đoán được thời điểm thị trường đảo chiều. Tuy nhiên, chúng ta có thể hạn chế rủi ro này thông qua kỹ thuật \u0026ldquo;Inverse volatility\u0026rdquo; (đi ngược biến động).\nThay vì cố đoán thời điểm đảo chiều, chúng ta sẽ xác định trước mức rủi ro chấp nhận được và điều chỉnh vị thế khi rủi ro tăng - Volatility targeting. Đối với cổ phiếu có biến động thấp, ta có thể sử dụng thêm margin để tăng mức độ phơi nhiễm lên mức mục tiêu, từ đó tối ưu hóa cơ hội đầu tư.\nBài viết gốc sử dụng mức rủi ro mục tiêu là 40% (hàng năm). Chúng ta sẽ cùng thử nghiệm với cổ phiếu ACB, sử dụng volatility targeting để xác định kích thước vị thế như sau:\nCột màu cam thể hiện số lượng cổ phiếu mua dưới dạng phần trăm (%) trong khoảng từ 0 đến 2 (tức 0–200%). Mức trần được giới hạn ở 2 với giả định chúng ta luôn ký quỹ được 50%. Trong các giai đoạn rủi ro cao như 2016, 2018 và 2022, sizing của chúng ta dao động từ 0 đến 1.8 tùy thời điểm. Đặc biệt năm 2018—năm biến động rất lớn của ACB—chúng ta chỉ mua cổ phiếu dưới 100% số tiền hiện có (đi ngược độ biến động), từ đó bảo toàn được vốn. Sizing phụ thuộc vào Volaitlity target chúng ta đã đặt từ trước, volatility target càng nhỏ thì position sẽ giảm lại.\nChung quy lại, TSMOM dựa trên 2 kỹ thuật sau\nXác định cổ phiếu có đang trend không dựa trên lợi nhuận tích luỹ trong “h” tháng vừa qua. Xác định số cổ phiếu cần mua dựa trên kỹ thuật volatility targeting. Backtest Backtest cho chiến lược này tương đối đơn giản. Ta sẽ lấy tín hiệu (signal) nhân với số lượng (sizing) và nhân với lợi nhuận tuần sau trừ đi chi phí\n$$ r_{t,t+1}^{TSMOM} =\\text{signal} *\\text{position} *\\text{return} - \\text{cost}= sign(r_{t-6, t})*\\frac{0.4}{\\sigma_t}*r_{t,t+1} - \\text{cost} $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def backtest(daily_price, vol_target=0.4, look_back = 6, commission=0.001): \u0026#34;\u0026#34;\u0026#34; Backtest a trading strategy that goes long (short) when the signal is +1 (-1). Parameters: returns (pd.Series): Asset returns. Returns: pd.Series: The strategy returns. \u0026#34;\u0026#34;\u0026#34; ## compute the daily ex ante volatility first daily_return = daily_price.pct_change().dropna() weekly_return = daily_price.resample(\u0026#39;W\u0026#39;).last().pct_change().dropna() daily_vol = exante_volatility(daily_return, com=60) weekly_vol = daily_vol.resample(\u0026#39;W\u0026#39;).last().ffill() ## position sizing position_size = vol_target / weekly_vol position_size = position_size.clip(upper=2) ## max margin is 2 signal = compute_signal(weekly_return, window=look_back) position = signal * position_size trade_cost = 2 * commission * position.diff().abs().shift(-1) ## Compute strategy returns: # Use the previous period\u0026#39;s position (position.shift(1)) applied to the current week\u0026#39;s return, # and subtract any commission cost incurred at the beginning of the period. strat_ret = (position * weekly_return.shift(-1) - trade_cost) return_df = pd.DataFrame({ \u0026#39;raw_rets\u0026#39;: weekly_return, \u0026#39;signal\u0026#39;: signal, \u0026#39;position_size\u0026#39;: position_size, \u0026#39;position\u0026#39;: position, \u0026#39;trade_cost\u0026#39;: trade_cost, \u0026#39;strat_rets\u0026#39;: strat_ret }) return return_df.dropna() Thử nghiệm trên cổ phiếu HPG, ta thấy lợi nhuận tích luỹ vượt trội hẳn so với chiến lược buy and hold thông thường.\nTSMOM - HPG Buy\u0026amp;Hold - HPG Lợi nhuận hàng năm 29.23% 23.63% Biến động hằng năm 29.03% 33.65% Sharpe 1.01 0.7 Skewness 0.36 0.36 Kurtosis 4.16 3.67 Vì chỉ mua khi cổ phiếu có “trend” và tận dụng sử dụng margin khi cổ phiếu biến động thấp và giảm vị thế khi cổ phiếu biến động mạnh, nên lợi nhuận cũng như độ biến động trung bình hằng năm đều tốt hơn so với chiến lược chỉ buy and hold cổ phiếu. Từ đó dẫn đến tỷ lệ sharpe vượt lên mức 1.\n60 cổ phiếu trong sample test\nsymbol Annualized Return Annualized Volatility Sharpe Ratio Skewness Kurtosis FPT 39% 31% 1.26 1.28 6.60 PDR 36% 32% 1.10 1.19 6.96 MBB 32% 31% 1.03 1.29 5.57 VND 31% 30% 1.03 1.38 7.40 HCM 28% 28% 1.02 1.00 5.98 TCM 29% 30% 0.98 1.52 7.69 HPG 27% 29% 0.93 0.41 4.11 VCB 30% 32% 0.93 1.33 6.02 VIX 29% 31% 0.93 1.53 7.92 HSG 24% 27% 0.89 0.73 4.03 NKG 29% 32% 0.89 0.98 5.41 PNJ 29% 33% 0.89 0.80 5.62 KDH 26% 32% 0.81 1.18 5.45 HDG 22% 28% 0.80 0.61 4.14 ACB 25% 33% 0.77 1.92 14.27 CTD 23% 33% 0.69 0.43 4.34 SSI 19% 28% 0.68 0.46 5.39 DIG 21% 31% 0.67 1.37 8.70 SHB 20% 32% 0.63 3.04 20.06 HDC 20% 32% 0.62 1.08 4.97 REE 17% 29% 0.60 0.47 4.33 TSC 20% 34% 0.60 1.03 7.22 CTG 18% 31% 0.59 0.66 6.49 KBC 18% 31% 0.58 1.19 8.48 PHR 19% 33% 0.58 0.78 5.49 NLG 16% 29% 0.57 1.03 7.06 KSB 15% 31% 0.50 0.81 6.68 VNM 13% 27% 0.50 0.70 7.44 IJC 16% 33% 0.49 2.77 19.07 TTF 14% 28% 0.49 0.18 5.94 CII 12% 27% 0.45 1.68 11.26 DBC 13% 31% 0.43 0.80 8.50 DXG 12% 28% 0.43 (0.15) 5.11 STB 13% 29% 0.43 0.39 5.32 FIT 14% 34% 0.41 1.30 9.66 DRC 11% 28% 0.40 0.22 4.87 LCG 10% 25% 0.40 0.45 5.13 GAS 11% 28% 0.39 0.12 7.19 HHS 14% 35% 0.39 (1.36) 21.84 PVT 11% 28% 0.39 0.60 7.48 IDI 12% 33% 0.38 1.90 13.31 KDC 12% 33% 0.38 2.93 25.95 PAN 12% 31% 0.37 1.86 15.15 VIC 10% 32% 0.30 1.48 9.59 OGC 9% 31% 0.29 1.98 12.33 SBT 9% 33% 0.28 (1.09) 14.14 VCG 7% 28% 0.26 0.48 6.63 HAG 7% 30% 0.24 1.35 13.48 ASM 7% 28% 0.23 (0.02) 7.90 PET 7% 31% 0.23 1.47 10.78 DPM 6% 28% 0.22 0.63 6.32 DLG 6% 31% 0.19 1.30 11.48 PVD 6% 30% 0.19 0.07 6.02 BVH 5% 30% 0.18 1.15 11.03 GMD 4% 26% 0.16 0.79 4.16 HQC 5% 31% 0.16 2.68 20.08 MSN 4% 31% 0.14 1.72 13.40 VHC 3% 30% 0.11 (0.15) 5.46 FCN 2% 29% 0.09 0.38 8.03 EIB -2% 29% (0.08) 0.40 8.00 Để tạo một factor TSMOM, chúng ta cần tính trung bình cho toàn bộ mẫu nghiên cứu.\n$$ r_{t,t+1}^{TSMOM} = \\frac{1}{S_t} \\sum sign(r_{t-6, t})\\frac{0.4}{\\sigma_t}r_{t,t+1} $$ So sánh với VNINDEX, trong khung thời gian tháng, ta có mối tương quan giữa chiến lược TSMOM và lợi nhuận của VNINDEX như sau Với đặc tính momentum, chúng ta sẽ được bảo vệ khi thị trường giảm hoặc volatile thì chúng ta sẽ không có vị thế hoặc position sẽ giảm lại. Trong khi gia tăng “expose” khi thị trường ổn định và “đu sóng”.\nKết luận Chiến lược Time Series Momentum (TSMOM) là một phương pháp giao dịch hiệu quả khai thác động lượng giá của cổ phiếu dựa trên hiệu suất quá khứ. Qua phân tích dữ liệu thị trường Việt Nam, chúng ta có thể rút ra một số kết luận chính:\nHiệu ứng momentum tồn tại rõ rệt trong ngắn hạn, đặc biệt là trong khoảng 2-6 tuần đầu tiên sau khi có tín hiệu. Các cổ phiếu có xu hướng duy trì đà tăng/giảm trong ngắn hạn, tuy nhiên sau khoảng 10 tuần có thể có hiện tượng đảo chiều → chúng ta có thể tạo ra một factor “Reversal” dựa trên phát hiện này. Để áp dụng hiệu quả chiến lược TSMOM, cần chú ý: Xác định chính xác tín hiệu mua dựa trên momentum quá khứ - Sử dụng các kĩ thuật regression analysis Điều chỉnh kích thước vị thế theo biến động của thị trường (a.k.a xác định volatility target) Chiến lược này có thể được cải thiện thêm bằng cách kết hợp với các chỉ báo kỹ thuật khác nhằm thay đổi signal hoặc sizing theo một công thức khác.\n","permalink":"http://localhost:1313/posts/2025/2025-03-24-tsmom-vietnam/","summary":"Chiến lược Time Series Momentum (TSMOM) khai thác động lượng giá cổ phiếu dựa trên hiệu suất quá khứ, cho thấy hiệu ứng momentum rõ rệt trong 2-6 tuần đầu tiên, chứng tỏ sự tồn tại của factor này tại thị trường Việt Nam.","title":"Time series momentum - A traditional approach"},{"content":"Đây là bài viết thứ 2 trong chuỗi series “LLM, AI agent, kinh tế học và đầu tư” của miquant. Chuỗi bài này mong muốn cung cấp cho nhà đầu tư, nhà phân tích và người dùng một góc nhìn tổng quan về mô hình ngôn ngữ lớn (LLM), trí tuệ nhân tạo (AI) và cách các công cụ này đang thay đổi thế giới nói chung và cách chúng ta đầu tư nói riêng. Đọc qua bài viết trước tại đây.\nCông nghiệp hoá và tự động hoá Thế giới đang bước vào cuộc cách mạng công nghiệp mới—cuộc cách mạng công nghiệp lần thứ 5.\nCông nghệ hóa và tự động hóa luôn đóng vai trò then chốt trong sự phát triển của loài người. Từ cuộc cách mạng công nghiệp lần thứ nhất với động cơ hơi nước cho đến cuộc cách mạng công nghiệp lần thứ tư với trí tuệ nhân tạo và robot, con người không ngừng tìm cách tối ưu hóa quy trình sản xuất và nâng cao năng suất lao động. Trong khi các cuộc cách mạng công nghiệp trước đây chủ yếu tập trung vào việc tự động hóa các công việc thể chất và lặp đi lặp lại, cuộc cách mạng công nghiệp lần thứ 5 đang mở ra một chương hoàn toàn mới với sự tự động hóa những công việc trí tuệ và sáng tạo.\n(https://www.rothschildandco.com/en/newsroom/insights/2024/10/the-dawn-of-industry-5.0/)\nCuộc cách mạng này được thúc đẩy mạnh mẽ bởi sự ra đời của các mô hình ngôn ngữ lớn (LLM) và AI tiên tiến. Những công cụ này có khả năng xử lý ngôn ngữ tự nhiên, tạo ra nội dung đa dạng, và tham gia vào các quá trình ra quyết định phức tạp mà trước đây chỉ con người mới thực hiện được. Đây chính là bước tiến đột phá, đánh dấu một bước ngoặt quan trọng trong lịch sử phát triển công nghệ của nhân loại, định hình lại cách con người tương tác với công nghệ.\nỞ trung tâm của cuộc cách mạng này, AI agent đang nổi lên như một trong những công nghệ tiên tiến nhất, kế thừa thành tựu phát triển của khoa học máy tính và khoa học dữ liệu—những yếu tố cốt lõi của cuộc cách mạng công nghiệp lần thứ 4. Đây là hệ thống trí tuệ nhân tạo có khả năng tự động hóa các tác vụ phức tạp, tương tác tự nhiên với người dùng và thực hiện nhiệm vụ theo mục tiêu với sự can thiệp tối thiểu từ con người.\nNhư đã bàn luận ở bài viết trước, một hệ thống AI cần thoả mãn 3 yếu tố: suy luận (reason), học hỏi (learn) và hành động (act) theo cách thông thường đòi hỏi trí thông minh của con người. Dựa trên định nghĩa này, AI agent là công nghệ đang tiến gần nhất đến việc đạt được cả ba yếu tố trên.\nVậy AI agent là gì, và tại sao công nghệ này lại mạnh mẽ đến thế? Chúng ta sẽ cùng tìm hiểu sâu hơn thông qua sản phẩm mới nhất của miquant, Valuation Engine 1.0 sẽ ra mắt trong thời gian sắp tới.\nAI agent là gì? Có rất nhiều định nghĩa chuyên sâu về AI agent như từ Anthropic hay Chip Huyen, NVDIA hay FPT, tuy nhiên, để tổng hợp lại thì chúng ta có thể hiểu AI agent như sau.\nAI Agent là một hệ thống trí tuệ nhân tạo dựa trên các mô hình ngôn ngữ lớn (LLM) để thực hiện suy luận (reason) và học hỏi (learn). Hệ thống này có khả năng đưa ra quyết định và thực hiện các hành động (act) thông qua các công cụ và hành vi được cho phép do người phát triển (developer) quyết định. AI Agent có những đặc điểm và cơ chế hoạt động khác nhau tuỳ thuộc vào môi trường nó được thiết kế để vận hành. Ví dụ, AI Agent cho việc nghiên cứu (Deep research) có thể truy cập internet, tìm kiếm và phân tích thông tin liên quan, sau đó tổng hợp thành một bài nghiên cứu hoàn chỉnh cho người dùng (act).\nTrong khi đó, một Agent cho tài chính có khả năng thu thập dữ liệu giá chứng khoán, thực hiện các phép tính cần thiết và đưa ra cảnh báo cho người dùng (act 1), hoặc thậm chí thực hiện lệnh đặt mua/bán (act 2)—tuỳ theo thiết kế và mức độ được phép từ nhà phát triển hệ thống AI.\nTuy nhiên, như mọi hệ thống khác, AI agent không thể tránh khỏi sai số và đôi khi có thể vi phạm các quy định pháp lý hoặc tiêu chuẩn đạo đức. Do đó, việc thiết kế và triển khai AI agent đòi hỏi sự thận trọng cao độ, kèm theo các biện pháp kiểm soát chặt chẽ và khung quản trị rõ ràng. Không chỉ nhà phát triển cần cẩn trọng trong quá trình nghiên cứu và vận hành, người dùng cuối cũng phải thận trọng khi sử dụng các công cụ AI và hiểu rõ cả khả năng lẫn giới hạn của chúng. Tương tự như khi sử dụng các phương tiện giao thông, trách nhiệm của nhà phát triển là đảm bảo phương tiện hoạt động an toàn và hiệu quả, trong khi người dùng cần trang bị kiến thức và kỹ năng cần thiết để sử dụng chúng một cách có trách nhiệm. Miquant luôn đặt sự an toàn và tính minh bạch lên hàng đầu trong quá trình phát triển các công cụ AI.\nUse case: định giá cổ phiếu sử dụng AI agent. Là một công ty fintech trong lĩnh vực nghiên cứu thị trường tài chính Việt Nam, miquant nhận thấy một thách thức lớn trong ngành đầu tư: sự chênh lệch về khả năng tiếp cận thông tin giữa các tổ chức tài chính chuyên nghiệp và nhà đầu tư cá nhân ngày càng gia tăng.\nThực tế này được phản ánh rõ qua dữ liệu thị trường: trong hơn 1.500 mã cổ phiếu niêm yết tại Việt Nam, các công ty chứng khoán hàng đầu chỉ thường xuyên phát hành báo cáo phân tích cho khoảng 50-100 mã, chủ yếu tập trung vào các doanh nghiệp vốn hóa lớn trong VN30 hoặc các cổ phiếu đầu ngành. Điều này đồng nghĩa với việc hàng nghìn mã cổ phiếu khác—bao gồm cả những cơ hội đầu tư tiềm năng—không được phân tích một cách đầy đủ và chuyên sâu.\nNgay cả khi có báo cáo phân tích, một vấn đề khác nảy sinh: mỗi báo cáo đều dựa trên một tập hợp giả định cố định của người phân tích, khiến nhà đầu tư khó có thể đánh giá các kịch bản khác nhau hoặc cập nhật mô hình định giá khi xuất hiện thông tin mới. Giải pháp truyền thống là tự xây dựng mô hình định giá riêng—một quá trình đòi hỏi hàng chục giờ nghiên cứu và nền tảng phân tích chuyên sâu mà không phải nhà đầu tư nào cũng có sẵn.\nCông nghệ có thể thay đổi cuộc chơi? Trước thách thức này, chúng tôi đặt ra câu hỏi:\nTrong kỷ nguyên dữ liệu, liệu công nghệ có thể giúp thu hẹp khoảng cách thông tin này? Liệu có thể xây dựng một công cụ giúp nhà đầu tư tiếp cận phân tích chuyên sâu cho bất kỳ cổ phiếu nào họ quan tâm, đồng thời cho phép điều chỉnh các giả định theo nhu cầu thực tế? Câu trả lời của chúng tôi: \u0026ldquo;Có\u0026rdquo;.\nĐó là lý do miquant phát triển Valuation Engine 1.0, ứng dụng công nghệ AI Agent, giúp nhà đầu tư tiếp cận phân tích định giá linh hoạt, có thể tùy chỉnh—một bước tiến mới trong việc xử lý các thông tin tài chính.\nValuation Engine: nhìn vào trong hộp đen Valuation Agent được thiết kế theo một quy trình 7 bước, mô phỏng cách làm việc của các chuyên gia phân tích tài chính:\nThu thập dữ liệu: Agent thu thập thông tin cơ bản về công ty, ngành nghề hoạt động và vị thế trên thị trường, dữ liệu về cơ bản và các dữ liệu liên quan. Lựa chọn phương pháp định giá: Dựa trên đặc điểm của công ty, Agent sẽ xác định phương pháp định giá phù hợp nhất, ví dụ như P/E cho công ty tiêu dùng ổn định, P/B cho ngân hàng, hay EV/EBITDA cho doanh nghiệp cơ sở hạ tầng. Xác định nhóm công ty tương đồng: Đây là bước quan trọng và phức tạp. Agent không chỉ đơn thuần tìm các công ty cùng ngành mà còn phân tích sâu về quy mô, cơ cấu vốn và mô hình kinh doanh để tìm ra những doanh nghiệp thực sự có thể so sánh được. Dự báo các chỉ số tài chính: Agent phân tích xu hướng lịch sử và thông tin hiện tại cũng như triển vọng tương lai liên quan đến công ty để đưa ra dự báo về các chỉ số tài chính trong tương lai. Thực hiện định giá và kiểm định: Kết hợp các chỉ số dự báo với hệ số định giá mục tiêu để xác định giá hợp lý cho cổ phiếu. Nếu như kết quả định giá vượt quá xa so với giá thị trường, Agent sẽ tìm kiếm thêm thông tin và điều chỉnh các giả định để đảm bảo kết quả phản ánh chính xác nhất giá trị thực của doanh nghiệp. Tạo báo cáo định giá: Cuối cùng, Agent tổng hợp tất cả thông tin thành một báo cáo định giá cấu trúc rõ ràng và dễ hiểu, và gửi cho người dùng file PDF phân tích đi kèm với file Excel định giá. Xuyên suốt quá trình này, Agent sẽ liên tục tìm kiếm thông tin, đọc BCTC, đọc các báo cáo phân tích trong lịch sử của công ty,… một quá trình nghiên cứu như một analyst thông thường, nhưng với tốc độ rất cao và với lượng dữ liệu xử lý rất rất lớn.\nMột báo cáo định giá truyền thống thường mất từ vài ngày đến vài tuần để hoàn thành. Với AI Agent, quy trình này được rút ngắn xuống còn vài phút. Nhưng điều này không đồng nghĩa với việc hy sinh chất lượng phân tích mà ngược lại, người dùng có thể cùng với công cụ này để thử nghiệm định giá dưới nhiều kịch bản khác nhau để đi tới được kết luận cuối cùng.\nThử nghiệm: định giá VN100 Miquant tiến hành định giá các cổ phiếu có trong chỉ số VN100 hiện tại (tính đến ngày thực hiện là 12/03/2025). Các kế quả thu được như sau. Kết quả khi thực hiện định giá các cổ phiếu có thể được tiếp cận tại đây.\nTổng hợp thống kê các khuyến nghị và các phương pháp định giá Phân phối tiềm năng tăng giảm giá của các cổ phiếu trong chỉ số VN100\nCác thống kê liên quan\nTiềm năng tăng/giảm giá Trung bình (Mean) 15.41 Trung vị (Median) 16.62 Độ lệch chuẩn (Std) 23.25 Giá trị lớn nhất (Max) 67.97 Giá trị nhỏ nhất (Min) -40.73 Top 10 cổ phiếu có tiềm năng tăng/giảm nhiều nhất\nCổ phiếu Tăng Cổ phiếu Giảm PPC 67.97 BCG -40.73 VTP 67.57 AAA -37.68 NLG 65.61 KDC -35.47 DXG 65.36 SJS -28.43 VND 65.08 FTS -26.01 VJC 58.23 HHV -24.78 SHB 57.59 LPB -23.64 VHM 55.68 GVR -23.24 NT2 50.62 NKG -20.67 PAN 49.74 CTS -17.70 Điểm yếu của hệ thống này nằm ở khả năng tiếp cận thông tin: chỉ giới hạn ở các thông tin công khai và chính thống (tính tới thời điểm hiện tại). Đôi khi, có những thông tin nội bộ chưa được công ty công bố ra thị trường mà hệ thống không thể tiếp cận được.\nTuy nhiên, điểm mạnh nổi bật nhất của hệ thống này là tính linh hoạt cho phép người dùng tùy chỉnh và đưa vào các giả định riêng. Với những thông tin độc quyền mà người dùng nắm được, họ có thể nhanh chóng kết hợp và thử nghiệm các thông tin này với dữ liệu thị trường sẵn có để nhận được kết quả định giá cổ phiếu phù hợp.\nNgoài ra, tốc độ xử lý thông tin nhanh chóng của AI Agent cho phép nhà đầu tư có thể nhanh chóng điều chỉnh và cập nhật mô hình định giá khi có thông tin mới. Điều này đặc biệt hữu ích trong thị trường chứng khoán năng động, nơi giá cổ phiếu có thể thay đổi nhanh chóng dựa trên tin tức và sự kiện mới.\nĐể giảm thiểu rủi ro hallucination (hiện tượng AI tạo ra thông tin không chính xác), miquant liên tục cải tiến và nâng cấp hệ thống của mình. Chúng tôi áp dụng nhiều biện pháp kiểm soát chất lượng như:\nKiểm tra chéo thông tin từ nhiều nguồn dữ liệu đáng tin cậy Giới hạn phạm vi dự đoán của AI trong khuôn khổ các dữ liệu đã được xác thực nếu người dùng không cung cấp thông tin mới. Cho phép người dùng điều chỉnh và kiểm tra lại các giả định quan trọng Cập nhật liên tục cơ sở dữ liệu và các phương pháp kiểm soát chất lượng. Ngoài ra, miquant cũng liên tục ứng dụng các công nghệ mới nhất như knowledge graph, reflection agent để liên tục cải tiến các dự đoán, giả định và xác minh thông tin.\nKết luận Công nghệ AI và LLM đang mang đến một cuộc cách mạng trong cách chúng ta tiếp cận phân tích và định giá cổ phiếu. Với khả năng xử lý thông tin nhanh chóng và chính xác, AI Agent có thể hỗ trợ đắc lực cho các nhà đầu tư và chuyên gia phân tích trong việc ra quyết định đầu tư.\nTuy nhiên, điều quan trọng cần nhấn mạnh là AI Agent không nhằm mục đích thay thế hoàn toàn các chuyên gia phân tích, mà đóng vai trò như một công cụ hỗ trợ mạnh mẽ. Sự kết hợp giữa kinh nghiệm của con người và khả năng xử lý dữ liệu của AI sẽ tạo ra những phân tích sâu sắc và toàn diện hơn, giúp nhà đầu tư đưa ra quyết định đầu tư thông minh và hiệu quả hơn.\nTrong tương lai, miquant sẽ tiếp tục phát triển và hoàn thiện công cụ AI Agent, tích hợp thêm nhiều tính năng mới và cải thiện độ chính xác của các phân tích, nhằm mang đến cho người dùng những trải nghiệm đầu tư tốt nhất.\nĐể sử dụng được AI Agent sớm nhất, gia nhập danh sách chờ tại miquant.vn\n","permalink":"http://localhost:1313/posts/2025/2025-03-18-agents-valuation/","summary":"Miquant khám phá cơ chế của các mô hình ngôn ngữ lớn (LLM) và tác động của AI trong đầu tư, nhấn mạnh khả năng tự động hóa phân tích định tính, cải thiện hiệu suất và độ chính xác trong các tác vụ tài chính, đồng thời khẳng định vai trò quan trọng của con người trong quá trình ra quyết định.","title":"Agents - Cách mạng công nghiệp lần thứ 5"},{"content":"Đây là bài viết đầu tiên trong chuỗi series “LLM, AI agent, kinh tế học và đầu tư” của miquant. Chuỗi bài này mong muốn cung cấp cho nhà đầu tư, nhà phân tích và người dùng một góc nhìn tổng quan về mô hình ngôn ngữ lớn (LLM), trí tuệ nhân tạo (AI) và cách các công cụ này đang thay đổi thế giới nói chung và cách chúng ta đầu tư nói riêng.\nSự tổng hợp của Trí tuệ Trong câu chuyện về sự tiến hóa của công nghệ, có những khoảnh khắc đặc biệt khi ranh giới giữa tưởng tượng và thực tế trở nên mờ nhạt. Trí tuệ nhân tạo là một câu chuyện như vậy - một hành trình từ ý tưởng táo bạo trở thành hiện thực đang định hình lại thế giới của chúng ta. Giống như cách ánh sáng điện đã thắp sáng bóng tối, hay Internet đã kết nối nhân loại, AI đang viết nên chương mới trong câu chuyện tiến hóa của công nghệ. Từ những giấc mơ về máy móc thông minh, chúng ta đã chứng kiến sự xuất hiện của những người bạn số có thể hiểu, giao tiếp và sáng tạo theo cách tưởng chừng chỉ có trên phim.\nKhông giống như các cuộc cách mạng công nghệ trước đây thường tập trung vào việc tăng cường sức mạnh thể chất, AI đang khuếch đại chính khả năng trí tuệ - thứ làm nên bản chất con người. Đây không đơn thuần là một công cụ mới, mà là một người cộng sự mới trong hành trình khám phá và sáng tạo của nhân loại.\nTrí tuệ nhân tạo AI, định nghĩa chung Dưới góc nhìn của miquant, AI được mô tả như sau:\nAI (trí tuệ nhân tạo) là tập hợp các thuật toán có khả năng lý luận (reason), học hỏi (learn) và hành động (act) theo cách mà thông thường đòi hỏi trí thông minh của con người. Điểm khác biệt lớn nhất giữa AI và con người nằm ở cường độ và tốc độ xử lý thông tin—AI có thể tiếp nhận và phân tích dữ liệu với quy mô vượt xa khả năng của con người. Trong một hệ thống AI, yếu tố cốt lõi tạo nên sự khác biệt chính là dữ liệu mà nó được “dạy”. Tương tự con người, khả năng lý luận, học hỏi và hành động ở AI cũng hình thành từ quá trình tích lũy kinh nghiệm, kiến thức và trải nghiệm. Khi gặp một vấn đề mới, con người thường tìm kiếm các trường hợp tương tự (tối ưu hóa xác suất) và áp dụng những giải pháp đã có.\nĐiều thú vị là AI cũng được xây dựng và tối ưu dựa trên nguyên lý tương tự. Từ bắt đầu, các thuật toán AI đã được thiết kế để tìm ra mối quan hệ giữa các sự vật hiện tượng thông qua phương thức pattern-matching (tìm kiếm điểm tương đồng). Bằng cách quan sát và học từ rất nhiều ví dụ, khi gặp một tình huống gần giống, AI sẽ đưa ra dự đoán có xác suất tương đồng cao nhất. Con người cũng hoạt động theo cách này—trước khi đến tuổi đi học, chúng ta học ngôn ngữ mẹ đẻ thông qua vô vàn ví dụ từ cha mẹ, được lặp đi lặp lại hằng ngày. Hoặc khi học một môn thể thao, chúng ta phải tập thử nhiều lần để dần đoán được hướng đi của quả bóng khi đá mạnh hay nhẹ.\nTrong vòng 5 năm trở lại đây, AI đã có một bước tiến vượt bậc—đến mức được xem như cột mốc lịch sử. Cũng như con người khác biệt với động vật ở khả năng hiểu và sử dụng ngôn ngữ, các nhà khoa học đã nhận ra tầm quan trọng của ngôn ngữ trong AI. Họ thành công xây dựng các mô hình ngôn ngữ lớn (Large Language Model—LLM) dựa trên cơ chế tương tự, coi đó như một phương thức giao tiếp chung giữa người với máy, giữa máy với máy, và giữa nhiều lĩnh vực khác nhau.\nCode là ngôn ngữ của máy tính, protein và phân tử là ngôn ngữ của tế bào. Giờ đây, các mô hình ngôn ngữ lớn (LLM) trở thành cây cầu nối giúp kết hợp khả năng suy luận và kiến thức, tạo nên một nền tảng chung vững chắc.\nPhần tiếp theo của bài viết sẽ đào sâu vào chủ đề LLM và giải thích tại sao nó đã và đang thay đổi cách con người làm việc.\nAI trong thập niên 2010 Trong thập niên 2010, trí tuệ nhân tạo (AI) chủ yếu được phát triển với mục tiêu giải quyết các nhiệm vụ cụ thể, như một loạt các “chuyên gia” nhỏ trong mỗi lĩnh vực. Các hệ thống AI thời đó được thiết kế để thực hiện những tác vụ định sẵn với hiệu suất cao, thay vì cố gắng hiểu và xử lý mọi vấn đề một cách tổng quát.\nVí dụ, trong lĩnh vực xử lý ngôn ngữ tự nhiên, các mô hình như máy dịch tự động hay phân tích cảm xúc (sentiment analysis) được tạo ra riêng biệt dành cho một ngôn ngữ xác định trước. Các hệ thống này được tạo ra với một múc đích xác định và cố gắng tối ưu duy nhất bài toán đó.\nHay trong tài chính, các mô hình AI như dự đoán giá cổ phiếu, giá commodity hay tối ưu hoá danh mục được thiết kế riêng nhằm tối ưu hoá bài toán cụ thể này.\nTrong thập kỷ này, sự thành công của AI nằm ở phương pháp học có giám sát (supervised learning) thông qua mô hình các học sâu (Deep learning) mà nền tảng của chúng là các mạng neural.\nVới học có giám sát, ở từng bài toán cụ thể, nhà phát triển (Developer) sẽ gán nhãn (label) các ví dụ để biểu diễn các hành vi mà họ muốn mô hình AI học và huấn luyện dựa trên các ví dụ đó. Sau khi được huấn luyện, các mô hình AI này có thể được áp dụng vào dữ liệu mới. Ví dụ, để đào tạo mô hình phát hiện gian lận, các giao dịch sẽ được gán nhãn (target) là \u0026ldquo;lừa đảo\u0026rdquo; hoặc \u0026ldquo;không lừa đảo\u0026rdquo; cùng với các đặc điểm của từng giao dịch (features). Sau khi mô hình học được từ các đặc điểm này (như địa điểm gửi tiền, số tiền, tốc độ, cường độ\u0026hellip;) và các nhãn tương ứng, nó có thể dự đoán liệu một giao dịch mới có phải là lừa đảo hay không chỉ dựa trên các đặc điểm tương tự.\nKhoảnh khắc ChatGPT (ChatGPT moment) Vào ngày 30/11/2022, ChatGPT, một mô hình ngôn ngữ lớn đột phá từ OpenAI ra mắt, đánh dấu một cột mốc quan trọng trong lịch sử công nghệ. Không phải là một phép màu, ChatGPT là kết quả tổng hợp của nhiều thập kỷ nghiên cứu từ những năm 1950.\nTổng quát mà nói, mô hình ngôn ngữ (language model) hoạt động theo nguyên tắc đơn giản: chúng mã hóa ngôn ngữ thành các thông tin mang tính thống kê để ước tính xác suất xuất hiện của một từ trong một ngữ cảnh nhất định. Điều này giúp mô hình dự đoán và tạo ra văn bản một cách tự nhiên, phù hợp với ngữ cảnh. Ví dụ, với ngữ cảnh \u0026ldquo;Yếu tố quan trọng nhất ảnh hưởng đến thị trường tài chính là __\u0026rdquo;, một mô hình ngôn ngữ sẽ dự đoán xác suất xuất hiện của chữ \u0026ldquo;kì vọng\u0026rdquo; sẽ cao hơn nhiều so với chữ \u0026ldquo;thời tiết\u0026rdquo;.\nDưới góc nhìn của thuật toán, khác với con người, thay vì nhìn một câu dài với nhiều kí tự, chúng sẽ sử dụng “token”. Một token có thể là một từ, một phần của từ hoặc thậm chí là các ký tự đơn lẻ, ví dụ chữ “nhắc” là 2 token là “nh” và “ắc” được biểu diễn bằng 2 token 5380 và 35708; trong khi chữ “đẹp” được biểu diễn bằng 1 token 75134. Với phương pháp trên, một câu sẽ được biểu diễn bằng nhiều token khác nhau.\nTiếng Việt:\nThông qua dataset trên hugging face, team miquant nhận thấy rằng, sau khi chuyển sang token (GPT-4o), một câu tiếng Việt ở dạng token sẽ dài hơn trung bình 35% so với câu nguyên bản. Tức 100 chữ tiếng Việt sẽ xấp xỉ 135 token.\nDựa trên các token này, một mô hình ngôn ngữ sẽ cố gắng dự đoán lần lượt các token tiếp theo có xác suất xuất hiện cao nhất dựa trên các token trước đó. Các mô hình này liên tục dự đoán lần lượt từng token cho đến khi hoàn thành câu. Quy trình này hoạt động dựa trên xác suất và không đảm bảo luôn luôn chính xác. Tuy nhiên, nhờ vào cơ chế này, LLM lại có một sức mạnh to lớn so với rất nhiều mô hình AI khác.\nVới mọi yêu cầu bạn đặt ra (prompt), các mô hình LLM này luôn cố gắng hoàn thành nó, vì thế được gọi là tác vụ hoàn thành (completion tasks). Rất nhiều vấn đề của chúng ta có thể được diễn giải thành các tác vụ phải hoàn thành. Các bài toán như phiên dịch, lập trình, phân tích dữ liệu, hay giải toán hay phân tích cơ bản, doanh thu đều có thể được thiết kế dưới dạng các tác vụ hoàn thành thông qua cách bạn thiết kế câu prompt. Từ đó, các mô hình ngôn ngữ sẽ cố gắng đưa ra câu trả lời có xác suất cao nhất dựa trên tất cả thông tin bạn cung cấp.\n💡 Ví dụ cụ thể\nCâu hỏi: Dựa trên bài báo được cung cấp, hãy phân tích cảm xúc (Sentiment) của bài báo này.\n\u0026lt;Tiêu đề\u0026gt;\nQuốc Hội đề xuất ….\n\u0026lt;Nội dung\u0026gt;\nNgày 28/12 vừa qua \u0026hellip;\nTrả lời: Tích cực!\nMô hình ngôn ngữ sẽ cố gắng xác định bài báo với nội dung này có cảm xúc là tích cực, tiêu cực hay trung tính. Từ đó, mô hình ngôn ngữ đã trở thành một mô hình phân loại cảm xúc cho tin tức thông qua quá trình tối ưu hoá xác suất.\nAI trong tài chính Như đã bàn luận kĩ ở trên, bản thân các mô hình ngôn ngữ có sức mạnh rất lớn, nhờ vào khả năng hoàn thành các yêu cầu được giao. Tuy nhiên, cần nhấn mạnh rằng quy trình này là một quy trình dự đoán dựa trên xác suất, nên không đảm bảo luôn luôn chính xác. Mặc dù vậy, nhờ khả năng này, rất nhiều tác vụ trước đây tốn nhiều nhân lực, nguồn lực và thời gian đã trở nên đơn giản hơn, hiệu quả hơn và còn có độ chính xác cao hơn rất nhiều.\nLấy một ví dụ cụ thể, tại miquant, trong các nghiên cứu đã thực hiện, việc tạo ra một mô hình phân tích cảm xúc cho các thông tin báo chí Việt Nam đòi hỏi một lượng dữ liệu có cấu trúc (structured data) rất lớn. Miquant từng xây dựng bộ dữ liệu với 20.000 điểm mẫu phân loại thông tin tích cực, tiêu cực hay trung tính. Không chỉ vậy, việc huấn luyện một mô hình nhận diện cảm xúc còn đòi hỏi rất nhiều thời gian để huấn luyện, kiểm tra và triển khai. Tuy nhiên, kể từ khi LLM xuất hiện, miquant đã tối ưu hóa đáng kể cho bài toán phân tích cảm xúc. Thay vì 20.000 điểm mẫu, miquant chỉ cần sử dụng chưa đến 200 mẫu, đồng thời thời gian triển khai được rút ngắn xuống còn vài giây để xử lý các bài báo khác nhau—với độ chính xác thậm chí còn cao hơn trước. Một kết quả mà ngay cả đội ngũ miquant cũng không ngờ tới!\nNgoài ra, các mô hình LLM cũng đã được thử nghiệm cho quá trình thu thập dữ liệu, backtest các chiến lược đầu tư, tối ưu hoá danh mục, hay gần hơn, các mô hình về AI agent nhằm tăng độ chính xác cho các bài toán phức tạp hơn.\nSong, miquant tin rằng, sức mạnh lớn nhất của các mô hình LLM chính là tự động hoá khâu phân tính định tính (qualitative analysis) hơn là cho các phân tích về định lượng (quantitative analysis) trong đầu tư. Dựa trên cơ chế khác hẳn các mô hình AI truyền thống, LLM có thể hiểu ngôn ngữ (understanding) , suy luận (reasoning) và đưa ra một kết luận hay khuyến nghị (acting). Chính nhờ các yếu tố này, khâu phân tính về định tính mà chỉ con người mới có thể làm được, giờ đây đã được tự động hoá.\n(https://arxiv.org/pdf/2405.05345v1)\nPhân tích định tính truyền thống trong đầu tư tài chính thường đòi hỏi chuyên gia xử lý và nghiên cứu hàng trăm báo cáo tài chính, tin tức, và tài liệu khác để đánh giá các yếu tố không thể lượng hóa như chiến lược công ty, năng lực ban lãnh đạo, các xu hướng thị trường và cảm xúc của nhà đầu tư. Với sự xuất hiện của các mô hình LLM, phân tích định tính không chỉ được tự động hoá mà còn nâng cao đáng kể theo nhiều cách:\nPhân tích báo cáo tài chính toàn diện: LLM có thể nhanh chóng đọc và tóm tắt các báo cáo tài chính dài, xác định các điểm mạnh, điểm yếu, cơ hội và thách thức mà công ty đang đối mặt. Các mô hình này có thể đạt độ chính xác tương đương với phương pháp tuyền thống như các analyst trên thị trường. Điều đặc biệt ở đây chính là tốc độ và độ bao phủ (cover) khắp các mã trên thị trường. Đánh giá điểm nhìn chung (consensus) thị trường: LLM có thể phân tích hàng nghìn bài báo, bình luận trên mạng xã hội và diễn đàn đầu tư để đánh giá cảm xúc thị trường đối với một cổ phiếu hoặc ngành cụ thể. Điều này giúp nhà đầu tư hiểu được phần \u0026ldquo;tâm lý\u0026rdquo; của thị trường, một yếu tố quan trọng trong việc định giá tài sản. Mặc dù vậy, cần lưu ý rằng LLM không phải là giải pháp tuyệt đối. Các mô hình này vẫn có thể gặp phải vấn đề như \u0026ldquo;ảo giác\u0026rdquo; (hallucination), khi chúng tạo ra thông tin không chính xác hoặc không tồn tại, điều này chính là một điểm yếu lớn vì bản thân cơ chế của LLM là dự đoán dựa trên xác suất. Do đó, vai trò của con người vẫn rất quan trọng trong việc xác minh, đánh giá và đưa ra quyết định cuối cùng dựa trên những phân tích này.\nĐể sử dụng được các mô hình LLM hiệu quả, điều quan trọng nhất là không được lệ thuộc vào nó. Như mọi giai đoạn phát triển của lịch sử khác, công nghệ đến rồi đi. Tất cả công nghệ cũng chỉ là công cụ nhằm hỗ trợ cho con người thực hiện được các tác vụ của họ với tốc độ nhanh nhất, xử lý được nhiều tác vụ nhất và độ chính xác là cao nhất.\nKết luận Trong thời đại của sự bùng nổ công nghệ AI, các mô hình ngôn ngữ lớn (LLM) và AI agent đang dần định hình lại cách thức hoạt động của ngành tài chính và đầu tư. Những công cụ này không chỉ đơn thuần là phương tiện tự động hóa các tác vụ lặp đi lặp lại, mà còn mở ra một kỷ nguyên mới trong phân tích tài chính—nơi phân tích định tính được thực hiện với tốc độ, quy mô và độ chính xác chưa từng có.\nTại miquant, chúng tôi tin rằng tương lai của đầu tư tài chính sẽ là sự kết hợp hài hòa giữa sức mạnh của AI và kinh nghiệm của con người. LLM và AI agent sẽ tiếp tục phát triển, trở thành những công cụ không thể thiếu cho các nhà đầu tư hiện đại, giúp họ đưa ra quyết định sáng suốt hơn trong một thị trường ngày càng phức tạp và biến động.\nTuy nhiên, đừng quên rằng, dù công nghệ có tiên tiến đến đâu, trí tuệ, kinh nghiệm và khả năng phán đoán của con người vẫn là yếu tố quyết định cuối cùng trong hành trình đầu tư thành công. Những công cụ AI chỉ thực sự phát huy giá trị khi được sử dụng như một phần bổ sung cho kiến thức và kỹ năng của nhà đầu tư, chứ không phải thay thế hoàn toàn vai trò của họ.\nTrong các bài viết tiếp theo của chuỗi series này, chúng tôi sẽ đi sâu hơn vào những ứng dụng cụ thể của LLM và AI agent trong đầu tư, chia sẻ những nghiên cứu và kinh nghiệm thực tế từ miquant, đồng thời mang đến những góc nhìn mới về tương lai của ngành tài chính trong kỷ nguyên trí tuệ nhân tạo.\n","permalink":"http://localhost:1313/posts/2025/2025-03-03-llm-concept-and-application/","summary":"Miquant khám phá cơ chế của các mô hình ngôn ngữ lớn (LLM) và tác động của AI trong đầu tư, nhấn mạnh khả năng tự động hóa phân tích định tính, cải thiện hiệu suất và độ chính xác trong các tác vụ tài chính, đồng thời khẳng định vai trò quan trọng của con người trong quá trình ra quyết định.","title":"AI - Người làm việc không ngừng nghỉ"},{"content":" Bài viết này thử nghiệm tối ưu hoá danh mục sử dụng Large Language Model (LLM). Thư viện langchain và OpenAI gpt4o và 4o-mini sẽ được sử dụng cho thử nghiệm.\nConcept Tối ưu hoá danh mục Tối ưu hóa danh mục đầu tư là quá trình phân bổ tài sản vào các khoản đầu tư khác nhau nhằm đạt được sự cân bằng tối ưu giữa lợi nhuận kỳ vọng và rủi ro. Các phương pháp tối ưu hóa truyền thống, như Modern Portfolio Theory hay Mean-variance optimization, chủ yếu dựa vào dữ liệu lịch sử và các giả định về phân phối lợi nhuận. Tuy nhiên, chúng có một số hạn chế:\nKhó khăn trong việc tích hợp các yếu tố định tính: Các phương pháp này thường không xem xét các yếu tố định tính như tin tức thị trường, sự kiện kinh tế hoặc quan điểm của chuyên gia, tiềm năng từ câu chuyện kinh doanh dẫn đến việc thiếu linh hoạt trong phản ứng với các biến động thị trường. Giả định đơn giản hóa: Việc giả định rằng lợi nhuận tuân theo phân phối chuẩn và mối quan hệ giữa các tài sản là tuyến tính có thể không phản ánh chính xác thực tế phức tạp của thị trường. LLM là gì? Mô hình ngôn ngữ lớn (LLM) là các mô hình học sâu được huấn luyện trên khối lượng dữ liệu văn bản khổng lồ, giúp chúng có khả năng hiểu và tạo ra ngôn ngữ tự nhiên. Với khả năng đọc hiểu ngôn ngữ này, LLM có thể trích xuất thông tin và phân tích dữ liệu phi cấu trúc nhanh chóng và đơn giản hơn nhiều so với các phương pháp truyền thống. Ví dụ, LLM có thể đọc 10 bài báo gần đây và tổng hợp ra 3 luận điểm đầu tư phổ biến nhất từ các bài báo đó—một nhiệm vụ mà các phương pháp truyền thống khó thực hiện được.\nTản mạn: một cách khoa học mà nói, chúng ta có thể \u0026ldquo;hình dung\u0026rdquo; LLM như sau: LLM là một cỗ máy đưa ra kết quả dựa trên trường hợp có xác suất cao nhất. Các câu prompt đóng vai trò giới hạn phạm vi lựa chọn của LLM, từ đó tăng xác suất cho kết quả mong muốn.\nLLM cho tối ưu hoá danh mục Với khả năng phân tích và tận dụng các dữ liệu phi cấu trúc trên, LLM là một ứng cử viên sáng giá cho quá trình tối ưu hoá danh mục khi có thể tận dụng được: (1) dữ liệu có cấu trúc, thống kê lợi nhuận rủi ro; (2) dữ liệu phi cấu trúc như tiềm năng tăng trưởng công ty, …; (3) đáp ứng được các yêu cầu về mục tiêu đầu tư (objective), hay các rằng buộc đi kèm (constraints) một các nhanh chóng.\nĐể dễ dàng hình dung, chúng ta sẽ cùng nhau đi qua thử nghiệm thực tế cho 6 cổ phiếu FPT, HPG, MWG, REE, VCB và VNM!\nXây dựng danh mục đầu tư sử dụng LLM Quy trình tối ưu sẽ nhau sau\nB1: Xác định rổ cổ phiếu đầu tư B2: Thu thập thông tin: (1) Sentiment của cổ phiếu; (2) Các luận điểm đầu tư - rủi ro của cổ phiếu; (3) tín hiệu kĩ thuật; (4) Độ tương quan giữa các cổ phiếu với nhau B3: Tối ưu hoá danh mục: (1) Các mục tiêu đầu tư cũng như giới hạn cho phép; (2) Xem xét danh mục quá khứ và hiệu suất quý gần nhất. B4: Backtest Thu thập tin tức từ google Tại bước đầu tiên, chúng ta cần xây dựng một pipeline để thu thập được các tin tức liên quan đến từng cổ phiếu, và phải phù hợp với thời gian của giai đoạn backtest. Tại khâu này, chúng ta sẽ sử dụng thư viện langchain để xây dựng pipeline và mô hình gpt-4o-mini để xử lý tin tức\n1 2 3 4 5 6 7 8 9 10 11 def load_google_news_before(query: str, before_date: datetime.date, num_results: int = 30): \u0026#34;\u0026#34;\u0026#34; Query Google News via LangChain’s Google Serper API Wrapper, filtering articles so that only those published before the given date are returned. \u0026#34;\u0026#34;\u0026#34; cd_min = convert_date_to_str(before_date - datetime.timedelta(days=90)) cd_max = convert_date_to_str(before_date) tbs = f\u0026#34;cdr:1,cd_min:{cd_min},cd_max:{cd_max}\u0026#34; search = GoogleSerperAPIWrapper(type=\u0026#34;news\u0026#34;, tbs=tbs, gl=\u0026#34;vn\u0026#34;,hl=\u0026#39;vi\u0026#39;,k=num_results) results = search.results(query) return results Ta có thể dễ dàng tạo ra một hàm đọc tin tức sử dụng Serper và thư viện langchain. Hàm trên thiết kế để lấy các tin tức trong vào 90 ngày trước ngày truyền vào.\nTrích xuất các thông tin mong muốn Để thu thập thông tin phục vụ đầu tư, ta tìm kiếm các bài báo với từ khóa \u0026ldquo;đầu tư vào cổ phiếu ABC\u0026rdquo;, trong đó ABC là mã cổ phiếu cần phân tích.\nSau khi có được các bài báo liên quan, ta thu thập các thông tin quan trọng bao gồm: (1) sentiment của cổ phiếu trong quý vừa qua; (2) lý giải cho kết quả sentiment đó; (3) các luận điểm đầu tư (Catalyst); (4) các yếu tố rủi ro.\nĐể thực hiện việc này, ta thiết kế câu prompt dựa trên các yêu cầu trên và sử dụng mô hình LLM gpt-4o-mini. Vì nhiệm vụ này chủ yếu là tổng hợp thông tin, không đòi hỏi nhiều suy luận phức tạp, nên việc sử dụng một mô hình nhỏ gọn, nhanh và tiết kiệm như gpt-4o-mini là lựa chọn phù hợp.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 ## Sử dụng pydantic để ép model trả về dạng dữ liệu như mong muốn class Sentiment(BaseModel): sentiment: str = Field(default=None, description=\u0026#34;The financial market sentiment of all news\u0026#34;, enum=[\u0026#34;bearish\u0026#34;, \u0026#34;bullish\u0026#34;, \u0026#34;neutral\u0026#34;]) explanation: str = Field(default=None, description=\u0026#34;The short explanation about the chosen sentiment \u0026#34;) catalyst: List[str] = Field(default=None, description=\u0026#34;The catalyst based on all news\u0026#34;) risk_factor: List[str] = Field(default=None, description=\u0026#34;The risk factor based on all news\u0026#34;) def extract_key_info(stock , date): \u0026#34;\u0026#34;\u0026#34; Extract key information from a news article. \u0026#34;\u0026#34;\u0026#34; nest_asyncio.apply() query = f\u0026#34;đầu tư cổ phiếu {stock}\u0026#34; # For example, retrieve articles published before January 1, 2025. cutoff_date = date articles = load_google_news_before(query, cutoff_date, num_results=15) links = pd.DataFrame(articles[\u0026#39;news\u0026#39;])[\u0026#39;link\u0026#39;].to_list() ## drop the link from https://etime.danviet.vn links = [link for link in links if \u0026#39;danviet.vn\u0026#39; not in link] ## skip for danviet.vn as it block the request loader = WebBaseLoader(links, continue_on_failure=True) loader.requests_per_second = 0.5 docs = loader.aload() ls = [] for doc in docs: ls.append(doc.page_content.replace(\u0026#39;\\n\\n\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;\\n\u0026#39;, \u0026#39; \u0026#39;)) llm = ChatOpenAI(model=\u0026#39;gpt-4o-mini\u0026#39;) prompt = \u0026#34;\u0026#34;\u0026#34; You\u0026#39;re a senior analyst. You are analyzing stock {stock} Your tasks are: - Analyze the sentiment of the given news - Explain the sentiment in few words - Identify the key catalysts or drivers - Identify the risk factors {document} Requirement - Be specific and concise - Do not make up information or make assumption - Do not use external source \u0026#34;\u0026#34;\u0026#34;.format(stock = stock,document=ls) response = llm.with_structured_output(Sentiment).invoke(prompt) return response date = datetime.date(2024, 1, 1) extract_key_info(\u0026#39;HPG\u0026#39;, date) Kết quả của hàm trên sẽ như sau:\nSentiment(sentiment=\u0026lsquo;bearish\u0026rsquo;, explanation=\u0026lsquo;Sentiment is bearish due to significant losses reported by HPG and related companies, along with ongoing challenges in the steel market.\u0026rsquo;, catalyst=[\u0026lsquo;Increased production and sales in October 2023\u0026rsquo;, \u0026lsquo;Expected recovery in demand and prices for steel\u0026rsquo;, \u0026lsquo;Completion of Dung Quất 2 project may increase capacity\u0026rsquo;], risk_factor=[\u0026lsquo;High competition in the steel industry\u0026rsquo;, \u0026lsquo;Dependence on domestic market recovery\u0026rsquo;, \u0026lsquo;Fluctuations in raw material prices\u0026rsquo;])\nTối ưu hoá danh mục Khi tối ưu hoá danh mục, việc xây dựng câu prompt chi tiết và cụ thể sẽ giúp giảm thiểu lỗi và tạo ra tỷ trọng hợp lý hơn. Do độ phức tạp tại khâu này tăng cao, chúng ta cần sử dụng một mô hình LLM mạnh mẽ hơn. Bài viết chọn sử dụng gpt-4o.\nMục tiêu đầu tư: Phân bổ tỷ trọng cho các cổ phiếu có tiềm năng tăng trưởng cao, dựa trên tín hiệu kỹ thuật, sentiment và các luận điểm đầu tư.\nCác rằng buộc:\nLong-only portfolio Không sử dụng margin Hold trong 3 tháng Không cần đầu tư hết 6 cổ phiếu nhưng phải có ít nhất 4 cổ phiếu Ngoài ra, câu prompt cũng bao gồm một workflow (quy trình) để LLM có thể dựa vào đó để đưa ra quyết định phân bổ tỷ trọng\nDựa trên các luận điểm đầu tư để xác định các cổ phiếu tiềm năng Đánh giá sentiment của các cổ phiếu đã chọn Sử dụng tín hiệu kỹ thuật để xác nhận thêm tiềm năng của các cổ phiếu Cuối cùng, sử dụng dữ liệu rủi ro-lợi nhuận của 3 tháng qua để xác định các cổ phiếu được mua quá mức hoặc bị thổi phồng, tránh phân bổ quá nhiều vốn cho chúng. Tuy nhiên, nếu luận điểm đầu tư hứa hẹn, ta vẫn có thể phân bổ tiền cho cổ phiếu đó nếu tin rằng nó có tiềm năng đáng kể Chia tỷ trọng cho từng cổ phiếu dựa trên tiềm năng của các luận điểm đầu tư Phân bổ nhiều tỷ trọng hơn cho các cổ phiếu có luận điểm đầu tư khả thi và tiềm năng cao 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 prompt = \u0026#34;\u0026#34;\u0026#34; Create a portfolio with the following stocks: {stocks} Objective: - Use the technical signal, market sentiment and catalysts to assign weight to each stock for a long-only portfolio - The portfolio aim to invest in the stock with the highest potential return Constraints: - The portfolio should be well diversified - You can invest in each stock with a maximum of 0.5 of the portfolio - You do not need to invest in all stocks, but you must invest in at least 4 stocks. Only select the most promising stocks. - There is no leverage \u0026amp; short selling - The holding period is 3 months (1 quarter) - The weight should be a float number between 0 and 1 - The sum of all weights should be 1 Workflow: 1. Analyze information first. 2. Based on the catalysts, identify the stocks that are likely to perform well. 3. Assess the sentiment of the news to confirm the potential of the selected stocks. 4. Use the technical signal to further confirm the potential of the stocks. 5. Finally, use the past 3 months\u0026#39; risk-return data to identify overbought or hyped stocks to avoid allocating too much capital to them. However, if the catalyst is highly promising, you may still allocate funds to it if you believe it has significant potential. 6. Assign weights to each stock based on the potential return driven by the catalyst. 7. Allocate more weight to stocks where the catalyst is more likely to occur and has a higher magnitude. 8. Be confident in assigning higher weights to stocks with better catalysts and sentiment. 9. **Think carefully; do not simply split the weights equally.** Technical signal: simple EMA50-200 crossover, 1 for bullish, 0 for bearish {ta_signal} Past 3 risk return {risk_return} News sentiment (0 for bearish, 1 for bullish, 2 for neutral) \u0026amp; Catalysts {sentiment} Correlation between stocks {correlation} Your previous portfolio {past_portfolios} Your previous return: {last_period_return} Output - A list with weight for every stock. The weight must correspond to the oder {stocks}. For stock that not invest, simple return 0 for the weight - A detailed explanation of the rationale behind the portfolio allocation \u0026#34;\u0026#34;\u0026#34; Sau đó, ta sẽ tính toán các tham số và đưa các thông tin cần thiết vào trong mô hình để mô hình thực hiện và tính toán.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class PortfolioOptimizationResult(BaseModel): \u0026#34;\u0026#34;\u0026#34; A class to represent the result of portfolio optimization. \u0026#34;\u0026#34;\u0026#34; stock_weight: List[float] = Field( default=None, description=\u0026#34;A list containing the weight for all stocks in the portfolio.\u0026#34; ) explanation: str = Field( default=None, description=\u0026#34;A detailed explanation of the rationale behind the portfolio allocation.\u0026#34; ) def llm_optimizer(date, stocks = [\u0026#39;FPT\u0026#39;, \u0026#39;HPG\u0026#39;, \u0026#39;MWG\u0026#39;, \u0026#39;REE\u0026#39;, \u0026#39;VCB\u0026#39;, \u0026#39;VNM\u0026#39;], past_portfolios = None, last_period_return = None): results = run_in_parallel(stocks, date) llm = ChatOpenAI(model=\u0026#39;gpt-4o\u0026#39;) ta_signal = ta_compare[:date].iloc[-1] start_sample = date - datetime.timedelta(days=30*3) ## 3 months sample_rets = rets[start_sample:date] risk_return = ((1+sample_rets).product()-1).to_frame() risk_return.columns = [\u0026#39;past_3m_rets\u0026#39;] risk_return[\u0026#39;past_3m_volatility\u0026#39;] = sample_rets.std() risk_return[\u0026#39;past_3m_sharpe_ratio\u0026#39;] = sample_rets.mean()/risk_return[\u0026#39;past_3m_volatility\u0026#39;] risk_return = risk_return.round(4) correlation = sample_rets.corr().round(4) chatprompt = prompt.format(ta_signal=ta_signal, sentiment=results, risk_return=risk_return, stocks=stocks, correlation=correlation, past_portfolios=past_portfolios, last_period_return=last_period_return) optimizer = llm.with_structured_output(PortfolioOptimizationResult).invoke(chatprompt) weight = pd.DataFrame(optimizer.stock_weight, index=stocks, columns=[\u0026#39;weights\u0026#39;]) weight = weight/weight.sum() ## make sure the sum of weight is 1 retionale = optimizer.explanation return weight, retionale weight, expla = llm_optimizer(datetime.date(2024, 6, 1)) ## this takes around 35 seconds Sau khi chạy hàm trên, ta sẽ nhận được kết quả như sau (danh mục cho tháng 6.2024):\nTỷ trọng danh mục\nweights FPT 0.25 HPG 0.2 MWG 0.3 REE 0.15 VCB 0.1 VNM 0.0 Lý do cho tỷ trọng trên\n💡 In constructing this portfolio, we focused on stocks with strong bullish technical signals and favorable market sentiment, while considering catalysts and risks.\nFPT (Weight: 0.25): FPT has a bullish technical signal, strong sentiment driven by its partnership with NVIDIA for AI development, and projected revenue growth. Although its valuation is high, its catalysts are promising, warranting a significant allocation. HPG (Weight: 0.2): HPG also exhibits a bullish technical signal and optimistic market sentiment due to strong Q1 financial results and upcoming projects like Dung Quất 2. Despite high debt levels, its growth potential justifies a substantial allocation. MWG (Weight: 0.3): MWG has demonstrated strong past returns and a bullish sentiment, supported by strategic investments and projected growth. The high weight reflects its potential for recovery and profit, despite some risks. REE (Weight: 0.15): REE\u0026rsquo;s neutral sentiment and mixed outlook are balanced by its dividend plans and growth in renewable energy. Its lower weight reflects these uncertainties, but its diversification benefits and potential in energy justify inclusion. VCB (Weight: 0.1): Despite a neutral sentiment and recent negative returns, VCB\u0026rsquo;s catalysts such as planned capital increases and potential equity sales offer upside potential, meriting a smaller allocation. VNM (Weight: 0): VNM is excluded due to bearish sentiment and negative past performance, with significant foreign sell-offs and declining market position, making it less attractive for the portfolio. Overall, this portfolio is designed to be diversified and capitalize on stocks with strong potential returns driven by clear catalysts, while mitigating risks associated with each stock\u0026rsquo;s market position and sentiment.\nBacktest cho phương pháp trên Ta tiến hành xây dựng mô hình backtest đơn giản.\nMua danh mục vào đầu mỗi quý Bỏ qua các yếu tố như phí giao dịch, market impact 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 ## backtest the strategy - \u0026gt;=35s for 1 loop -\u0026gt; 4 years = 4*4*35s \u0026gt;= 560s ~ 9.3 minutes ## create a function that takes the weights and returns the portfolio returns def backtest_strategy(weights, returns): \u0026#34;\u0026#34;\u0026#34; Backtest the strategy \u0026#34;\u0026#34;\u0026#34; port_rets = returns @ weights port_rets.rename(columns={\u0026#39;weights\u0026#39;:\u0026#39;rets\u0026#39;}, inplace=True) return port_rets date_idx = pd.date_range(\u0026#39;2020-12-31\u0026#39;, \u0026#39;2024-12-31\u0026#39;, freq=\u0026#39;QE\u0026#39;) llm_rets = pd.DataFrame() llm_retionale = {} llm_weights = {} for date in date_idx[:-1]: i = 0 start_sample = date - pd.DateOffset(years=3) end_sample = date start_out_sample = date+ pd.DateOffset(days=1) end_out_sample = date + pd.DateOffset(months=3) ## data for the sample period sample_data = rets[start_sample:end_sample] out_sample = rets[start_out_sample:end_out_sample] # run the optimizer if i \u0026gt;=1: past_port = pd.DataFrame(llm_weights[previous_date.strftime(\u0026#34;%Y-%m-%d\u0026#34;)]) w, explanation = llm_optimizer(date, past_portfolios=past_port, last_period_return= past_return) else: w, explanation = llm_optimizer(date) llm_retionale[date.strftime(\u0026#34;%Y-%m-%d\u0026#34;)] = explanation llm_weights[date.strftime(\u0026#34;%Y-%m-%d\u0026#34;)] = w.to_dict()[\u0026#34;weights\u0026#34;] # run the backtest llm_backtest = backtest_strategy(w, out_sample) llm_rets = pd.concat([llm_rets, llm_backtest], axis=0) i+=1 previous_date = date past_return = (1+llm_backtest).product()-1 Từ đó, ta thu được kết quả sau:\nLợi nhuận danh mục từ 2021 tới nay\nThống kê liên quan\nLLM VNINDEX Lợi nhuận tích luỹ 70.9% 12.6% Lợi nhuận trung bình hàng năm 14.52% 3.04% Độ biến động trung bình hằng năm 21.66% 19.6% Sharpe ratio 0.73 0.25 Lần sụt giảm mạnh nhất 24.19% 40.34% Chiến lược sử dụng LLM cho kết quả tương đối tốt với lợi nhuận tích luỹ 70.9% so với 12.6% của vnindex qua 4 năm. Tuy nhiên, tỷ lệ sharpe thì tương đối khiêm tốn chỉ 0.73 với độ biến động 21.6%. Ngoài ra lần sụt giảm lớn nhất đạt 24.19% là một kết quả chấp nhận được trong đầu tư.\nDanh mục qua từng giai đoạn\nKết luận Bài viết đã tiến hành xây dựng tối ưu hoá danh mục sử dụng LLM. Thông qua LLM, ta có thể đưa các dữ liệu phi cấu trúc (unstructured-data) vào trong quá trình tối ưu. Đây là một điểm thú vị và các phương pháp truyền thống tận dụng các tính chất thống kê chưa đưa vào được. Tuy nhiên, phải nhấn mạnh lại rằng, tối ưu danh mục theo phương pháp này thì mang nặng yếu tố định tính (qualitative) hơn so với định lượng (quantitative).\nMột điểm thú vị này, ta có thể kết hợp 2 phương pháp này với nhau. Thông qua LLM, ta có thể tìm ra một prior belief mang tính định tính và dùng nó để bắt đầu quá trình tối ưu hoá định lượng hơn (bayesian style).\nBài viết sau sẽ tiến hành so sánh các phương pháp truyền thống với các phương pháp mới này. Để nhận file code và data đầy đủ, xin hãy để comment vào bài viết trên linkedin hoặc gửi email đến hung.ha@miquant.vn.\n","permalink":"http://localhost:1313/posts/2025/2025-02-03-llm-portfolio-optimization/","summary":"Trong kỷ nguyên mới, dữ liệu phi cấu trúc như tin tức, báo cáo phân tích, và thống kê kinh tế trở thành nguồn thông tin quan trọng trong quyết định đầu tư. Tuy nhiên, các mô hình tài chính truyền thống như Lý thuyết Danh mục Hiện đại (MPT) và Tối ưu Rủi ro-Lợi nhuận (Mean-Variance Optimization) chủ yếu dựa vào dữ liệu định lượng, bỏ qua yếu tố định tính quan trọng. Sự phát triển của Large Language Model (LLM) mang đến tiềm năng kết hợp dữ liệu định lượng và dữ liệu phi cấu trúc trong quản lý danh mục đầu tư. Bài viết này trình bày một thực nghiệm ứng dụng LLM trong quá trình tối ưu danh mục.","title":"Tối ưu hoá danh mục thông qua LLM"},{"content":" Concept AI Agent, một topic hiện đang rất hot trên thế giới về lĩnh vực AI.\nTheo fpt.ai:\nAI Agent là một hệ thống phần mềm hoặc phần cứng được thiết kế để thực hiện các nhiệm vụ một cách tự động và độc lập, nhằm đạt được những mục tiêu nhất định.\nMột AI Agent có khả năng xử lý thông tin, đưa ra quyết định và thực hiện các hành động để tương tác với các điều kiện và hệ thống liên quan. Nhân sự AI tạo sinh có thể áp dụng cho nhiều lĩnh vực, từ trợ lý ảo cho khách hàng đến hệ thống điều khiển tự động phức tạp, thậm chí là robot trong các môi trường thực tế.\nTheo IBM:\nAn artificial intelligence (AI) agent refers to a system or program that is capable of autonomously performing tasks on behalf of a user or another system by designing its workflow and utilizing available tools.\nTóm gọn lại: AI agent là một hệ thống thông minh và autonomous (tự trị → tự suy nghĩ, suy luận, ra quyết định,…). Mỗi agent có thể thực hiện được các task mà người thiết kế giao. Và một điểm đặc biệt so với chatbot thông thường là người thiết kế cho thể đưa cho Agent các tools để tự sử dụng và các Agent phối hợp với nhau để hoàn thành mục tiêu cuối cùng.\nVí dụ, trong đầu tư, ngày xưa bạn có tool lấy giữ liệu từ database lên. Bạn sử dụng nó và tool trả ra output, bạn dùng output này cho vô AI. Giờ thì đã khác, bạn có thể đưa sẵn tool cho Agent, và các Agent này sẽ sử dụng chính cái tool này để thực hiện các task cần thiết, loại bỏ bạn ra trong quá trình “tự trị” của nó.\nNếu bạn chưa hiểu thì hãy tưởng tượng như sau.\nBạn là 1 financial analysis, trong quá trình phân tích, bạn sẽ luôn follow 1 số quy trình nhất định và sử dụng 1 vài tools nhất định.\nB1: lên google search cổ phiếu HPG.\nTại bước này, bạn sử dụng 1 tools là google_search\nB2: download các bài báo về, lưu xuống database, sử dụng 1 tool sentiment calculator để tính điểm các bài báo này.\nTại bước này, bạn sử dụng 1 tools là sentiment_calculator\nB3: Viết report theo 1 khung nhất định (Thân bài, mở bài, kết bài) đi kèm các tool khác như: vẽ chart correlation với thị trường, tính toán các chỉ số tài chính ,..\nTại bước này, bạn sử dụng 2 tools là report_writing và metrics_calculator.\nVà, điều thú vị ở đây là, bạn chỉ cần chuẩn hoá các tools trên như là 1 function trong python và đưa cho Agent. Các Agent đã có thể thực hiện từ bước 1 đến bước 3 mà không cần bạn can thiệp vào bước nào. Ngoài ra, thông qua cơ chế Specialization và Multi-Agent system, bạn còn tăng được độ chính xác, hiệu quả và khả năng thích ứng của hệ thống tự trị này (fpt.ai).\nVà điều thú vị hơn nữa, là làm những thứ này, rất đơn giản, thông qua kĩ thuật prompt engineering.\nXây dựng một hệ thống phân tích kĩ thuật, thông tin và tư vấn chiến lược đầu tư. Hãy cùng nhau thử nghiệm đi xây dựng một team phân tích tư vấn như một phòng trong công ty chứng khoán nhé. Lưu ý, phòng tư vấn này “tự trị” - autonomous. Việc cần làm là đưa vào 1 mã cổ phiếu (ví dụ HPG), các AI Agent sẽ tự xử lý 100%.\nPhòng tư vấn của ctck có mô hình tổ chức đơn giản nhất như sau:\n1 bạn phân tích kĩ thuật (technical analyst) 1 bạn phân tích thông tin, báo (news analyst) 1 bạn tư vấn đầu tư (investment advisor) 1 bạn xây dựng chiến lược giao dịch (strategy advisor) Cùng nhau xây phòng tư vấn nào. Trước tiên, để thực hiện các nghiệp vụ trên, ta cần các công cụ sau:\nLấy giá cổ phiếu, tính các chỉ báo kĩ thuật Lên google search và thu thập các thông tin liên quan đến cổ phiếu ta muốn 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 ## Mình sử dụng gpt-4o-mini là model llm bên dưới llm = ChatOpenAI(model=\u0026#34;gpt-4o-mini\u0026#34;) ## Tools def load_price_and_ta(company_stock): query = f\u0026#34;SELECT * FROM eod WHERE symbol = \u0026#39;{company_stock}\u0026#39; order by timestamp DESC LIMIT 1000\u0026#34; host = \u0026#39;xyz\u0026#39; (database link) response = requests.get( host + \u0026#39;/exec\u0026#39;, params={\u0026#39;query\u0026#39;: query}).json() df = pd.DataFrame(response[\u0026#39;dataset\u0026#39;], columns=pd.DataFrame(response[\u0026#39;columns\u0026#39;])[\u0026#39;name\u0026#39;].values) df.set_index(\u0026#39;timestamp\u0026#39;, inplace=True) df.index = pd.to_datetime(df.index).strftime(\u0026#39;%Y-%m-%d\u0026#39;) df = df.iloc[::-1] ## indicator df[\u0026#39;RSI\u0026#39;] = ta.rsi(df[\u0026#39;close\u0026#39;]) adx = ta.adx(df[\u0026#39;high\u0026#39;], df[\u0026#39;low\u0026#39;], df[\u0026#39;close\u0026#39;], 14) df[adx.columns] = adx bbands = ta.bbands(df[\u0026#39;close\u0026#39;]) df[bbands.columns] = bbands df[\u0026#39;SMA200\u0026#39;] = ta.sma(df[\u0026#39;close\u0026#39;], 200) df[\u0026#39;SMA50\u0026#39;] = ta.sma(df[\u0026#39;close\u0026#39;], 50) df[\u0026#39;SMA20\u0026#39;] = ta.sma(df[\u0026#39;close\u0026#39;], 20) macd = ta.macd(df[\u0026#39;close\u0026#39;]) df[macd.columns] = macd return df.dropna() ## Tool lấy giá và tính các indicator ta_tool = Tool( name = \u0026#34;Price and technical indicator tools for Vietnam stock\u0026#34;, description = \u0026#34;Collect stocks prices adn technical indicator for {company_stock} about a specific company in Vietnam stock market.\u0026#34;, func= lambda symbol: load_price_and_ta(symbol) ) ## Tool search web search_tool = SerperDevTool( country=\u0026#34;vn\u0026#34;,locale=\u0026#34;vn\u0026#34;,location=\u0026#34;Vietnam\u0026#34;, n_results=20, ) ## Tool scraping scrape_tool = ScrapeWebsiteTool() Sau đó, chúng ta sẽ tiến hành đi xây dựng các nhân sự.\nNhân sự AI Bài viết sử dụng framework Crewai nhằm xây dựng các agent. Để xây dựng 1 agent, ta cần các thông tin sau:\nRole: vai trò của agent này là gì? Goal: mục tiêu của agent này là gì? Backstory: câu chuyện về agent này là gì? Bạn có thể đọc thêm tại Crew.ai\nTechnical analyst 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## Chuyên viên phân tích kĩ thuật ## Phân tích TA, dự đoán giá trong 20 ngày tới technicalAnalyst = Agent( role= \u0026#34;Senior Technical Analyst\u0026#34;, goal=\u0026#34;\u0026#34;\u0026#34;Provide the most recent technical analysis about trend, momentum and stockprice prediction in short timeframe (20 days) for targeted stock\u0026#34;\u0026#34;\u0026#34;, backstory=\u0026#34;\u0026#34;\u0026#34;You\u0026#39;re the best at technical analysis, stock price trend prediction and market risk assessment. and you\u0026#39;re working for a top security firm in Vietnam.\u0026#34;\u0026#34;\u0026#34;, verbose=True, tools=[ta_tool], allow_delegation=True, llm = llm ) News analyst 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## chuyên viên phân tích thông tin ## tìm catalyst, phân tích các yếu tố rủi ro newsAnalyst = Agent( role= \u0026#34;Senior Market Analyst\u0026#34;, goal=\u0026#34;\u0026#34;\u0026#34;Provide the most recent news and market sentiment analysis that can highly affect stock price performance. Also, you must provide the top catalyst as well as risk factors that can highly affect company business performance.\u0026#34;\u0026#34;\u0026#34;, backstory= \u0026#34;\u0026#34;\u0026#34; You\u0026#39;re highly experienced in analyzing the market information and risk assessment that can affect company revenue. You are also excel in analyzing market sentiment and always skepticism and consider also the source of the news articles. \u0026#34;\u0026#34;\u0026#34;, verbose=True, tools=[search_tool,scrape_tool], allow_delegation=True, llm=llm, ) Investment advisor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## chuyên viên tư vấn đầu tư ## Phân tích dựa vào 2 nhân tố giá và thông tin để đưa ra tư vấn đầu tư investmentAdvisor = Agent( role= \u0026#34;Senior Investment Advisor\u0026#34;, goal=\u0026#34;\u0026#34;\u0026#34;Impress your customers with full analyses over stocks based on technical, and news insights and complete investment recommendations. All the information must come from the technicalAnalyst and newsAnalyst.\u0026#34;\u0026#34;\u0026#34;, backstory= \u0026#34;\u0026#34;\u0026#34; You\u0026#39;re the best at providing investment advice and risk management for the targeted stock. You\u0026#39;re also excel in risk management and always consider the risk factors that can affect the investment performance. You are now working for a super important customer you need to impress. \u0026#34;\u0026#34;\u0026#34;, verbose=True, allow_delegation=True, tools=[search_tool, scrape_tool], llm=llm, ) Strategy builder 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## chuyên viên xây dựng chiến lược ## chiến lược mua hợp lý tối ưu hoá lợi nhuận tradingAdvisor = Agent( role=\u0026#34;Trade Advisor\u0026#34;, goal=\u0026#34;\u0026#34;\u0026#34;Develop and test various trading strategies based on the technicalAnalyst and investmentAdvisor\u0026#39;s recommendation. Finally, you must provide the best trading strategy to maximize profit\u0026#34;\u0026#34;\u0026#34;, backstory=\u0026#34;This agent specializes in analyzing the timing, price, \u0026#34; \u0026#34;and logistical details of potential trades. By evaluating \u0026#34; \u0026#34;these factors, it provides well-founded suggestions for \u0026#34; \u0026#34;when and how trades should be executed to maximize \u0026#34; \u0026#34;efficiency and adherence to strategy.\u0026#34;, verbose=True, allow_delegation=True, tools = [ta_tool], llm=llm, ) Ta đã có đủ nhân sự rồi, giờ chúng ta sẽ xây dựng các “nhiệm vụ” để hoàn thành được công việc.\nQuy trình sẽ như sau:\nPhân tích giá cổ phiếu, TA Phân tích thông tin, tìm kiếm catalyst, phân tích các rủi ro có thể có Tổng hợp và đưa ra khuyến nghị Xây dựng chiến lược mua bán hợp lý Nhiệm vụ Phân tích giá cổ phiếu, TA 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ## phân tích TA, kháng cự, hỗ trợ ## tìm hiểu xem cổ phiếu quá mua/bán chưa getStockPrice = Task( description= \u0026#34;\u0026#34;\u0026#34; Conduct a technical analysis of {company_stock} stock price trends and patterns, resistance and support level utilizing technical indicators that have provided. Considering whether the stock is overbought or oversold. \u0026#34;\u0026#34;\u0026#34;, expected_output = \u0026#34;\u0026#34;\u0026#34; The final report must include a detailed analysis of the stock\u0026#39;s price trends, key technical indicators, support-resistance level, and any potential buy/sell signals. Make sure to provide a clear and concise summary of the stock\u0026#39;s current technical position and any potential price targets or risk levels. Make sure to use the most recent data possible. Finally, please translate the final output to Vietnamese. \u0026#34;\u0026#34;\u0026#34;, agent= technicalAnalyst, output_file = \u0026#34;TA_analyses.md\u0026#34; ) Phân tích thông tin, tìm kiếm catalyst, phân tích các rủi ro có thể có 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## phân tích các thông tin gần nhất ## xem xét coi có sự kiện gì sắp xảy ra không ## tâm lý thị trường đang như thế nào, các analyst đang nghĩ gì get_news = Task( description= \u0026#34;\u0026#34;\u0026#34; Collect and summarize recent news articles, press releases, and market analyses related to the {company_stock} stock and its industry. Pay special attention to any significant events, market sentiments, and analysts\u0026#39; opinions. Also include upcoming events like earnings and others.\u0026#34;\u0026#34;\u0026#34;, expected_output = \u0026#34;\u0026#34;\u0026#34; A summary of the overall market and short summary. Include a sentiment score for targeted stock based on the news, notable shifts in market sentiment, and potential impacts for the stock. Make sure to use the most recent data as possible. Finally, please translate the final output to Vietnamese. \u0026#34;\u0026#34;\u0026#34;, agent= newsAnalyst, output_file = \u0026#34;news_analyses.md\u0026#34; ) Tổng hợp và đưa ra khuyến nghị 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## tổng hợp thông tin và đưa ra quyết định có nên tư vấn không writeAnalyses = Task( description = \u0026#34;\u0026#34;\u0026#34;Use the stock price analysis and the news analysis to create an report and investment advice about the {company_stock} company. Focus on the stock price trend, news and market sentiment. What are the near future considerations? Include the previous analyses of stock trend and news summary. Be straight foward and cite or provide reasons for each of your point.\u0026#34;\u0026#34;\u0026#34;, expected_output= \u0026#34;\u0026#34;\u0026#34; A research report of the {company_stock} formated as markdown in an easy readable manner. It should contain: - An overall summary - A detailed analysis of the stock price trend from the Technical Analyst - A detailed analysis of the stock news from the News Analyst - Key catalysts and risks for the stock - Price prediction for the stock in future and does it over or under value with the current price based on the catalysts and risks - A conclusion about key facts and concrete future trend prediction - up, down or sideways. Will it worth to buy or sell the stock when take into account the risk and catalysts. Finally, please translate the final output to Vietnamese. \u0026#34;\u0026#34;\u0026#34;, agent = investmentAdvisor, output_file = \u0026#34;final_report.md\u0026#34; ) Xây dựng chiến lược mua bán hợp lý 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## xây dựng chiến lược mua khi cổ phiếu quá bán ## bán khi cổ phiếu có sentiment lên quá cao tradingStrategy = Task( description= \u0026#34;\u0026#34;\u0026#34;Develop and refine trading strategies based on the insights from the technical analyst and investment advisor. The objective strategy is buy when the stock is oversold and sell when the sentiment is too high. Also, consider the risk factors that can affect the stock price based on the news analysis \u0026#34;\u0026#34;\u0026#34;, expected_output= \u0026#34;\u0026#34;\u0026#34;A set of potential trading strategies for {company_stock} that align with the objective startegy. Finally, please translate the final output to Vietnamese.\u0026#34;\u0026#34;\u0026#34;, agent=tradingAdvisor, output_file = \u0026#34;trading_strategy.md\u0026#34; ) Kết quả Mình có lưu lại kết quả ở các phase, các bạn có thể vô đọc file markdown. Ngoài ra, file Full chain of thought sẽ cho ta thấy được toàn thể quá trình vận hành của team tư vấn này ^^.\nPhân tích kĩ thuật\nPhân tích thông tin\nTư vấn đầu tư\nTư vấn chiến lược giao dịch\nFull Chain-of-thought\nMình lấy ra 1 vài mẫu mình thấy đặc biệt để cùng nhau cảm nhận (mình làm mẫu trên cổ phiếu HPG)\nTA Analyst\n5. **Tóm tắt vị thế kỹ thuật hiện tại:** - Cổ phiếu HPG hiện đang có xu hướng giảm nhưng không bị quá bán. Tuy nhiên, các chỉ số cho thấy có thể có sự phục hồi nhẹ nếu vượt qua ngưỡng kháng cự. - Cần theo dõi các mức hỗ trợ và kháng cự để đưa ra quyết định đầu tư hợp lý. 6. **Mục tiêu giá và mức rủi ro:** - **Mục tiêu giá ngắn hạn:** Nếu cổ phiếu vượt qua mức kháng cự 27.00 VNĐ, có thể hướng tới mục tiêu 28.00 VNĐ. - **Mức rủi ro:** Cần cẩn trọng nếu giá xuống dưới 26.30 VNĐ, có thể dẫn đến sự sụt giảm sâu hơn. Tóm lại, cổ phiếu HPG hiện tại đang trong giai đoạn điều chỉnh, và cần lưu ý các yếu tố kỹ thuật trước khi tham gia giao dịch. News analyst\n5. **Sự kiện sắp tới**: - HPG dự kiến sẽ công bố báo cáo lợi nhuận từ ngày 28 tháng 1 đến ngày 3 tháng 2 năm 2025. Đây sẽ là một sự kiện quan trọng, vì phản ứng của thị trường có thể bị ảnh hưởng bởi kết quả. 9. **Yếu tố kích thích và rủi ro**: - **Yếu tố kích thích hàng đầu**: - Bứt phá trên mức kháng cự 27,00 VNĐ. - Báo cáo lợi nhuận tích cực vào đầu tháng Hai. - **Yếu tố rủi ro**: - Tiếp tục giảm giá thép. - Tăng áp lực bán từ nước ngoài. Advisor\n2. **Khuyến Nghị Từ Các Nhà Phân Tích**: - Các nhà phân tích đồng thuận rằng giá cổ phiếu sẽ tăng 25.5% trong năm tới. - Các mục tiêu giá đã được điều chỉnh tăng lên nhiều lần, với mục tiêu mới nhất được đặt là ₫34,075. 3. **Cảm Nhận Thị Trường**: - Sự giảm giá gần đây của cổ phiếu đã khiến HPG được xem là không được định giá đúng, làm tăng sự quan tâm của các nhà đầu tư. ## Các Yếu Tố Thúc Đẩy - **Nhu Cầu Tăng**: Ngành xây dựng tại Việt Nam tiếp tục phát triển, thúc đẩy nhu cầu cho các sản phẩm thép. - **Tăng Trưởng Lợi Nhuận**: Dự báo tăng trưởng lợi nhuận 24.27% mỗi năm, được thúc đẩy bởi khối lượng bán hàng cao hơn và biên lợi nhuận cải thiện. - **Vị Trí Thị Trường**: HPG nắm giữ thị phần lớn trong thép xây dựng và đang ở vị trí tốt để tận dụng các xu hướng ngành. Strategy recommendation\n1. **Tín hiệu Mua**: - Khi chỉ số sức mạnh tương đối (RSI) dưới 30, cho thấy cổ phiếu bị bán quá mức. Việc cổ phiếu được phân loại là bị bán quá mức cho thấy rằng nó có thể được định giá thấp, tạo ra cơ hội mua vào tiềm năng. - Thêm vào đó, xem xét giá cổ phiếu hiện tại là ₫26,750, thấp hơn khoảng 30.7% so với giá trị hợp lý ước tính, càng củng cố thêm cho trường hợp vào lệnh khi RSI chỉ ra điều kiện đánh giá thấp. Kết luận Topic về AI Agent mình tin sẽ sắp được ứng dụng rộng rãi vào thị trường trong 5 năm tới. Việc tạo ra một agent rất dễ. Cái khó ở đây là tạo ra 1 agent đáng tin và đủ giỏi để thực hiện các việc tự động hoá suy luận cao cấp hơn.\nTại start up của mình và các ae (miquant.vn), AI Agent cũng là một trong những sản phẩm đang được chú trọng xây dựng. Thay vì chỉ sử dụng các phương pháp prompt đơn giản như này, tụi mình nâng cấp lên thông qua các công cụ phân tích định lượng cũng như các dự báo kinh tế, expertise include, … nhằm nâng cao tính “trustworthy” của các agent này.\nStay tuned.\nRef Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in Financial Research ","permalink":"http://localhost:1313/posts/2024/2024-12-02-ai-agent-for-investment/","summary":"AI Agent là một hệ thống tự động và độc lập, có khả năng xử lý thông tin, ra quyết định và thực hiện các nhiệm vụ trong nhiều lĩnh vực. Bài viết mô tả cách xây dựng một đội ngũ phân tích đầu tư tự trị với các vai trò như phân tích kỹ thuật, phân tích thông tin, tư vấn đầu tư và xây dựng chiến lược giao dịch, sử dụng các công cụ để tự động hóa quy trình phân tích và ra quyết định. AI Agent có tiềm năng ứng dụng rộng rãi trong tương lai gần, tuy nhiên, việc tạo ra một agent đáng tin cậy vẫn là thách thức lớn.","title":"AI Agent: The future of Investment research"},{"content":"Giới thiệu Stochastic process là ngôn ngữ để miêu tả sự ngẫu nhiên, cũng giống như Calculus là ngôn ngữ để miêu tả sự thay đổi của hàm số.\nBài viết giới thiệu về stochastic process thông qua bài toán Gambler\u0026rsquo;s Ruin. Bài viết cũng mô phỏng chiến lược giao dịch VN30F1M như một trường hợp của Gambler\u0026rsquo;s ruin và sử dụng phương pháp Monte Carlo để xấp xỉ các giá trị cần thiết, nhấn mạnh tầm quan trọng của xác suất và kỳ vọng trong việc ra quyết định.\nConcept Gambler’s Ruin, hay Sự phá sản của con bạc, là một trong những concept cổ điển trong lý thuyết xác suất (probability theory) và quá trình ngẫu nhiên (stochastic process). Vấn đề này có thể được mô tả trong nhiều trường hợp, trong đó phổ biến nhất là:\nMột con bạc bước vào sòng bạc với số tiền $n trong tay và bắt đầu chơi một trò chơi, trong đó anh ta thắng với xác suất p và thua với xác suất $q = 1-p$. Người chơi lặp lại trò chơi này nhiều lần, đặt cược $1 mỗi lượt. Anh ta sẽ rời khỏi trò chơi nếu tổng số tiền của anh ta đạt đến $N hoặc nếu anh ta hết tiền (phá sản), tùy thuộc vào điều gì xảy ra trước. Xác suất mà con bạc bị phá sản hoặc thắng chung cuộc là bao nhiêu?\nXác suất con bạc thắng cả trận Gambler’s Ruin có thể được mô hình hoá như một bước đi ngẫu nhiên (random walk) mà ở đó chúng ta quan tâm đến xác suất người chơi sẽ thắng khi đạt được $N mong muốn. Ở bài viết này, mình sẽ sử dụng kết quả, bước giải chi tiết các bạn có thể tham khảo tại đây.\nVới $P_N(n)$ là xác suất người chơi sẽ đạt được $N với số tiền hiện tại là $n. Tương tự, $P_N(n+1)$ là xác suất người chơi sẽ đạt được $N với số tiền hiện tại là $n+1. $p$ là xác suất thắng 1 trận, $q = 1 - p$ là xác suất thua của 1 trận.\n$$ P(\\text{sucess}) = P(\\text{sucess}| \\text{win first round}) P(\\text{win first round})\\\\ + P(\\text{sucess}| \\text{lose first round}) P(\\text{lose first round}) \\\\ P_N(n) = P(n| W) P(W) + P(n| L) P(L) \\\\ P_N(n) = P_N(n+1) p + P_N(n-1) q $$Với $\\lambda = \\frac{q}{p}$, xác suất người chơi sẽ đạt được N là:\n$$ P_N(n) = \\begin{cases} \\frac{1 - \\lambda^n}{1 - \\lambda^N}, \u0026 \\lambda \\neq 1 \\\\ \\frac{n}{N}, \u0026 \\lambda = 1 \\end{cases} \\tag{1} $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def win_probability(p, initial_cap, expected_cap): assert 0\u0026lt;= p \u0026lt;=1 , \u0026#34;`p` must be a probability between 0 and 1.\u0026#34; assert 0 \u0026lt;= initial_cap \u0026lt;= expected_cap, \u0026#34;`` an initial_cap integer between 0 and expected_cap.\u0026#34; very_small_number = 1e-12 lambda_ = (1-p)/p if p\u0026lt;= very_small_number: return 0 if p\u0026gt;= 1-very_small_number: return 1 if lambda_==1: return initial_cap/expected_cap return (1-lambda_**initial_cap)/(1-lambda_**expected_cap) Bài toán giả định như sau:\nGiả sử trường hợp sau, anh A có 10 đồng và quyết định đi đánh black jack (xì dách) với mục tiêu sẽ gấp đôi số tiền (20 đồng), mỗi trận thắng/thua anh A sẽ lời/mất 1 đồng. Anh A sử dụng chiến thuật với xác suất thắng trong 1 ván là (a) 50%, (b) 55%, (c) 45% thì xác suất anh A đạt được mục tiêu 20 đồng là bao nhiêu.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ### Closed-form solution p = 0.5 initial_cap = 10 expected_cap = 20 win_rate = round(win_probability(p, initial_cap, expected_cap),5) p = 0.55 win_rate = round(win_probability(p, initial_cap, expected_cap),5) p = 0.45 win_rate = round(win_probability(p, initial_cap, expected_cap),5) ----------------------------------- Output: Win rate: 0.5, Initial capital: 10, Expected capital: 20 Sucess rate: 50% ----------------------------------- Win rate: 0.55, Initial capital: 10, Expected capital: 20 Sucess rate: 88.15% ----------------------------------- Win rate: 0.45, Initial capital: 10, Expected capital: 20 Sucess rate: 11.85% ----------------------------------- Từ công thức trên, ta có thể tính toán được xác suất con bạc thắng chung cuộc. Vậy, trong thực tế, nó sẽ “trông” như thế nào nhỉ?\nTa tiến hành giả lập chuỗi thời gian của bài toán này. Từ xác suất trên, ta có thể tính toán được nhiều thứ. Giá trị kì vọng trong 3 trường hợp 0.5, 0.55, 0.45 lần lượt là 10, 18, 2. Với việc giả lập 30 lần, ta có thể thấy giá trị trung bình (hay kì vọng) cũng tiến tới mức này.\nThời gian kỳ vọng con bạc dừng lại Ngoài ra, dựa trên giả lập trên, ta cũng có thể nhận thấy 1 tính chất rằng, sẽ tới 1 thời gian là các chuỗi sẽ đạt tới điểm dừng (absorbing state). Nôm na là thời điểm kì vọng (hay trung bình) con bạc đạt được $N hoặc thua hết “xèng”.\nVới S là thời gian kỳ vọng, D là bước (step), ta có:\n$$ \\begin{align*} E(\\text{duration}) = E(\\text{duration}| \\text{win first round}) P(\\text{win first round}) \\\\+ E(\\text{duration}| \\text{lose first round}) P(\\text{lose first round})\\\\ \\end{align*}\\\\ $$$$ \\begin{align*} E_n(S) \u0026= E(S|D_1=n+1)p +E(S|D_1=n-1)q\\\\ \u0026= (1+E(S|D_0=n+1))p + (1+E(S|D_0=n-1))q\\\\ \u0026= p+q+E(S|D_0=n+1)p +E(S|D_0=n-1)q\\\\ \u0026= 1+E_{n+1}(S)p +E_{n-1}(S)q \\end{align*} $$$$ \\begin{align*} E_n(S) = \\frac{n}{q - p} - \\frac{N}{q - p} \\cdot \\frac{(\\frac{q}{p})^n - 1}{(\\frac{q}{p})^N - 1} \\end{align*} $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def expected_duration(p, initial_cap, expected_cap): assert 0\u0026lt;= p \u0026lt;=1 , \u0026#34;`p` must be a probability between 0 and 1.\u0026#34; assert 0 \u0026lt;= initial_cap \u0026lt;= expected_cap, \u0026#34;`` an initial_cap integer between 0 and expected_cap.\u0026#34; very_small_number = 1e-12 q = 1-p lambda_ = q/p if lambda_==1: return initial_cap*(expected_cap - initial_cap) duration = ( initial_cap/(q-p) - expected_cap/(q-p)* ((lambda_**initial_cap-1)/(lambda_**expected_cap-1)) ) return duration ------- initial_cap = 10 expected_cap = 20 Win rate: 0.5, Expected duration: 100 Win rate: 0.45, Expected duration: 76.3 Win rate: 0.55, Expected duration: 76.3 Vậy, ta có thể tự tin nói rằng: Nếu xác suất của ván bài là 50/50 cho mỗi ván, thì kỳ vọng con bạc sẽ dừng cuộc chơi (cả thắng lẫn thua) sẽ là sau 100 ván. Còn nếu xác suất là 45% hoặc 55% thì (dự kiến) sau 77 ván con bạc sẽ dừng cuộc chơi.\nVN30F1M Bối cảnh về con bạc đã xong, giờ ta qua tới VN30F1. Ta sẽ trade với chiến lược siêu đơn giản như sau: Long giá mở cửa (Open) và đóng giá đóng cửa (Close). Với dữ liệu daily từ 2018 tới nay, ta thu được các kết quả như sau:\nTrung bình tăng: 9.21 Trung bình giảm: -10.14 Số ngày tăng: 777; Số ngày giảm 734 Xác suất tăng giảm hằng ngày: 51% Cũng khá tương đồng với bài toán black jack ở trên 🙂. Từ các tham số trên, ta mô hình hoá như sau: Xác suất lời lỗ là 50%, mỗi ngày tăng giảm trung bình 10 điểm. Giả sử bạn 1 số tiền đủ để bạn “risk” 200 điểm (~200tr). Bạn kỳ vọng sẽ gấp đôi trong 400 điểm. Để đưa về bài toán gambler’s ruin, ta cần chuẩn hoá lời lỗ về 1 điểm. Từ đó, thông số của bài toán sẽ là p=0.5, n=20, N= 40.\nSucess_prob = n/N = 20/40 = 50%\nExpected_duration = n*(N-n) = 20*(40-20) = 400 (days)\nLợi nhuận kỳ vọng: 40 * 50% + 0 * 50% = 20 (bằng số vốn ban đầu)\nTừ đó công thức ta có thể suy luận các ý như sau:\nKhi bạn kỳ vọng càng cao (so với số tiền bạn có) thì xác suất bạn thành công càng thấp và thời gian dự kiến bạn lỗ sạch càng nhanh. Bạn muốn tăng xác suất thành công thì nên có vốn dài (hay có nhiều tiền) 😃 Tăng xác suất thắng cho từng lần trade thì lợi nhuận kỳ vọng tăng và xác suất bạn thua sạch càng thấp. Giả lập VN30F1M cho các trường hợp xác suất thắng cho từng lần trade là 55%, 45% và 50%.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def simulation_plot(p, initial_cap, each_step, expected_cap, n, n_sim, expected_stopping = None,title = \u0026#39;Simulate Gambler Ruin\u0026#39;): capital = np.zeros((n_sim, n)) for i in range(n_sim): capital[i] = simulate_gambler_ruin(p, initial_cap, expected_cap, each_step, n) plt.figure(figsize=(15,5), dpi = 200) plt.plot(capital.T, alpha = 0.75) ## Mean and standard deviation plt.hlines(initial_cap, 0, n, colors=\u0026#39;black\u0026#39;, linestyles=\u0026#39;dashed\u0026#39;, label=\u0026#39;Initial capital\u0026#39;) # plt.fill_between(np.arange(n), np.min(capital, axis = 0), np.max(capital, axis = 0), color = \u0026#39;gray\u0026#39;, alpha = 0.5, label = \u0026#39;Mean +/- std\u0026#39;) #Mean plt.plot(np.mean(capital, axis = 0), color = \u0026#39;black\u0026#39;, linewidth = 2, label = \u0026#39;Mean\u0026#39;) if expected_stopping: plt.vlines(expected_stopping,0, expected_cap, colors=\u0026#39;black\u0026#39;, label=\u0026#39;Expected stopping point\u0026#39;) plt.title(title) plt.xlabel(\u0026#39;tradingDate\u0026#39;) plt.ylabel(\u0026#39;Cumulative points\u0026#39;) plt.show() return capital initial_cap = 200 each_step = 10 expected_cap = 400 n= 500 n_sim = 100 Với chiến lược có xác suất lời 55%, chỉ có 2 lần là bạn thua sạch tiền trong số 100 lần giả lập.\nTừ giả lập trên, ta có thể dễ dàng nhận ra 1 số tính chất cơ bản và nền móng của stochastic process như sau:\nNếu xác suất là p=0.5, giá trị kỳ vọng bằng đúng với giá trị ban đầu. Đây là tính chất martingale cơ bản trong stochastic process $E(X_n) = E(X_0) \\text{ với } n \\ge 0$ . Với p \u0026gt; 0.5, ta có thể thấy xu hướng của chuỗi thời gian có chiều hướng lên (positive drift), và ngược lại với. p\u0026lt; 0.5, xu hướng có chiều hướng xuống (negative drift). Giá trị của $X_{n+1}$ chỉ phụ thuộc vào $X_{n}$, hay giá trị của ngày hôm sau chỉ phụ thuộc vào ngày hôm nay và không phụ thuộc vào quá khứ trước đó. Đây là tính chất Markov. Variance của chuỗi thời gian này mở rộng theo thời gian (hay phụ thuộc vào thời gian t). Ngoài lề: Monte carlo simulation xấp xỉ các giá trị cần thiết Ngoài ra, dựa vào phương pháp giả lập (Monte carlo) này, ta có thể xấp xỉ các giá trị xác suất, kỳ vọng như phương pháp closed-form solution như trên. Dựa trên lý thuyết số lớn (Law of large number), bằng cách lấy mẫu ngẫu nhiên nhiều lần, ta có thể mô phỏng lại các trường hợp có thể xảy ra nhằm tính toán các giá trị mong muốn.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # tham số initial_cap = 200 each_step = 10 expected_cap = 400 ## giả lập n = 2000 # (giả lập tới vô cực; 2000 là cũng đủ lớn) n_sim = 20000 # (giả lập nhiều lần; 20000 là cũng đủ lớn) capital_up = simulation_plot(0.55, initial_cap, each_step, expected_cap, n, n_sim, title = \u0026#39;Simulate Gambler Ruin with positive drift (p\u0026gt;0.5)\u0026#39;) capital_down = simulation_plot(0.45, initial_cap, each_step, expected_cap, n, n_sim, title = \u0026#39;Simulate Gambler Ruin with negative drift (p\u0026lt;0.5)\u0026#39;) capital_neutral = simulation_plot(0.5, initial_cap, each_step, expected_cap, n, n_sim, title = \u0026#39;Simulate Gambler Ruin with no drift (p=0.5)\u0026#39;) def stopping_time(capital, threshold): ls_positive = [] ls_negative = [] for i in range(capital.shape[0]): ls_positive.append(np.argmax(capital[i,:]\u0026gt;=threshold)) ls_negative.append(np.argmax(capital[i,:]\u0026lt;=0)) # ls = ls_positive + ls_negative ls = pd.DataFrame([ls_positive, ls_negative]).sum(axis = 0).mean() return ls ## tính toán xác suất unique, counts = np.unique(capital_neutral[:,-1], return_counts=True) print(counts/np.sum(counts)) unique, counts = np.unique(capital_up[:,-1], return_counts=True) print(counts/np.sum(counts)) unique, counts = np.unique(capital_down[:,-1], return_counts=True) print(counts/np.sum(counts)) ## Tính toán stopping time stopping_time(capital_neutral, 400), stopping_time(capital_up, 400), stopping_time(capital_down, 400) Kết quả giả lập so với sử dụng closed-form như sau:\nSucess probability Expected duration p Simulation Closed-form Simulation Closed-form 0.5 0.4944 0.5 395.27 400 0.55 0.9803 0.98 190.263 192.9 0.45 0.0157 0.02 190.594 192.9 Từ đó, ta có thể thấy rằng, với những bài toán chưa có một cách giải “đẹp”, ta có thể tiến hành giả lập các trường hợp xảy ra như một phương án chữa cháy để tính toán các giá trị mong muốn. Tuy nhiên, ta phải đánh đổi bằng tốc độ và độ “đẹp” của kết quả.\nKết luận Okay, đã đủ cho bài giới thiệu về stochastic process rồi. Chúng ta đã cùng nhau đi qua nhiều thứ nền móng: (1) Gambler’s ruin để giới thiệu về stochastic process; (2) Giả lập cho VN30F1M; (3) Monte Carlo để đi xấp xỉ các giá trị cần thiết.\nThông qua bài viết trên, bạn đã nắm được gì:\nNền móng cho việc “gambling”: dựa vào xác suất, vốn của bạn, và kỳ vọng, bạn có thể gamble tốt hơn rồi đó. Trước khi bắt đầu bet vào một thứ gì đó, hãy chậm lại 1 bước, suy nghĩ về toán một tí, rồi mới quyết định chơi hay không. Hay đúng không nào ^^ Khi gặp 1 bài toán và bí. Hãy đi giả lập nó để xấp xỉ trước kết quả cuối cùng. Strategy phái sinh có xác suất thắng dưới 50% thì nên xem xét lại. Nền móng cho vài tính chất cơ bản của stochastic process. Ngoài ra, mình có để những bài viết rất hay của chủ đề tương tự ở phần ref, bạn nên nghía qua để hiểu sâu hơn về phần toán ở phía dưới nhé!\nRef https://randomdeterminism.wordpress.com/2010/07/07/gamblers-ruin/\nhttps://web.mit.edu/neboat/Public/6.042/randomwalks.pdf\nhttps://sites.pitt.edu/~jdnorton/teaching/paradox/chapters/probability_from_expectation/gambler_ruin.pdf\nhttps://en.wikipedia.org/wiki/Monte_Carlo_method#:~:text=Sawilowsky distinguishes between a simulation,uses repeated sampling to obtain\n","permalink":"http://localhost:1313/posts/2024/2024-10-20-stochastic-process-part-1/","summary":"Bài viết giới thiệu về quá trình ngẫu nhiên thông qua bài toán Gambler\u0026rsquo;s Ruin. Bài viết cũng mô phỏng chiến lược giao dịch VN30F1M và sử dụng phương pháp Monte Carlo để xấp xỉ các giá trị cần thiết, nhấn mạnh tầm quan trọng của xác suất và kỳ vọng trong việc ra quyết định.","title":"Stochastic process part 1: Gambler's ruin of VN30F"},{"content":"Hôm nay, chúng ta sẽ cùng nhau thảo luận về các phương pháp khác nhau để giải quyết bài toán Linear regression - Hồi quy tuyến tính.\nVậy Linear regression là gì? Hồi quy tuyến tính là phương pháp thống kê dùng để mô hình hóa mối quan hệ giữa biến phụ thuộc và các biến độc lập. Hay nói cách khác, hồi quy tuyến tính là đi tìm mối quan hệ tuyến tính (y=ax+b) giữa 2 biến với nhau.\nỞ bài này, mình sẽ không bàn luận sâu về toán, các bạn có thể đọc ở đây để nắm lý thuyết cần thiết link. Thay vào đó, bài viết này sẽ đi thực hiện bài toán đơn giản này theo 3 cách tiếp cận khác nhau:\nSử dụng linear algebra để tính trực tiếp Thông qua deep learning, sử dụng đạo hàm Thông qua bayesian inference, sử dụng phương pháp lấy mẫu (sampling) Ngoài ra, bài viết sẽ áp dụng phương pháp này để đi xác định hệ số Beta cho cổ phiếu HPG. Các bạn có thể đọc về beta tại đây.\nỞ bài toán này, dữ liệu X sẽ là VNINDEX, trong khi y sẽ là HPG.\nVề mặt tài chính, điều này có thể được xem như là ước tính rủi ro thị trường, hay rủi ro hệ thống (market risk, systematic risk) cho HPG.\n1. Tính trực tiếp từ dữ liệu Lời giả cho phương pháp này là:\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Algebra_linear_regression: def __init__(self): self.coef_ = None self.intercept_ = None def fit(self, X, y): X = np.array(X) y = np.array(y) X = np.c_[np.ones(X.shape[0]), X] self.coef_ = np.linalg.inv(X.T @ X) @ X.T @ y self.intercept_ = self.coef_[0] self.coef_ = self.coef_[1:] def predict(self, X): X = np.array(X) return X @ self.coef_ + self.intercept_ def score(self, X, y): X = np.array(X) y = np.array(y) y_pred = self.predict(X) return 1 - ((y - y_pred) ** 2).sum() / ((y - y.mean()) ** 2).sum() model = Algebra_linear_regression() model.fit(X,y) model.coef_.round(3), model.intercept_.round(4) -\u0026gt; output: (array([[1.23]]), array([0.0005])) Từ phương pháp này, ta có thể ước lượng được rằng hệ số beta cho cổ phiếu HPG là 1.23 tương ứng với việc rủi ro thị trường tương đối cao.\n2. Sử dụng deep learning Deep learning cũng là một trong những phương pháp thông dụng có thể được sử dụng trong bài toán này. Thông qua thuật toán Gradient Descent, ta có đi tìm bộ tham số phù hợp với hàm mất mát (loss function) là thấp nhất.\nQuá trình đi tìm điểm tối ưu của bài toán này giống như khi bạn đi xuống núi. Ban đầu, bạn sẽ bắt đầu ở một điểm nào đó, tại mỗi điểm bạn sẽ luôn biết nên đi hướng nào. Mục tiêu của bạn sẽ đi xuống núi từng bước nhỏ một. Bạn cứ liên tục đi cho đến khi bạn đạt đến điểm trũng của thung lũng!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import torch import torch.nn as nn # Create class class LinearRegressionModel(nn.Module): def __init__(self, input_dim, output_dim): super(LinearRegressionModel, self).__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): out = self.linear(x) return out input_dim = 1 output_dim = 1 model = LinearRegressionModel(input_dim, output_dim) criterion = nn.MSELoss() learning_rate = 0.001 optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) epochs = 1000 losses = [] # List to store loss at each epoch for epoch in range(epochs): epoch += 1 # Convert numpy array to torch Variable inputs = torch.from_numpy(X).requires_grad_() labels = torch.from_numpy(y) # Clear gradients w.r.t. parameters optimizer.zero_grad() # Forward to get output outputs = model(inputs) # Calculate Loss loss = criterion(outputs, labels) # Getting gradients w.r.t. parameters loss.backward() # Updating parameters optimizer.step() losses.append(loss.item()) print(\u0026#39;epoch {}, loss {}\u0026#39;.format(epoch, loss.item())) Thông qua phương pháp này, ta cũng tìm được mối quan hệ giống hệt với phương pháp tính trực tiếp!\n3. Bayesian linear regression Thay vì chỉ đưa ra một giá trị cố định cho các hệ số như hồi quy tuyến tính thông thường, phương pháp bayes coi các hệ số là những giá trị có thể thay đổi và có xác suất xảy ra. Ban đầu, ta có một \u0026ldquo;niềm tin\u0026rdquo; về các tham số này (prior). Khi thu thập thêm dữ liệu, ta sẽ cập nhật niềm tin đó, từ đó giúp dự đoán chính xác hơn và biết mức độ chắc chắn của các kết quả (posterior). Khoá Coursera này sẽ cho bạn cái nhìn kĩ hơn về phương pháp này.\nĐể hiểu đơn giản hơn, ta có một ví dụ này: Với 2 phương pháp trên, hệ số beta của HPG là 1.23. Vậy có bao nhiêu phần trăm (%) hệ số beta là 1.23, hệ số beta lớn hơn 1 với độ tự tin bao nhiêu phần trăm (%)? Giá trị của hệ số beta của HPG có thể có giá trị từ đâu tới đâu với độ tư tin là bao nhiêu phần trăm (%)?\nĐây là một trong những vấn đề có thể được giải quyết thông qua phương pháp Bayes. Bạn có thể đọc kĩ hơn về ý tưởng đằng sau phương pháp này tại đây.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def numpyro_model(X, y): # Priors for the parameters num_features = X.shape[1] beta = numpyro.sample(\u0026#34;beta\u0026#34;, dist.Normal(0,1)) # Coefficients intercept = numpyro.sample(\u0026#34;intercept\u0026#34;, dist.Normal(0, 1)) # Intercept sigma = numpyro.sample(\u0026#34;sigma\u0026#34;, dist.Normal(0,1)) # Noise level # Linear model mean = jnp.dot(X, beta) + intercept # Likelihood numpyro.sample(\u0026#34;obs\u0026#34;, dist.Normal(mean, sigma), obs=y) # Instantiate a `MCMC` object using a NUTS sampler mcmc = MCMC(sampler=NUTS(numpyro_model), num_warmup=1000, num_samples=1000, num_chains=4) # Run the MCMC sampler and collect samples mcmc.run(rng_key=random.PRNGKey(seed=42), X=X, y=y) az.plot_trace(mcmc, var_names=[\u0026#39;intercept\u0026#39;, \u0026#39;beta\u0026#39;, \u0026#39;sigma\u0026#39;], figsize=(9,9)); $$ Y = \\beta_0 + \\beta_1 X + \\epsilon $$ Phương pháp biểu diễn các biến trong Bayesian inference (plate notation). Ở bài toán này, ta sẽ có 3 biến chính: beta, intercept và sigma. Tất cả đều được giả định là phân phối chuẩn.\nmean std median 5.0% 95.0% n_eff r_hat beta 1.23 0.03 1.23 1.18 1.28 734.39 1.01 intercept 0.00 0.00 0.00 -0.00 0.00 3212.78 1.00 sigma 0.02 0.00 0.02 0.02 0.02 5063.55 1.00 Hình này được gọi là trace plot. Phương pháp này được dùng để đánh giá quá trình hội tụ của mô hình Bayes. Cột bên trái cho ta thấy phân phối, trong khi bên phải cho ta thấy sự giao động của các tham số.\nTừ kết quả mô hình trên, ta ra được kết luận rằng, hệ số beta trung bình cho cổ phiếu HPG cũng là 1.23, và ta có thể tự tin nói rằng, 90% trường hợp hệ số beta của HPG sẽ trong vùng 1.18 - 1.23.\nKết luận Từ 3 phương pháp trên ta đều thu được chung 1 kết quả. Ngoài ra, mỗi phương pháp có ưu và nhược điểm của mình.\nVới phương pháp tính toán trực tiếp, ta có thể dễ dàng tính toán ra được đáp án với độ phức tạp là thấp nhất. Tuy nhiên, với những trường hợp mà dữ liệu là không khả nghịch (singular), thì bài toán có khả năng không có khả năng giải được theo phương pháp này (analytical solution).\nNgược lại, với phương pháp deep learning, ta có thể dễ dàng xử lý các bài toán khi không thể giải trực tiếp (analytical solution). Thông qua Gradient Descent, ta có thể ước tính được các điểm tối ưu, tuy nhiên đi kèm với đó là yêu cầu tính toán lớn hơn nhiều (computational cost).\nCuối cùng, phương pháp Bayesian có thể được sử dụng khi ta mong muốn đưa niềm tin của ta vào mô hình, cũng như đầu ra mong muốn là một phân phối các kết quả có thể xảy ra thay vì chỉ muốn một điểm ước lượng duy nhất. Quan trọng nhất là mô hình hoá được độ không chắc chắn (uncertainty). Tuy nhiên, yếu tố giả định của người sử dụng có thể có tác động tiêu cực đến mô hình nếu giả định sai.\nVà tất nhiên, 2 phương pháp sau là phức tạp hoá vấn đề cho bài toán đơn giản này. Tuy nhiên, để gần gũi nhất thì mình chọn ước lượng beta cho dễ hình dung!\n","permalink":"http://localhost:1313/posts/2024/2024-10-10-linear-regression/","summary":"\u003cp\u003eHôm nay, chúng ta sẽ cùng nhau thảo luận về các phương pháp khác nhau để giải quyết bài toán Linear regression - Hồi quy tuyến tính.\u003c/p\u003e\n\u003ch2 id=\"vậy-linear-regression-là-gì\"\u003eVậy Linear regression là gì?\u003c/h2\u003e\n\u003cp\u003eHồi quy tuyến tính là phương pháp thống kê dùng để mô hình hóa mối quan hệ giữa biến phụ thuộc và các biến độc lập. Hay nói cách khác, hồi quy tuyến tính là đi tìm mối quan hệ tuyến tính (y=ax+b) giữa 2 biến với nhau.\u003c/p\u003e","title":"Linear regression"},{"content":"\nHi, mình là Hùng. Hiện mình là sinh viên thạc sĩ AI tại RMIT Vietnam với đam mê dành cho mảng đầu tư định lượng (Quant). Blog này là nơi mình chia sẻ kiến thức (đa phần là lưu trữ cái mình học) và nếu nó giúp được bạn trong chuyến phiêu lưu ngành quant thì mình rất vui!\nCác mảng yêu thích AI \u0026amp; data sci: Learning to Rank, Network science, Causal ML, Deep learning, Reinforcement learning, … Tài chính: Portfolio optimization, Asset pricing, Factor investing, High frequency trading, \u0026hellip; Rất open với collaboration :), liên hệ thông qua Linkedin Publication:\nDecoding the Infrastructure Announcement Effect to Real Estate Price in Urban Vietnam Case Study: The Impact of Metro Line 1 HCMC, preprint The International Conference on Business and Technology (ICBT’Cairo2025), draft\nMiquant Mình và các anh em có xây dựng một start up \u0026ldquo;nhỏ\u0026rdquo; về tài chính định lượng và AI mang tên miquant. Sản phẩm này giúp nhà đầu tư, nhà phân tích hay các nhà nghiên cứu chuyên sâu về cổ phiếu một cách nhanh chóng, đáng tin và chính xác.\nMời bạn tham khảo tại miquant.vn để tìm hiểu thêm!\nHọc vấn MSc Artificial Intelligent (RMIT Vietnam) BSc Economics \u0026amp; Finance (RMIT Vietnam) CFA lev 1 Liên hệ Email: hungha1412@gmail.com LinkedIn: Quốc Hùng ","permalink":"http://localhost:1313/about/","summary":"\u003cp\u003e\u003cimg src=\"/hamster.png\"\nalt=\"A hamster coding on a laptop\"\nwidth=\"50%\"\nstyle=\"display:block; margin:0 auto; height:auto;\"\u003e\u003c/p\u003e\n\u003cp\u003eHi, mình là Hùng. Hiện mình là sinh viên thạc sĩ AI tại RMIT Vietnam với đam mê dành cho mảng đầu tư định lượng (Quant). Blog này là nơi mình chia sẻ kiến thức (đa phần là lưu trữ cái mình học) và nếu nó giúp được bạn trong chuyến phiêu lưu ngành quant thì mình rất vui!\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"các-mảng-yêu-thích\"\u003eCác mảng yêu thích\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eAI \u0026amp; data sci: Learning to Rank, Network science, Causal ML, Deep learning, Reinforcement learning, …\u003c/li\u003e\n\u003cli\u003eTài chính: Portfolio optimization, Asset pricing, Factor investing, High frequency trading, \u0026hellip;\u003c/li\u003e\n\u003cli\u003eRất open với collaboration :), liên hệ thông qua \u003ca href=\"https://www.linkedin.com/in/haquochung11/\"\u003eLinkedin\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ePublication\u003c/strong\u003e:\u003cbr\u003e\nDecoding the Infrastructure Announcement Effect to Real Estate Price in Urban Vietnam Case Study: The Impact of Metro Line 1 HCMC, preprint The International Conference on Business and Technology (ICBT’Cairo2025), \u003ca href=\"https://drive.google.com/file/d/1R4bEaI_Yz0ah5vFO80vJTwyDcUaP1UPG/view?usp=share_link\"\u003edraft\u003c/a\u003e\u003c/p\u003e","title":"About"},{"content":"","permalink":"http://localhost:1313/blog/","summary":"blog","title":"Blog"},{"content":"","permalink":"http://localhost:1313/research/","summary":"","title":"Research"}]