[{"content":"Giới thiệu Stochastic process là ngôn ngữ để miêu tả sự ngẫu nhiên, cũng giống như Calculus là ngôn ngữ để miêu tả sự thay đổi của hàm số.\nBài viết giới thiệu về stochastic process thông qua bài toán Gambler\u0026rsquo;s Ruin. Bài viết cũng mô phỏng chiến lược giao dịch VN30F1M như một trường hợp của Gambler\u0026rsquo;s ruin và sử dụng phương pháp Monte Carlo để xấp xỉ các giá trị cần thiết, nhấn mạnh tầm quan trọng của xác suất và kỳ vọng trong việc ra quyết định.\nConcept Gambler’s Ruin, hay Sự phá sản của con bạc, là một trong những concept cổ điển trong lý thuyết xác suất (probability theory) và quá trình ngẫu nhiên (stochastic process). Vấn đề này có thể được mô tả trong nhiều trường hợp, trong đó phổ biến nhất là:\nMột con bạc bước vào sòng bạc với số tiền $n trong tay và bắt đầu chơi một trò chơi, trong đó anh ta thắng với xác suất p và thua với xác suất $q = 1-p$. Người chơi lặp lại trò chơi này nhiều lần, đặt cược $1 mỗi lượt. Anh ta sẽ rời khỏi trò chơi nếu tổng số tiền của anh ta đạt đến $N hoặc nếu anh ta hết tiền (phá sản), tùy thuộc vào điều gì xảy ra trước. Xác suất mà con bạc bị phá sản hoặc thắng chung cuộc là bao nhiêu?\nXác suất con bạc thắng cả trận Gambler’s Ruin có thể được mô hình hoá như một bước đi ngẫu nhiên (random walk) mà ở đó chúng ta quan tâm đến xác suất người chơi sẽ thắng khi đạt được $N mong muốn. Ở bài viết này, mình sẽ sử dụng kết quả, bước giải chi tiết các bạn có thể tham khảo tại đây.\nVới $P_N(n)$ là xác suất người chơi sẽ đạt được $N với số tiền hiện tại là $n. Tương tự, $P_N(n+1)$ là xác suất người chơi sẽ đạt được $N với số tiền hiện tại là $n+1. $p$ là xác suất thắng 1 trận, $q = 1 - p$ là xác suất thua của 1 trận.\n$$ P(\\text{sucess}) = P(\\text{sucess}| \\text{win first round}) P(\\text{win first round})\\\\ + P(\\text{sucess}| \\text{lose first round}) P(\\text{lose first round}) \\\\ P_N(n) = P(n| W) P(W) + P(n| L) P(L) \\\\ P_N(n) = P_N(n+1) p + P_N(n-1) q $$Với $\\lambda = \\frac{q}{p}$, xác suất người chơi sẽ đạt được N là:\n$$ P_N(n) = \\begin{cases} \\frac{1 - \\lambda^n}{1 - \\lambda^N}, \u0026 \\lambda \\neq 1 \\\\ \\frac{n}{N}, \u0026 \\lambda = 1 \\end{cases} \\tag{1} $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def win_probability(p, initial_cap, expected_cap): assert 0\u0026lt;= p \u0026lt;=1 , \u0026#34;`p` must be a probability between 0 and 1.\u0026#34; assert 0 \u0026lt;= initial_cap \u0026lt;= expected_cap, \u0026#34;`` an initial_cap integer between 0 and expected_cap.\u0026#34; very_small_number = 1e-12 lambda_ = (1-p)/p if p\u0026lt;= very_small_number: return 0 if p\u0026gt;= 1-very_small_number: return 1 if lambda_==1: return initial_cap/expected_cap return (1-lambda_**initial_cap)/(1-lambda_**expected_cap) Bài toán giả định như sau:\nGiả sử trường hợp sau, anh A có 10 đồng và quyết định đi đánh black jack (xì dách) với mục tiêu sẽ gấp đôi số tiền (20 đồng), mỗi trận thắng/thua anh A sẽ lời/mất 1 đồng. Anh A sử dụng chiến thuật với xác suất thắng trong 1 ván là (a) 50%, (b) 55%, (c) 45% thì xác suất anh A đạt được mục tiêu 20 đồng là bao nhiêu.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ### Closed-form solution p = 0.5 initial_cap = 10 expected_cap = 20 win_rate = round(win_probability(p, initial_cap, expected_cap),5) p = 0.55 win_rate = round(win_probability(p, initial_cap, expected_cap),5) p = 0.45 win_rate = round(win_probability(p, initial_cap, expected_cap),5) ----------------------------------- Output: Win rate: 0.5, Initial capital: 10, Expected capital: 20 Sucess rate: 50% ----------------------------------- Win rate: 0.55, Initial capital: 10, Expected capital: 20 Sucess rate: 88.15% ----------------------------------- Win rate: 0.45, Initial capital: 10, Expected capital: 20 Sucess rate: 11.85% ----------------------------------- Từ công thức trên, ta có thể tính toán được xác suất con bạc thắng chung cuộc. Vậy, trong thực tế, nó sẽ “trông” như thế nào nhỉ?\nTa tiến hành giả lập chuỗi thời gian của bài toán này. Từ xác suất trên, ta có thể tính toán được nhiều thứ. Giá trị kì vọng trong 3 trường hợp 0.5, 0.55, 0.45 lần lượt là 10, 18, 2. Với việc giả lập 30 lần, ta có thể thấy giá trị trung bình (hay kì vọng) cũng tiến tới mức này.\nNgoài ra, dựa trên giả lập trên, ta cũng có thể nhận thấy 1 tính chất rằng, sẽ tới 1 thời gian là các chuỗi sẽ đạt tới điểm dừng (absorbing state). Nôm na là thời điểm kì vọng (hay trung bình) con bạc đạt được $N hoặc thua hết “xèng”.\nVới S là thời gian kỳ vọng, D là bước (step), ta có:\n$$ \\begin{align*} E(\\text{duration}) = E(\\text{duration}| \\text{win first round}) P(\\text{win first round}) \\\\+ E(\\text{duration}| \\text{lose first round}) P(\\text{lose first round})\\\\ \\end{align*}\\\\ $$$$ \\begin{align*} E_n(S) \u0026= E(S|D_1=n+1)p +E(S|D_1=n-1)q\\\\ \u0026= (1+E(S|D_0=n+1))p + (1+E(S|D_0=n-1))q\\\\ \u0026= p+q+E(S|D_0=n+1)p +E(S|D_0=n-1)q\\\\ \u0026= 1+E_{n+1}(S)p +E_{n-1}(S)q \\end{align*} $$$$ \\begin{align*} E_n(S) = \\frac{n}{q - p} - \\frac{N}{q - p} \\cdot \\frac{(\\frac{q}{p})^n - 1}{(\\frac{q}{p})^N - 1} \\end{align*} $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def expected_duration(p, initial_cap, expected_cap): assert 0\u0026lt;= p \u0026lt;=1 , \u0026#34;`p` must be a probability between 0 and 1.\u0026#34; assert 0 \u0026lt;= initial_cap \u0026lt;= expected_cap, \u0026#34;`` an initial_cap integer between 0 and expected_cap.\u0026#34; very_small_number = 1e-12 q = 1-p lambda_ = q/p if lambda_==1: return initial_cap*(expected_cap - initial_cap) duration = ( initial_cap/(q-p) - expected_cap/(q-p)* ((lambda_**initial_cap-1)/(lambda_**expected_cap-1)) ) return duration ------- initial_cap = 10 expected_cap = 20 Win rate: 0.5, Expected duration: 100 Win rate: 0.45, Expected duration: 76.3 Win rate: 0.55, Expected duration: 76.3 Vậy, ta có thể tự tin nói rằng: Nếu xác suất của ván bài là 50/50 cho mỗi ván, thì kỳ vọng con bạc sẽ dừng cuộc chơi (cả thắng lẫn thua) sẽ là sau 100 ván. Còn nếu xác suất là 45% hoặc 55% thì (dự kiến) sau 77 ván con bạc sẽ dừng cuộc chơi.\nVN30F1M Bối cảnh về con bạc đã xong, giờ ta qua tới VN30F1. Ta sẽ trade với chiến lược siêu đơn giản như sau: Long giá mở cửa (Open) và đóng giá đóng cửa (Close). Với dữ liệu daily từ 2018 tới nay, ta thu được các kết quả như sau:\nTrung bình tăng: 9.21 Trung bình giảm: -10.14 Số ngày tăng: 777; Số ngày giảm 734 Xác suất tăng giảm hằng ngày: 51% Cũng khá tương đồng với bài toán black jack ở trên 🙂. Từ các tham số trên, ta mô hình hoá như sau: Xác suất lời lỗ là 50%, mỗi ngày tăng giảm trung bình 10 điểm. Giả sử bạn 1 số tiền đủ để bạn “risk” 200 điểm (~200tr). Bạn kỳ vọng sẽ gấp đôi trong 400 điểm. Để đưa về bài toán gambler’s ruin, ta cần chuẩn hoá lời lỗ về 1 điểm. Từ đó, thông số của bài toán sẽ là p=0.5, n=20, N= 40.\nSucess_prob = n/N = 20/40 = 50%\nExpected_duration = n*(N-n) = 20*(40-20) = 400 (days)\nLợi nhuận kỳ vọng: 40 * 50% + 0 * 50% = 20 (bằng số vốn ban đầu)\nTừ đó công thức ta có thể suy luận các ý như sau:\nKhi bạn kỳ vọng càng cao (so với số tiền bạn có) thì xác suất bạn thành công càng thấp và thời gian dự kiến bạn lỗ sạch càng nhanh. Bạn muốn tăng xác suất thành công thì nên có vốn dài (hay có nhiều tiền) 😃 Tăng xác suất thắng cho từng lần trade thì lợi nhuận kỳ vọng tăng và xác suất bạn thua sạch càng thấp. Giả lập VN30F1M cho các trường hợp xác suất thắng cho từng lần trade là 55%, 45% và 50%.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def simulation_plot(p, initial_cap, each_step, expected_cap, n, n_sim, expected_stopping = None,title = \u0026#39;Simulate Gambler Ruin\u0026#39;): capital = np.zeros((n_sim, n)) for i in range(n_sim): capital[i] = simulate_gambler_ruin(p, initial_cap, expected_cap, each_step, n) plt.figure(figsize=(15,5), dpi = 200) plt.plot(capital.T, alpha = 0.75) ## Mean and standard deviation plt.hlines(initial_cap, 0, n, colors=\u0026#39;black\u0026#39;, linestyles=\u0026#39;dashed\u0026#39;, label=\u0026#39;Initial capital\u0026#39;) # plt.fill_between(np.arange(n), np.min(capital, axis = 0), np.max(capital, axis = 0), color = \u0026#39;gray\u0026#39;, alpha = 0.5, label = \u0026#39;Mean +/- std\u0026#39;) #Mean plt.plot(np.mean(capital, axis = 0), color = \u0026#39;black\u0026#39;, linewidth = 2, label = \u0026#39;Mean\u0026#39;) if expected_stopping: plt.vlines(expected_stopping,0, expected_cap, colors=\u0026#39;black\u0026#39;, label=\u0026#39;Expected stopping point\u0026#39;) plt.title(title) plt.xlabel(\u0026#39;tradingDate\u0026#39;) plt.ylabel(\u0026#39;Cumulative points\u0026#39;) plt.show() return capital initial_cap = 200 each_step = 10 expected_cap = 400 n= 500 n_sim = 100 Với chiến lược có xác suất lời 55%, chỉ có 2 lần là bạn thua sạch tiền trong số 100 lần giả lập.\nTừ giả lập trên, ta có thể dễ dàng nhận ra 1 số tính chất cơ bản và nền móng của stochastic process như sau:\nNếu xác suất là p=0.5, giá trị kỳ vọng bằng đúng với giá trị ban đầu. Đây là tính chất martingale cơ bản trong stochastic process $E(X_n) = E(X_0) \\text{ với } n \\ge 0$ . Với p \u0026gt; 0.5, ta có thể thấy xu hướng của chuỗi thời gian có chiều hướng lên (positive drift), và ngược lại với. p\u0026lt; 0.5, xu hướng có chiều hướng xuống (negative drift). Giá trị của $X_{n+1}$ chỉ phụ thuộc vào $X_{n}$, hay giá trị của ngày hôm sau chỉ phụ thuộc vào ngày hôm nay và không phụ thuộc vào quá khứ trước đó. Đây là tính chất Markov. Variance của chuỗi thời gian này mở rộng theo thời gian (hay phụ thuộc vào thời gian t). Ngoài lề: Monte carlo simulation xấp xỉ các giá trị cần thiết Ngoài ra, dựa vào phương pháp giả lập (Monte carlo) này, ta có thể xấp xỉ các giá trị xác suất, kỳ vọng như phương pháp closed-form solution như trên. Dựa trên lý thuyết số lớn (Law of large number), bằng cách lấy mẫu ngẫu nhiên nhiều lần, ta có thể mô phỏng lại các trường hợp có thể xảy ra nhằm tính toán các giá trị mong muốn.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # tham số initial_cap = 200 each_step = 10 expected_cap = 400 ## giả lập n = 2000 # (giả lập tới vô cực; 2000 là cũng đủ lớn) n_sim = 20000 # (giả lập nhiều lần; 20000 là cũng đủ lớn) capital_up = simulation_plot(0.55, initial_cap, each_step, expected_cap, n, n_sim, title = \u0026#39;Simulate Gambler Ruin with positive drift (p\u0026gt;0.5)\u0026#39;) capital_down = simulation_plot(0.45, initial_cap, each_step, expected_cap, n, n_sim, title = \u0026#39;Simulate Gambler Ruin with negative drift (p\u0026lt;0.5)\u0026#39;) capital_neutral = simulation_plot(0.5, initial_cap, each_step, expected_cap, n, n_sim, title = \u0026#39;Simulate Gambler Ruin with no drift (p=0.5)\u0026#39;) def stopping_time(capital, threshold): ls_positive = [] ls_negative = [] for i in range(capital.shape[0]): ls_positive.append(np.argmax(capital[i,:]\u0026gt;=threshold)) ls_negative.append(np.argmax(capital[i,:]\u0026lt;=0)) # ls = ls_positive + ls_negative ls = pd.DataFrame([ls_positive, ls_negative]).sum(axis = 0).mean() return ls ## tính toán xác suất unique, counts = np.unique(capital_neutral[:,-1], return_counts=True) print(counts/np.sum(counts)) unique, counts = np.unique(capital_up[:,-1], return_counts=True) print(counts/np.sum(counts)) unique, counts = np.unique(capital_down[:,-1], return_counts=True) print(counts/np.sum(counts)) ## Tính toán stopping time stopping_time(capital_neutral, 400), stopping_time(capital_up, 400), stopping_time(capital_down, 400) Kết quả giả lập so với sử dụng closed-form như sau:\nSucess probability Expected duration p Simulation Closed-form Simulation Closed-form 0.5 0.4944 0.5 395.27 400 0.55 0.9803 0.98 190.263 192.9 0.45 0.0157 0.02 190.594 192.9 Từ đó, ta có thể thấy rằng, với những bài toán chưa có một cách giải “đẹp”, ta có thể tiến hành giả lập các trường hợp xảy ra như một phương án chữa cháy để tính toán các giá trị mong muốn. Tuy nhiên, ta phải đánh đổi bằng tốc độ và độ “đẹp” của kết quả.\nKết luận Okay, đã đủ cho bài giới thiệu về stochastic process rồi. Chúng ta đã cùng nhau đi qua nhiều thứ nền móng: (1) Gambler’s ruin để giới thiệu về stochastic process; (2) Giả lập cho VN30F1M; (3) Monte Carlo để đi xấp xỉ các giá trị cần thiết.\nThông qua bài viết trên, bạn đã nắm được gì:\nNền móng cho việc “gambling”: dựa vào xác suất, vốn của bạn, và kỳ vọng, bạn có thể gamble tốt hơn rồi đó. Trước khi bắt đầu bet vào một thứ gì đó, hãy chậm lại 1 bước, suy nghĩ về toán một tí, rồi mới quyết định chơi hay không. Hay đúng không nào ^^ Khi gặp 1 bài toán và bí. Hãy đi giả lập nó để xấp xỉ trước kết quả cuối cùng. Strategy phái sinh có xác suất thắng dưới 50% thì nên xem xét lại. Nền móng cho vài tính chất cơ bản của stochastic process. Ngoài ra, mình có để những bài viết rất hay của chủ đề tương tự ở phần ref, bạn nên nghía qua để hiểu sâu hơn về phần toán ở phía dưới nhé!\nRef https://randomdeterminism.wordpress.com/2010/07/07/gamblers-ruin/\nhttps://web.mit.edu/neboat/Public/6.042/randomwalks.pdf\nhttps://sites.pitt.edu/~jdnorton/teaching/paradox/chapters/probability_from_expectation/gambler_ruin.pdf\nhttps://en.wikipedia.org/wiki/Monte_Carlo_method#:~:text=Sawilowsky distinguishes between a simulation,uses repeated sampling to obtain\n","permalink":"http://localhost:1313/posts/2024/2024-10-20-stochastic-process-part-1/","summary":"Bài viết giới thiệu về quá trình ngẫu nhiên thông qua bài toán Gambler\u0026rsquo;s Ruin. Bài viết cũng mô phỏng chiến lược giao dịch VN30F1M và sử dụng phương pháp Monte Carlo để xấp xỉ các giá trị cần thiết, nhấn mạnh tầm quan trọng của xác suất và kỳ vọng trong việc ra quyết định.","title":"Stochastic process part 1: Gambler's ruin of VN30F"},{"content":"Hôm nay, chúng ta sẽ cùng nhau thảo luận về các phương pháp khác nhau để giải quyết bài toán Linear regression - Hồi quy tuyến tính.\nVậy Linear regression là gì? Hồi quy tuyến tính là phương pháp thống kê dùng để mô hình hóa mối quan hệ giữa biến phụ thuộc và các biến độc lập. Hay nói cách khác, hồi quy tuyến tính là đi tìm mối quan hệ tuyến tính (y=ax+b) giữa 2 biến với nhau.\nỞ bài này, mình sẽ không bàn luận sâu về toán, các bạn có thể đọc ở đây để nắm lý thuyết cần thiết link. Thay vào đó, bài viết này sẽ đi thực hiện bài toán đơn giản này theo 3 cách tiếp cận khác nhau:\nSử dụng linear algebra để tính trực tiếp Thông qua deep learning, sử dụng đạo hàm Thông qua bayesian inference, sử dụng phương pháp lấy mẫu (sampling) Ngoài ra, bài viết sẽ áp dụng phương pháp này để đi xác định hệ số Beta cho cổ phiếu HPG. Các bạn có thể đọc về beta tại đây.\nỞ bài toán này, dữ liệu X sẽ là VNINDEX, trong khi y sẽ là HPG.\nVề mặt tài chính, điều này có thể được xem như là ước tính rủi ro thị trường, hay rủi ro hệ thống (market risk, systematic risk) cho HPG.\n1. Tính trực tiếp từ dữ liệu Lời giả cho phương pháp này là:\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Algebra_linear_regression: def __init__(self): self.coef_ = None self.intercept_ = None def fit(self, X, y): X = np.array(X) y = np.array(y) X = np.c_[np.ones(X.shape[0]), X] self.coef_ = np.linalg.inv(X.T @ X) @ X.T @ y self.intercept_ = self.coef_[0] self.coef_ = self.coef_[1:] def predict(self, X): X = np.array(X) return X @ self.coef_ + self.intercept_ def score(self, X, y): X = np.array(X) y = np.array(y) y_pred = self.predict(X) return 1 - ((y - y_pred) ** 2).sum() / ((y - y.mean()) ** 2).sum() model = Algebra_linear_regression() model.fit(X,y) model.coef_.round(3), model.intercept_.round(4) -\u0026gt; output: (array([[1.23]]), array([0.0005])) Từ phương pháp này, ta có thể ước lượng được rằng hệ số beta cho cổ phiếu HPG là 1.23 tương ứng với việc rủi ro thị trường tương đối cao.\n2. Sử dụng deep learning Deep learning cũng là một trong những phương pháp thông dụng có thể được sử dụng trong bài toán này. Thông qua thuật toán Gradient Descent, ta có đi tìm bộ tham số phù hợp với hàm mất mát (loss function) là thấp nhất.\nQuá trình đi tìm điểm tối ưu của bài toán này giống như khi bạn đi xuống núi. Ban đầu, bạn sẽ bắt đầu ở một điểm nào đó, tại mỗi điểm bạn sẽ luôn biết nên đi hướng nào. Mục tiêu của bạn sẽ đi xuống núi từng bước nhỏ một. Bạn cứ liên tục đi cho đến khi bạn đạt đến điểm trũng của thung lũng!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import torch import torch.nn as nn # Create class class LinearRegressionModel(nn.Module): def __init__(self, input_dim, output_dim): super(LinearRegressionModel, self).__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): out = self.linear(x) return out input_dim = 1 output_dim = 1 model = LinearRegressionModel(input_dim, output_dim) criterion = nn.MSELoss() learning_rate = 0.001 optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) epochs = 1000 losses = [] # List to store loss at each epoch for epoch in range(epochs): epoch += 1 # Convert numpy array to torch Variable inputs = torch.from_numpy(X).requires_grad_() labels = torch.from_numpy(y) # Clear gradients w.r.t. parameters optimizer.zero_grad() # Forward to get output outputs = model(inputs) # Calculate Loss loss = criterion(outputs, labels) # Getting gradients w.r.t. parameters loss.backward() # Updating parameters optimizer.step() losses.append(loss.item()) print(\u0026#39;epoch {}, loss {}\u0026#39;.format(epoch, loss.item())) Thông qua phương pháp này, ta cũng tìm được mối quan hệ giống hệt với phương pháp tính trực tiếp!\n3. Bayesian linear regression Thay vì chỉ đưa ra một giá trị cố định cho các hệ số như hồi quy tuyến tính thông thường, phương pháp bayes coi các hệ số là những giá trị có thể thay đổi và có xác suất xảy ra. Ban đầu, ta có một \u0026ldquo;niềm tin\u0026rdquo; về các tham số này (prior). Khi thu thập thêm dữ liệu, ta sẽ cập nhật niềm tin đó, từ đó giúp dự đoán chính xác hơn và biết mức độ chắc chắn của các kết quả (posterior). Khoá Coursera này sẽ cho bạn cái nhìn kĩ hơn về phương pháp này.\nĐể hiểu đơn giản hơn, ta có một ví dụ này: Với 2 phương pháp trên, hệ số beta của HPG là 1.23. Vậy có bao nhiêu phần trăm (%) hệ số beta là 1.23, hệ số beta lớn hơn 1 với độ tự tin bao nhiêu phần trăm (%)? Giá trị của hệ số beta của HPG có thể có giá trị từ đâu tới đâu với độ tư tin là bao nhiêu phần trăm (%)?\nĐây là một trong những vấn đề có thể được giải quyết thông qua phương pháp Bayes. Bạn có thể đọc kĩ hơn về ý tưởng đằng sau phương pháp này tại đây.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def numpyro_model(X, y): # Priors for the parameters num_features = X.shape[1] beta = numpyro.sample(\u0026#34;beta\u0026#34;, dist.Normal(0,1)) # Coefficients intercept = numpyro.sample(\u0026#34;intercept\u0026#34;, dist.Normal(0, 1)) # Intercept sigma = numpyro.sample(\u0026#34;sigma\u0026#34;, dist.Normal(0,1)) # Noise level # Linear model mean = jnp.dot(X, beta) + intercept # Likelihood numpyro.sample(\u0026#34;obs\u0026#34;, dist.Normal(mean, sigma), obs=y) # Instantiate a `MCMC` object using a NUTS sampler mcmc = MCMC(sampler=NUTS(numpyro_model), num_warmup=1000, num_samples=1000, num_chains=4) # Run the MCMC sampler and collect samples mcmc.run(rng_key=random.PRNGKey(seed=42), X=X, y=y) az.plot_trace(mcmc, var_names=[\u0026#39;intercept\u0026#39;, \u0026#39;beta\u0026#39;, \u0026#39;sigma\u0026#39;], figsize=(9,9)); $$ Y = \\beta_0 + \\beta_1 X + \\epsilon $$ Phương pháp biểu diễn các biến trong Bayesian inference (plate notation). Ở bài toán này, ta sẽ có 3 biến chính: beta, intercept và sigma. Tất cả đều được giả định là phân phối chuẩn.\nmean std median 5.0% 95.0% n_eff r_hat beta 1.23 0.03 1.23 1.18 1.28 734.39 1.01 intercept 0.00 0.00 0.00 -0.00 0.00 3212.78 1.00 sigma 0.02 0.00 0.02 0.02 0.02 5063.55 1.00 Hình này được gọi là trace plot. Phương pháp này được dùng để đánh giá quá trình hội tụ của mô hình Bayes. Cột bên trái cho ta thấy phân phối, trong khi bên phải cho ta thấy sự giao động của các tham số.\nTừ kết quả mô hình trên, ta ra được kết luận rằng, hệ số beta trung bình cho cổ phiếu HPG cũng là 1.23, và ta có thể tự tin nói rằng, 90% trường hợp hệ số beta của HPG sẽ trong vùng 1.18 - 1.23.\nKết luận Từ 3 phương pháp trên ta đều thu được chung 1 kết quả. Ngoài ra, mỗi phương pháp có ưu và nhược điểm của mình.\nVới phương pháp tính toán trực tiếp, ta có thể dễ dàng tính toán ra được đáp án với độ phức tạp là thấp nhất. Tuy nhiên, với những trường hợp mà dữ liệu là không khả nghịch (singular), thì bài toán có khả năng không có khả năng giải được theo phương pháp này (analytical solution).\nNgược lại, với phương pháp deep learning, ta có thể dễ dàng xử lý các bài toán khi không thể giải trực tiếp (analytical solution). Thông qua Gradient Descent, ta có thể ước tính được các điểm tối ưu, tuy nhiên đi kèm với đó là yêu cầu tính toán lớn hơn nhiều (computational cost).\nCuối cùng, phương pháp Bayesian có thể được sử dụng khi ta mong muốn đưa niềm tin của ta vào mô hình, cũng như đầu ra mong muốn là một phân phối các kết quả có thể xảy ra thay vì chỉ muốn một điểm ước lượng duy nhất. Quan trọng nhất là mô hình hoá được độ không chắc chắn (uncertainty). Tuy nhiên, yếu tố giả định của người sử dụng có thể có tác động tiêu cực đến mô hình nếu giả định sai.\nVà tất nhiên, 2 phương pháp sau là phức tạp hoá vấn đề cho bài toán đơn giản này. Tuy nhiên, để gần gũi nhất thì mình chọn ước lượng beta cho dễ hình dung!\n","permalink":"http://localhost:1313/posts/2024/2024-10-10-linear-regression/","summary":"\u003cp\u003eHôm nay, chúng ta sẽ cùng nhau thảo luận về các phương pháp khác nhau để giải quyết bài toán Linear regression - Hồi quy tuyến tính.\u003c/p\u003e\n\u003ch2 id=\"vậy-linear-regression-là-gì\"\u003eVậy Linear regression là gì?\u003c/h2\u003e\n\u003cp\u003eHồi quy tuyến tính là phương pháp thống kê dùng để mô hình hóa mối quan hệ giữa biến phụ thuộc và các biến độc lập. Hay nói cách khác, hồi quy tuyến tính là đi tìm mối quan hệ tuyến tính (y=ax+b) giữa 2 biến với nhau.\u003c/p\u003e","title":"Linear regression"},{"content":"Hi, mình là Hùng.\nHiện mình là sinh viên thạc sĩ AI tại RMIT Vietnam với đam mê dành cho mảng đầu tư định lượng (Quant).\nBlog này là nơi mình chia sẻ kiến thức (đa phần là lưu trữ cái mình học) và nếu nó giúp được bạn trong chuyến phiêu lưu ngành quant thì mình rất vui!\nCác mảng yêu thích Ứng dụng: Learning to Rank, Probabilistic forecasting, Network Science … Lý thuyết: Causal ML, Bayesian Inference, Stochastic process … Tài chính: Portfolio optimization, Asset pricing, Factor investing, … Rất mong muốn hợp tác để thực hiện publication (không giới hạn vào các chủ đề trên) Học vấn MSc Artificial Intelligent (RMIT Vietnam) BSc Economics \u0026amp; Finance (RMIT Vietnam) CFA lev 1 Self taught Data Sci \u0026amp; Quant! Liên hệ Email: hungha1412@gmail.com LinkedIn: qhung ","permalink":"http://localhost:1313/about/","summary":"\u003cp\u003eHi, mình là Hùng.\u003c/p\u003e\n\u003cp\u003eHiện mình là sinh viên thạc sĩ AI tại RMIT Vietnam với đam mê dành cho mảng đầu tư định lượng (Quant).\u003c/p\u003e\n\u003cp\u003eBlog này là nơi mình chia sẻ kiến thức (đa phần là lưu trữ cái mình học) và nếu nó giúp được bạn trong chuyến phiêu lưu ngành quant thì mình rất vui!\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"các-mảng-yêu-thích\"\u003eCác mảng yêu thích\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eỨng dụng: Learning to Rank, Probabilistic forecasting, Network Science …\u003c/li\u003e\n\u003cli\u003eLý thuyết: Causal ML, Bayesian Inference, Stochastic process …\u003c/li\u003e\n\u003cli\u003eTài chính: Portfolio optimization, Asset pricing, Factor investing, …\u003c/li\u003e\n\u003cli\u003eRất mong muốn hợp tác để thực hiện publication (không giới hạn vào các chủ đề trên)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003chr\u003e\n\u003ch2 id=\"học-vấn\"\u003eHọc vấn\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eMSc Artificial Intelligent (RMIT Vietnam)\u003c/li\u003e\n\u003cli\u003eBSc Economics \u0026amp; Finance (RMIT Vietnam)\u003c/li\u003e\n\u003cli\u003eCFA lev 1\u003c/li\u003e\n\u003cli\u003eSelf taught Data Sci \u0026amp; Quant!\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"liên-hệ\"\u003eLiên hệ\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eEmail: \u003ca href=\"mailto:hungha1412@gmail.com\"\u003ehungha1412@gmail.com\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eLinkedIn: \u003ca href=\"https://www.linkedin.com/in/haquochung11/\"\u003eqhung\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"About"},{"content":"","permalink":"http://localhost:1313/blog/","summary":"blog","title":"Blog"},{"content":"","permalink":"http://localhost:1313/research/","summary":"","title":"Research"}]