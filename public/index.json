[{"content":"Giá»›i thiá»‡u Stochastic process lÃ  ngÃ´n ngá»¯ Ä‘á»ƒ miÃªu táº£ sá»± ngáº«u nhiÃªn, cÅ©ng giá»‘ng nhÆ° Calculus lÃ  ngÃ´n ngá»¯ Ä‘á»ƒ miÃªu táº£ sá»± thay Ä‘á»•i cá»§a hÃ m sá»‘.\nBÃ i viáº¿t giá»›i thiá»‡u vá» stochastic process thÃ´ng qua bÃ i toÃ¡n Gambler\u0026rsquo;s Ruin. BÃ i viáº¿t cÅ©ng mÃ´ phá»ng chiáº¿n lÆ°á»£c giao dá»‹ch VN30F1M nhÆ° má»™t trÆ°á»ng há»£p cá»§a Gambler\u0026rsquo;s ruin vÃ  sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p Monte Carlo Ä‘á»ƒ xáº¥p xá»‰ cÃ¡c giÃ¡ trá»‹ cáº§n thiáº¿t, nháº¥n máº¡nh táº§m quan trá»ng cá»§a xÃ¡c suáº¥t vÃ  ká»³ vá»ng trong viá»‡c ra quyáº¿t Ä‘á»‹nh.\nConcept Gamblerâ€™s Ruin, hay Sá»± phÃ¡ sáº£n cá»§a con báº¡c, lÃ  má»™t trong nhá»¯ng concept cá»• Ä‘iá»ƒn trong lÃ½ thuyáº¿t xÃ¡c suáº¥t (probability theory) vÃ  quÃ¡ trÃ¬nh ngáº«u nhiÃªn (stochastic process). Váº¥n Ä‘á» nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ táº£ trong nhiá»u trÆ°á»ng há»£p, trong Ä‘Ã³ phá»• biáº¿n nháº¥t lÃ :\nMá»™t con báº¡c bÆ°á»›c vÃ o sÃ²ng báº¡c vá»›i sá»‘ tiá»n $n trong tay vÃ  báº¯t Ä‘áº§u chÆ¡i má»™t trÃ² chÆ¡i, trong Ä‘Ã³ anh ta tháº¯ng vá»›i xÃ¡c suáº¥t p vÃ  thua vá»›i xÃ¡c suáº¥t $q = 1-p$. NgÆ°á»i chÆ¡i láº·p láº¡i trÃ² chÆ¡i nÃ y nhiá»u láº§n, Ä‘áº·t cÆ°á»£c $1 má»—i lÆ°á»£t. Anh ta sáº½ rá»i khá»i trÃ² chÆ¡i náº¿u tá»•ng sá»‘ tiá»n cá»§a anh ta Ä‘áº¡t Ä‘áº¿n $N hoáº·c náº¿u anh ta háº¿t tiá»n (phÃ¡ sáº£n), tÃ¹y thuá»™c vÃ o Ä‘iá»u gÃ¬ xáº£y ra trÆ°á»›c. XÃ¡c suáº¥t mÃ  con báº¡c bá»‹ phÃ¡ sáº£n hoáº·c tháº¯ng chung cuá»™c lÃ  bao nhiÃªu?\nXÃ¡c suáº¥t con báº¡c tháº¯ng cáº£ tráº­n Gamblerâ€™s Ruin cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ hÃ¬nh hoÃ¡ nhÆ° má»™t bÆ°á»›c Ä‘i ngáº«u nhiÃªn (random walk) mÃ  á»Ÿ Ä‘Ã³ chÃºng ta quan tÃ¢m Ä‘áº¿n xÃ¡c suáº¥t ngÆ°á»i chÆ¡i sáº½ tháº¯ng khi Ä‘áº¡t Ä‘Æ°á»£c $N mong muá»‘n. á» bÃ i viáº¿t nÃ y, mÃ¬nh sáº½ sá»­ dá»¥ng káº¿t quáº£, bÆ°á»›c giáº£i chi tiáº¿t cÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o táº¡i Ä‘Ã¢y.\nVá»›i $P_N(n)$ lÃ  xÃ¡c suáº¥t ngÆ°á»i chÆ¡i sáº½ Ä‘áº¡t Ä‘Æ°á»£c $N vá»›i sá»‘ tiá»n hiá»‡n táº¡i lÃ  $n. TÆ°Æ¡ng tá»±, $P_N(n+1)$ lÃ  xÃ¡c suáº¥t ngÆ°á»i chÆ¡i sáº½ Ä‘áº¡t Ä‘Æ°á»£c $N vá»›i sá»‘ tiá»n hiá»‡n táº¡i lÃ  $n+1. $p$ lÃ  xÃ¡c suáº¥t tháº¯ng 1 tráº­n, $q = 1 - p$ lÃ  xÃ¡c suáº¥t thua cá»§a 1 tráº­n.\n$$ P(\\text{sucess}) = P(\\text{sucess}| \\text{win first round}) P(\\text{win first round})\\\\ + P(\\text{sucess}| \\text{lose first round}) P(\\text{lose first round}) \\\\ P_N(n) = P(n| W) P(W) + P(n| L) P(L) \\\\ P_N(n) = P_N(n+1) p + P_N(n-1) q $$Vá»›i $\\lambda = \\frac{q}{p}$, xÃ¡c suáº¥t ngÆ°á»i chÆ¡i sáº½ Ä‘áº¡t Ä‘Æ°á»£c N lÃ :\n$$ P_N(n) = \\begin{cases} \\frac{1 - \\lambda^n}{1 - \\lambda^N}, \u0026 \\lambda \\neq 1 \\\\ \\frac{n}{N}, \u0026 \\lambda = 1 \\end{cases} \\tag{1} $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def win_probability(p, initial_cap, expected_cap): assert 0\u0026lt;= p \u0026lt;=1 , \u0026#34;`p` must be a probability between 0 and 1.\u0026#34; assert 0 \u0026lt;= initial_cap \u0026lt;= expected_cap, \u0026#34;`` an initial_cap integer between 0 and expected_cap.\u0026#34; very_small_number = 1e-12 lambda_ = (1-p)/p if p\u0026lt;= very_small_number: return 0 if p\u0026gt;= 1-very_small_number: return 1 if lambda_==1: return initial_cap/expected_cap return (1-lambda_**initial_cap)/(1-lambda_**expected_cap) BÃ i toÃ¡n giáº£ Ä‘á»‹nh nhÆ° sau:\nGiáº£ sá»­ trÆ°á»ng há»£p sau, anh A cÃ³ 10 Ä‘á»“ng vÃ  quyáº¿t Ä‘á»‹nh Ä‘i Ä‘Ã¡nh black jack (xÃ¬ dÃ¡ch) vá»›i má»¥c tiÃªu sáº½ gáº¥p Ä‘Ã´i sá»‘ tiá»n (20 Ä‘á»“ng), má»—i tráº­n tháº¯ng/thua anh A sáº½ lá»i/máº¥t 1 Ä‘á»“ng. Anh A sá»­ dá»¥ng chiáº¿n thuáº­t vá»›i xÃ¡c suáº¥t tháº¯ng trong 1 vÃ¡n lÃ  (a) 50%, (b) 55%, (c) 45% thÃ¬ xÃ¡c suáº¥t anh A Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu 20 Ä‘á»“ng lÃ  bao nhiÃªu.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ### Closed-form solution p = 0.5 initial_cap = 10 expected_cap = 20 win_rate = round(win_probability(p, initial_cap, expected_cap),5) p = 0.55 win_rate = round(win_probability(p, initial_cap, expected_cap),5) p = 0.45 win_rate = round(win_probability(p, initial_cap, expected_cap),5) ----------------------------------- Output: Win rate: 0.5, Initial capital: 10, Expected capital: 20 Sucess rate: 50% ----------------------------------- Win rate: 0.55, Initial capital: 10, Expected capital: 20 Sucess rate: 88.15% ----------------------------------- Win rate: 0.45, Initial capital: 10, Expected capital: 20 Sucess rate: 11.85% ----------------------------------- Tá»« cÃ´ng thá»©c trÃªn, ta cÃ³ thá»ƒ tÃ­nh toÃ¡n Ä‘Æ°á»£c xÃ¡c suáº¥t con báº¡c tháº¯ng chung cuá»™c. Váº­y, trong thá»±c táº¿, nÃ³ sáº½ â€œtrÃ´ngâ€ nhÆ° tháº¿ nÃ o nhá»‰?\nTa tiáº¿n hÃ nh giáº£ láº­p chuá»—i thá»i gian cá»§a bÃ i toÃ¡n nÃ y. Tá»« xÃ¡c suáº¥t trÃªn, ta cÃ³ thá»ƒ tÃ­nh toÃ¡n Ä‘Æ°á»£c nhiá»u thá»©. GiÃ¡ trá»‹ kÃ¬ vá»ng trong 3 trÆ°á»ng há»£p 0.5, 0.55, 0.45 láº§n lÆ°á»£t lÃ  10, 18, 2. Vá»›i viá»‡c giáº£ láº­p 30 láº§n, ta cÃ³ thá»ƒ tháº¥y giÃ¡ trá»‹ trung bÃ¬nh (hay kÃ¬ vá»ng) cÅ©ng tiáº¿n tá»›i má»©c nÃ y.\nNgoÃ i ra, dá»±a trÃªn giáº£ láº­p trÃªn, ta cÅ©ng cÃ³ thá»ƒ nháº­n tháº¥y 1 tÃ­nh cháº¥t ráº±ng, sáº½ tá»›i 1 thá»i gian lÃ  cÃ¡c chuá»—i sáº½ Ä‘áº¡t tá»›i Ä‘iá»ƒm dá»«ng (absorbing state). NÃ´m na lÃ  thá»i Ä‘iá»ƒm kÃ¬ vá»ng (hay trung bÃ¬nh) con báº¡c Ä‘áº¡t Ä‘Æ°á»£c $N hoáº·c thua háº¿t â€œxÃ¨ngâ€.\nVá»›i S lÃ  thá»i gian ká»³ vá»ng, D lÃ  bÆ°á»›c (step), ta cÃ³:\n$$ \\begin{align*} E(\\text{duration}) = E(\\text{duration}| \\text{win first round}) P(\\text{win first round}) \\\\+ E(\\text{duration}| \\text{lose first round}) P(\\text{lose first round})\\\\ \\end{align*}\\\\ $$$$ \\begin{align*} E_n(S) \u0026= E(S|D_1=n+1)p +E(S|D_1=n-1)q\\\\ \u0026= (1+E(S|D_0=n+1))p + (1+E(S|D_0=n-1))q\\\\ \u0026= p+q+E(S|D_0=n+1)p +E(S|D_0=n-1)q\\\\ \u0026= 1+E_{n+1}(S)p +E_{n-1}(S)q \\end{align*} $$$$ \\begin{align*} E_n(S) = \\frac{n}{q - p} - \\frac{N}{q - p} \\cdot \\frac{(\\frac{q}{p})^n - 1}{(\\frac{q}{p})^N - 1} \\end{align*} $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def expected_duration(p, initial_cap, expected_cap): assert 0\u0026lt;= p \u0026lt;=1 , \u0026#34;`p` must be a probability between 0 and 1.\u0026#34; assert 0 \u0026lt;= initial_cap \u0026lt;= expected_cap, \u0026#34;`` an initial_cap integer between 0 and expected_cap.\u0026#34; very_small_number = 1e-12 q = 1-p lambda_ = q/p if lambda_==1: return initial_cap*(expected_cap - initial_cap) duration = ( initial_cap/(q-p) - expected_cap/(q-p)* ((lambda_**initial_cap-1)/(lambda_**expected_cap-1)) ) return duration ------- initial_cap = 10 expected_cap = 20 Win rate: 0.5, Expected duration: 100 Win rate: 0.45, Expected duration: 76.3 Win rate: 0.55, Expected duration: 76.3 Váº­y, ta cÃ³ thá»ƒ tá»± tin nÃ³i ráº±ng: Náº¿u xÃ¡c suáº¥t cá»§a vÃ¡n bÃ i lÃ  50/50 cho má»—i vÃ¡n, thÃ¬ ká»³ vá»ng con báº¡c sáº½ dá»«ng cuá»™c chÆ¡i (cáº£ tháº¯ng láº«n thua) sáº½ lÃ  sau 100 vÃ¡n. CÃ²n náº¿u xÃ¡c suáº¥t lÃ  45% hoáº·c 55% thÃ¬ (dá»± kiáº¿n) sau 77 vÃ¡n con báº¡c sáº½ dá»«ng cuá»™c chÆ¡i.\nVN30F1M Bá»‘i cáº£nh vá» con báº¡c Ä‘Ã£ xong, giá» ta qua tá»›i VN30F1. Ta sáº½ trade vá»›i chiáº¿n lÆ°á»£c siÃªu Ä‘Æ¡n giáº£n nhÆ° sau: Long giÃ¡ má»Ÿ cá»­a (Open) vÃ  Ä‘Ã³ng giÃ¡ Ä‘Ã³ng cá»­a (Close). Vá»›i dá»¯ liá»‡u daily tá»« 2018 tá»›i nay, ta thu Ä‘Æ°á»£c cÃ¡c káº¿t quáº£ nhÆ° sau:\nTrung bÃ¬nh tÄƒng: 9.21 Trung bÃ¬nh giáº£m: -10.14 Sá»‘ ngÃ y tÄƒng: 777; Sá»‘ ngÃ y giáº£m 734 XÃ¡c suáº¥t tÄƒng giáº£m háº±ng ngÃ y: 51% CÅ©ng khÃ¡ tÆ°Æ¡ng Ä‘á»“ng vá»›i bÃ i toÃ¡n black jack á»Ÿ trÃªn ğŸ™‚. Tá»« cÃ¡c tham sá»‘ trÃªn, ta mÃ´ hÃ¬nh hoÃ¡ nhÆ° sau: XÃ¡c suáº¥t lá»i lá»— lÃ  50%, má»—i ngÃ y tÄƒng giáº£m trung bÃ¬nh 10 Ä‘iá»ƒm. Giáº£ sá»­ báº¡n 1 sá»‘ tiá»n Ä‘á»§ Ä‘á»ƒ báº¡n â€œriskâ€ 200 Ä‘iá»ƒm (~200tr). Báº¡n ká»³ vá»ng sáº½ gáº¥p Ä‘Ã´i trong 400 Ä‘iá»ƒm. Äá»ƒ Ä‘Æ°a vá» bÃ i toÃ¡n gamblerâ€™s ruin, ta cáº§n chuáº©n hoÃ¡ lá»i lá»— vá» 1 Ä‘iá»ƒm. Tá»« Ä‘Ã³, thÃ´ng sá»‘ cá»§a bÃ i toÃ¡n sáº½ lÃ  p=0.5, n=20, N= 40.\nSucess_prob = n/N = 20/40 = 50%\nExpected_duration = n*(N-n) = 20*(40-20) = 400 (days)\nLá»£i nhuáº­n ká»³ vá»ng: 40 * 50% + 0 * 50% = 20 (báº±ng sá»‘ vá»‘n ban Ä‘áº§u)\nTá»« Ä‘Ã³ cÃ´ng thá»©c ta cÃ³ thá»ƒ suy luáº­n cÃ¡c Ã½ nhÆ° sau:\nKhi báº¡n ká»³ vá»ng cÃ ng cao (so vá»›i sá»‘ tiá»n báº¡n cÃ³) thÃ¬ xÃ¡c suáº¥t báº¡n thÃ nh cÃ´ng cÃ ng tháº¥p vÃ  thá»i gian dá»± kiáº¿n báº¡n lá»— sáº¡ch cÃ ng nhanh. Báº¡n muá»‘n tÄƒng xÃ¡c suáº¥t thÃ nh cÃ´ng thÃ¬ nÃªn cÃ³ vá»‘n dÃ i (hay cÃ³ nhiá»u tiá»n) ğŸ˜ƒ TÄƒng xÃ¡c suáº¥t tháº¯ng cho tá»«ng láº§n trade thÃ¬ lá»£i nhuáº­n ká»³ vá»ng tÄƒng vÃ  xÃ¡c suáº¥t báº¡n thua sáº¡ch cÃ ng tháº¥p. Giáº£ láº­p VN30F1M cho cÃ¡c trÆ°á»ng há»£p xÃ¡c suáº¥t tháº¯ng cho tá»«ng láº§n trade lÃ  55%, 45% vÃ  50%.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def simulation_plot(p, initial_cap, each_step, expected_cap, n, n_sim, expected_stopping = None,title = \u0026#39;Simulate Gambler Ruin\u0026#39;): capital = np.zeros((n_sim, n)) for i in range(n_sim): capital[i] = simulate_gambler_ruin(p, initial_cap, expected_cap, each_step, n) plt.figure(figsize=(15,5), dpi = 200) plt.plot(capital.T, alpha = 0.75) ## Mean and standard deviation plt.hlines(initial_cap, 0, n, colors=\u0026#39;black\u0026#39;, linestyles=\u0026#39;dashed\u0026#39;, label=\u0026#39;Initial capital\u0026#39;) # plt.fill_between(np.arange(n), np.min(capital, axis = 0), np.max(capital, axis = 0), color = \u0026#39;gray\u0026#39;, alpha = 0.5, label = \u0026#39;Mean +/- std\u0026#39;) #Mean plt.plot(np.mean(capital, axis = 0), color = \u0026#39;black\u0026#39;, linewidth = 2, label = \u0026#39;Mean\u0026#39;) if expected_stopping: plt.vlines(expected_stopping,0, expected_cap, colors=\u0026#39;black\u0026#39;, label=\u0026#39;Expected stopping point\u0026#39;) plt.title(title) plt.xlabel(\u0026#39;tradingDate\u0026#39;) plt.ylabel(\u0026#39;Cumulative points\u0026#39;) plt.show() return capital initial_cap = 200 each_step = 10 expected_cap = 400 n= 500 n_sim = 100 Vá»›i chiáº¿n lÆ°á»£c cÃ³ xÃ¡c suáº¥t lá»i 55%, chá»‰ cÃ³ 2 láº§n lÃ  báº¡n thua sáº¡ch tiá»n trong sá»‘ 100 láº§n giáº£ láº­p.\nTá»« giáº£ láº­p trÃªn, ta cÃ³ thá»ƒ dá»… dÃ ng nháº­n ra 1 sá»‘ tÃ­nh cháº¥t cÆ¡ báº£n vÃ  ná»n mÃ³ng cá»§a stochastic process nhÆ° sau:\nNáº¿u xÃ¡c suáº¥t lÃ  p=0.5, giÃ¡ trá»‹ ká»³ vá»ng báº±ng Ä‘Ãºng vá»›i giÃ¡ trá»‹ ban Ä‘áº§u. ÄÃ¢y lÃ  tÃ­nh cháº¥t martingale cÆ¡ báº£n trong stochastic process $E(X_n) = E(X_0) \\text{ vá»›i } n \\ge 0$ . Vá»›i p \u0026gt; 0.5, ta cÃ³ thá»ƒ tháº¥y xu hÆ°á»›ng cá»§a chuá»—i thá»i gian cÃ³ chiá»u hÆ°á»›ng lÃªn (positive drift), vÃ  ngÆ°á»£c láº¡i vá»›i. p\u0026lt; 0.5, xu hÆ°á»›ng cÃ³ chiá»u hÆ°á»›ng xuá»‘ng (negative drift). GiÃ¡ trá»‹ cá»§a $X_{n+1}$ chá»‰ phá»¥ thuá»™c vÃ o $X_{n}$, hay giÃ¡ trá»‹ cá»§a ngÃ y hÃ´m sau chá»‰ phá»¥ thuá»™c vÃ o ngÃ y hÃ´m nay vÃ  khÃ´ng phá»¥ thuá»™c vÃ o quÃ¡ khá»© trÆ°á»›c Ä‘Ã³. ÄÃ¢y lÃ  tÃ­nh cháº¥t Markov. Variance cá»§a chuá»—i thá»i gian nÃ y má»Ÿ rá»™ng theo thá»i gian (hay phá»¥ thuá»™c vÃ o thá»i gian t). NgoÃ i lá»: Monte carlo simulation xáº¥p xá»‰ cÃ¡c giÃ¡ trá»‹ cáº§n thiáº¿t NgoÃ i ra, dá»±a vÃ o phÆ°Æ¡ng phÃ¡p giáº£ láº­p (Monte carlo) nÃ y, ta cÃ³ thá»ƒ xáº¥p xá»‰ cÃ¡c giÃ¡ trá»‹ xÃ¡c suáº¥t, ká»³ vá»ng nhÆ° phÆ°Æ¡ng phÃ¡p closed-form solution nhÆ° trÃªn. Dá»±a trÃªn lÃ½ thuyáº¿t sá»‘ lá»›n (Law of large number), báº±ng cÃ¡ch láº¥y máº«u ngáº«u nhiÃªn nhiá»u láº§n, ta cÃ³ thá»ƒ mÃ´ phá»ng láº¡i cÃ¡c trÆ°á»ng há»£p cÃ³ thá»ƒ xáº£y ra nháº±m tÃ­nh toÃ¡n cÃ¡c giÃ¡ trá»‹ mong muá»‘n.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # tham sá»‘ initial_cap = 200 each_step = 10 expected_cap = 400 ## giáº£ láº­p n = 2000 # (giáº£ láº­p tá»›i vÃ´ cá»±c; 2000 lÃ  cÅ©ng Ä‘á»§ lá»›n) n_sim = 20000 # (giáº£ láº­p nhiá»u láº§n; 20000 lÃ  cÅ©ng Ä‘á»§ lá»›n) capital_up = simulation_plot(0.55, initial_cap, each_step, expected_cap, n, n_sim, title = \u0026#39;Simulate Gambler Ruin with positive drift (p\u0026gt;0.5)\u0026#39;) capital_down = simulation_plot(0.45, initial_cap, each_step, expected_cap, n, n_sim, title = \u0026#39;Simulate Gambler Ruin with negative drift (p\u0026lt;0.5)\u0026#39;) capital_neutral = simulation_plot(0.5, initial_cap, each_step, expected_cap, n, n_sim, title = \u0026#39;Simulate Gambler Ruin with no drift (p=0.5)\u0026#39;) def stopping_time(capital, threshold): ls_positive = [] ls_negative = [] for i in range(capital.shape[0]): ls_positive.append(np.argmax(capital[i,:]\u0026gt;=threshold)) ls_negative.append(np.argmax(capital[i,:]\u0026lt;=0)) # ls = ls_positive + ls_negative ls = pd.DataFrame([ls_positive, ls_negative]).sum(axis = 0).mean() return ls ## tÃ­nh toÃ¡n xÃ¡c suáº¥t unique, counts = np.unique(capital_neutral[:,-1], return_counts=True) print(counts/np.sum(counts)) unique, counts = np.unique(capital_up[:,-1], return_counts=True) print(counts/np.sum(counts)) unique, counts = np.unique(capital_down[:,-1], return_counts=True) print(counts/np.sum(counts)) ## TÃ­nh toÃ¡n stopping time stopping_time(capital_neutral, 400), stopping_time(capital_up, 400), stopping_time(capital_down, 400) Káº¿t quáº£ giáº£ láº­p so vá»›i sá»­ dá»¥ng closed-form nhÆ° sau:\nSucess probability Expected duration p Simulation Closed-form Simulation Closed-form 0.5 0.4944 0.5 395.27 400 0.55 0.9803 0.98 190.263 192.9 0.45 0.0157 0.02 190.594 192.9 Tá»« Ä‘Ã³, ta cÃ³ thá»ƒ tháº¥y ráº±ng, vá»›i nhá»¯ng bÃ i toÃ¡n chÆ°a cÃ³ má»™t cÃ¡ch giáº£i â€œÄ‘áº¹pâ€, ta cÃ³ thá»ƒ tiáº¿n hÃ nh giáº£ láº­p cÃ¡c trÆ°á»ng há»£p xáº£y ra nhÆ° má»™t phÆ°Æ¡ng Ã¡n chá»¯a chÃ¡y Ä‘á»ƒ tÃ­nh toÃ¡n cÃ¡c giÃ¡ trá»‹ mong muá»‘n. Tuy nhiÃªn, ta pháº£i Ä‘Ã¡nh Ä‘á»•i báº±ng tá»‘c Ä‘á»™ vÃ  Ä‘á»™ â€œÄ‘áº¹pâ€ cá»§a káº¿t quáº£.\nKáº¿t luáº­n Okay, Ä‘Ã£ Ä‘á»§ cho bÃ i giá»›i thiá»‡u vá» stochastic process rá»“i. ChÃºng ta Ä‘Ã£ cÃ¹ng nhau Ä‘i qua nhiá»u thá»© ná»n mÃ³ng: (1) Gamblerâ€™s ruin Ä‘á»ƒ giá»›i thiá»‡u vá» stochastic process; (2) Giáº£ láº­p cho VN30F1M; (3) Monte Carlo Ä‘á»ƒ Ä‘i xáº¥p xá»‰ cÃ¡c giÃ¡ trá»‹ cáº§n thiáº¿t.\nThÃ´ng qua bÃ i viáº¿t trÃªn, báº¡n Ä‘Ã£ náº¯m Ä‘Æ°á»£c gÃ¬:\nNá»n mÃ³ng cho viá»‡c â€œgamblingâ€: dá»±a vÃ o xÃ¡c suáº¥t, vá»‘n cá»§a báº¡n, vÃ  ká»³ vá»ng, báº¡n cÃ³ thá»ƒ gamble tá»‘t hÆ¡n rá»“i Ä‘Ã³. TrÆ°á»›c khi báº¯t Ä‘áº§u bet vÃ o má»™t thá»© gÃ¬ Ä‘Ã³, hÃ£y cháº­m láº¡i 1 bÆ°á»›c, suy nghÄ© vá» toÃ¡n má»™t tÃ­, rá»“i má»›i quyáº¿t Ä‘á»‹nh chÆ¡i hay khÃ´ng. Hay Ä‘Ãºng khÃ´ng nÃ o ^^ Khi gáº·p 1 bÃ i toÃ¡n vÃ  bÃ­. HÃ£y Ä‘i giáº£ láº­p nÃ³ Ä‘á»ƒ xáº¥p xá»‰ trÆ°á»›c káº¿t quáº£ cuá»‘i cÃ¹ng. Strategy phÃ¡i sinh cÃ³ xÃ¡c suáº¥t tháº¯ng dÆ°á»›i 50% thÃ¬ nÃªn xem xÃ©t láº¡i. Ná»n mÃ³ng cho vÃ i tÃ­nh cháº¥t cÆ¡ báº£n cá»§a stochastic process. NgoÃ i ra, mÃ¬nh cÃ³ Ä‘á»ƒ nhá»¯ng bÃ i viáº¿t ráº¥t hay cá»§a chá»§ Ä‘á» tÆ°Æ¡ng tá»± á»Ÿ pháº§n ref, báº¡n nÃªn nghÃ­a qua Ä‘á»ƒ hiá»ƒu sÃ¢u hÆ¡n vá» pháº§n toÃ¡n á»Ÿ phÃ­a dÆ°á»›i nhÃ©!\nRef https://randomdeterminism.wordpress.com/2010/07/07/gamblers-ruin/\nhttps://web.mit.edu/neboat/Public/6.042/randomwalks.pdf\nhttps://sites.pitt.edu/~jdnorton/teaching/paradox/chapters/probability_from_expectation/gambler_ruin.pdf\nhttps://en.wikipedia.org/wiki/Monte_Carlo_method#:~:text=Sawilowsky distinguishes between a simulation,uses repeated sampling to obtain\n","permalink":"http://localhost:1313/posts/2024/2024-10-20-stochastic-process-part-1/","summary":"BÃ i viáº¿t giá»›i thiá»‡u vá» quÃ¡ trÃ¬nh ngáº«u nhiÃªn thÃ´ng qua bÃ i toÃ¡n Gambler\u0026rsquo;s Ruin. BÃ i viáº¿t cÅ©ng mÃ´ phá»ng chiáº¿n lÆ°á»£c giao dá»‹ch VN30F1M vÃ  sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p Monte Carlo Ä‘á»ƒ xáº¥p xá»‰ cÃ¡c giÃ¡ trá»‹ cáº§n thiáº¿t, nháº¥n máº¡nh táº§m quan trá»ng cá»§a xÃ¡c suáº¥t vÃ  ká»³ vá»ng trong viá»‡c ra quyáº¿t Ä‘á»‹nh.","title":"Stochastic process part 1: Gambler's ruin of VN30F"},{"content":"HÃ´m nay, chÃºng ta sáº½ cÃ¹ng nhau tháº£o luáº­n vá» cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhau Ä‘á»ƒ giáº£i quyáº¿t bÃ i toÃ¡n Linear regression - Há»“i quy tuyáº¿n tÃ­nh.\nVáº­y Linear regression lÃ  gÃ¬? Há»“i quy tuyáº¿n tÃ­nh lÃ  phÆ°Æ¡ng phÃ¡p thá»‘ng kÃª dÃ¹ng Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a má»‘i quan há»‡ giá»¯a biáº¿n phá»¥ thuá»™c vÃ  cÃ¡c biáº¿n Ä‘á»™c láº­p. Hay nÃ³i cÃ¡ch khÃ¡c, há»“i quy tuyáº¿n tÃ­nh lÃ  Ä‘i tÃ¬m má»‘i quan há»‡ tuyáº¿n tÃ­nh (y=ax+b) giá»¯a 2 biáº¿n vá»›i nhau.\ná» bÃ i nÃ y, mÃ¬nh sáº½ khÃ´ng bÃ n luáº­n sÃ¢u vá» toÃ¡n, cÃ¡c báº¡n cÃ³ thá»ƒ Ä‘á»c á»Ÿ Ä‘Ã¢y Ä‘á»ƒ náº¯m lÃ½ thuyáº¿t cáº§n thiáº¿t link. Thay vÃ o Ä‘Ã³, bÃ i viáº¿t nÃ y sáº½ Ä‘i thá»±c hiá»‡n bÃ i toÃ¡n Ä‘Æ¡n giáº£n nÃ y theo 3 cÃ¡ch tiáº¿p cáº­n khÃ¡c nhau:\nSá»­ dá»¥ng linear algebra Ä‘á»ƒ tÃ­nh trá»±c tiáº¿p ThÃ´ng qua deep learning, sá»­ dá»¥ng Ä‘áº¡o hÃ m ThÃ´ng qua bayesian inference, sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p láº¥y máº«u (sampling) NgoÃ i ra, bÃ i viáº¿t sáº½ Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p nÃ y Ä‘á»ƒ Ä‘i xÃ¡c Ä‘á»‹nh há»‡ sá»‘ Beta cho cá»• phiáº¿u HPG. CÃ¡c báº¡n cÃ³ thá»ƒ Ä‘á»c vá» beta táº¡i Ä‘Ã¢y.\ná» bÃ i toÃ¡n nÃ y, dá»¯ liá»‡u X sáº½ lÃ  VNINDEX, trong khi y sáº½ lÃ  HPG.\nVá» máº·t tÃ i chÃ­nh, Ä‘iá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° lÃ  Æ°á»›c tÃ­nh rá»§i ro thá»‹ trÆ°á»ng, hay rá»§i ro há»‡ thá»‘ng (market risk, systematic risk) cho HPG.\n1. TÃ­nh trá»±c tiáº¿p tá»« dá»¯ liá»‡u Lá»i giáº£ cho phÆ°Æ¡ng phÃ¡p nÃ y lÃ :\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top yÂ $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Algebra_linear_regression: def __init__(self): self.coef_ = None self.intercept_ = None def fit(self, X, y): X = np.array(X) y = np.array(y) X = np.c_[np.ones(X.shape[0]), X] self.coef_ = np.linalg.inv(X.T @ X) @ X.T @ y self.intercept_ = self.coef_[0] self.coef_ = self.coef_[1:] def predict(self, X): X = np.array(X) return X @ self.coef_ + self.intercept_ def score(self, X, y): X = np.array(X) y = np.array(y) y_pred = self.predict(X) return 1 - ((y - y_pred) ** 2).sum() / ((y - y.mean()) ** 2).sum() model = Algebra_linear_regression() model.fit(X,y) model.coef_.round(3), model.intercept_.round(4) -\u0026gt; output: (array([[1.23]]), array([0.0005])) Tá»« phÆ°Æ¡ng phÃ¡p nÃ y, ta cÃ³ thá»ƒ Æ°á»›c lÆ°á»£ng Ä‘Æ°á»£c ráº±ng há»‡ sá»‘ beta cho cá»• phiáº¿u HPG lÃ  1.23 tÆ°Æ¡ng á»©ng vá»›i viá»‡c rá»§i ro thá»‹ trÆ°á»ng tÆ°Æ¡ng Ä‘á»‘i cao.\n2. Sá»­ dá»¥ng deep learning Deep learning cÅ©ng lÃ  má»™t trong nhá»¯ng phÆ°Æ¡ng phÃ¡p thÃ´ng dá»¥ng cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng trong bÃ i toÃ¡n nÃ y. ThÃ´ng qua thuáº­t toÃ¡n Gradient Descent, ta cÃ³ Ä‘i tÃ¬m bá»™ tham sá»‘ phÃ¹ há»£p vá»›i hÃ m máº¥t mÃ¡t (loss function) lÃ  tháº¥p nháº¥t.\nQuÃ¡ trÃ¬nh Ä‘i tÃ¬m Ä‘iá»ƒm tá»‘i Æ°u cá»§a bÃ i toÃ¡n nÃ y giá»‘ng nhÆ° khi báº¡n Ä‘i xuá»‘ng nÃºi. Ban Ä‘áº§u, báº¡n sáº½ báº¯t Ä‘áº§u á»Ÿ má»™t Ä‘iá»ƒm nÃ o Ä‘Ã³, táº¡i má»—i Ä‘iá»ƒm báº¡n sáº½ luÃ´n biáº¿t nÃªn Ä‘i hÆ°á»›ng nÃ o. Má»¥c tiÃªu cá»§a báº¡n sáº½ Ä‘i xuá»‘ng nÃºi tá»«ng bÆ°á»›c nhá» má»™t. Báº¡n cá»© liÃªn tá»¥c Ä‘i cho Ä‘áº¿n khi báº¡n Ä‘áº¡t Ä‘áº¿n Ä‘iá»ƒm trÅ©ng cá»§a thung lÅ©ng!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 import torch import torch.nn as nn # Create class class LinearRegressionModel(nn.Module): def __init__(self, input_dim, output_dim): super(LinearRegressionModel, self).__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): out = self.linear(x) return out input_dim = 1 output_dim = 1 model = LinearRegressionModel(input_dim, output_dim) criterion = nn.MSELoss() learning_rate = 0.001 optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) epochs = 1000 losses = [] # List to store loss at each epoch for epoch in range(epochs): epoch += 1 # Convert numpy array to torch Variable inputs = torch.from_numpy(X).requires_grad_() labels = torch.from_numpy(y) # Clear gradients w.r.t. parameters optimizer.zero_grad() # Forward to get output outputs = model(inputs) # Calculate Loss loss = criterion(outputs, labels) # Getting gradients w.r.t. parameters loss.backward() # Updating parameters optimizer.step() losses.append(loss.item()) print(\u0026#39;epoch {}, loss {}\u0026#39;.format(epoch, loss.item())) ThÃ´ng qua phÆ°Æ¡ng phÃ¡p nÃ y, ta cÅ©ng tÃ¬m Ä‘Æ°á»£c má»‘i quan há»‡ giá»‘ng há»‡t vá»›i phÆ°Æ¡ng phÃ¡p tÃ­nh trá»±c tiáº¿p!\n3. Bayesian linear regression Thay vÃ¬ chá»‰ Ä‘Æ°a ra má»™t giÃ¡ trá»‹ cá»‘ Ä‘á»‹nh cho cÃ¡c há»‡ sá»‘ nhÆ° há»“i quy tuyáº¿n tÃ­nh thÃ´ng thÆ°á»ng, phÆ°Æ¡ng phÃ¡p bayes coi cÃ¡c há»‡ sá»‘ lÃ  nhá»¯ng giÃ¡ trá»‹ cÃ³ thá»ƒ thay Ä‘á»•i vÃ  cÃ³ xÃ¡c suáº¥t xáº£y ra. Ban Ä‘áº§u, ta cÃ³ má»™t \u0026ldquo;niá»m tin\u0026rdquo; vá» cÃ¡c tham sá»‘ nÃ y (prior). Khi thu tháº­p thÃªm dá»¯ liá»‡u, ta sáº½ cáº­p nháº­t niá»m tin Ä‘Ã³, tá»« Ä‘Ã³ giÃºp dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c hÆ¡n vÃ  biáº¿t má»©c Ä‘á»™ cháº¯c cháº¯n cá»§a cÃ¡c káº¿t quáº£ (posterior). KhoÃ¡ Coursera nÃ y sáº½ cho báº¡n cÃ¡i nhÃ¬n kÄ© hÆ¡n vá» phÆ°Æ¡ng phÃ¡p nÃ y.\nÄá»ƒ hiá»ƒu Ä‘Æ¡n giáº£n hÆ¡n, ta cÃ³ má»™t vÃ­ dá»¥ nÃ y: Vá»›i 2 phÆ°Æ¡ng phÃ¡p trÃªn, há»‡ sá»‘ beta cá»§a HPG lÃ  1.23. Váº­y cÃ³ bao nhiÃªu pháº§n trÄƒm (%) há»‡ sá»‘ beta lÃ  1.23, há»‡ sá»‘ beta lá»›n hÆ¡n 1 vá»›i Ä‘á»™ tá»± tin bao nhiÃªu pháº§n trÄƒm (%)? GiÃ¡ trá»‹ cá»§a há»‡ sá»‘ beta cá»§a HPG cÃ³ thá»ƒ cÃ³ giÃ¡ trá»‹ tá»« Ä‘Ã¢u tá»›i Ä‘Ã¢u vá»›i Ä‘á»™ tÆ° tin lÃ  bao nhiÃªu pháº§n trÄƒm (%)?\nÄÃ¢y lÃ  má»™t trong nhá»¯ng váº¥n Ä‘á» cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i quyáº¿t thÃ´ng qua phÆ°Æ¡ng phÃ¡p Bayes. Báº¡n cÃ³ thá»ƒ Ä‘á»c kÄ© hÆ¡n vá» Ã½ tÆ°á»Ÿng Ä‘áº±ng sau phÆ°Æ¡ng phÃ¡p nÃ y táº¡i Ä‘Ã¢y.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def numpyro_model(X, y): # Priors for the parameters num_features = X.shape[1] beta = numpyro.sample(\u0026#34;beta\u0026#34;, dist.Normal(0,1)) # Coefficients intercept = numpyro.sample(\u0026#34;intercept\u0026#34;, dist.Normal(0, 1)) # Intercept sigma = numpyro.sample(\u0026#34;sigma\u0026#34;, dist.Normal(0,1)) # Noise level # Linear model mean = jnp.dot(X, beta) + intercept # Likelihood numpyro.sample(\u0026#34;obs\u0026#34;, dist.Normal(mean, sigma), obs=y) # Instantiate a `MCMC` object using a NUTS sampler mcmc = MCMC(sampler=NUTS(numpyro_model), num_warmup=1000, num_samples=1000, num_chains=4) # Run the MCMC sampler and collect samples mcmc.run(rng_key=random.PRNGKey(seed=42), X=X, y=y) az.plot_trace(mcmc, var_names=[\u0026#39;intercept\u0026#39;, \u0026#39;beta\u0026#39;, \u0026#39;sigma\u0026#39;], figsize=(9,9)); $$ Y = \\beta_0 + \\beta_1 X + \\epsilon $$ PhÆ°Æ¡ng phÃ¡p biá»ƒu diá»…n cÃ¡c biáº¿n trong Bayesian inference (plate notation). á» bÃ i toÃ¡n nÃ y, ta sáº½ cÃ³ 3 biáº¿n chÃ­nh: beta, intercept vÃ  sigma. Táº¥t cáº£ Ä‘á»u Ä‘Æ°á»£c giáº£ Ä‘á»‹nh lÃ  phÃ¢n phá»‘i chuáº©n.\nmean std median 5.0% 95.0% n_eff r_hat beta 1.23 0.03 1.23 1.18 1.28 734.39 1.01 intercept 0.00 0.00 0.00 -0.00 0.00 3212.78 1.00 sigma 0.02 0.00 0.02 0.02 0.02 5063.55 1.00 HÃ¬nh nÃ y Ä‘Æ°á»£c gá»i lÃ  trace plot. PhÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ quÃ¡ trÃ¬nh há»™i tá»¥ cá»§a mÃ´ hÃ¬nh Bayes. Cá»™t bÃªn trÃ¡i cho ta tháº¥y phÃ¢n phá»‘i, trong khi bÃªn pháº£i cho ta tháº¥y sá»± giao Ä‘á»™ng cá»§a cÃ¡c tham sá»‘.\nTá»« káº¿t quáº£ mÃ´ hÃ¬nh trÃªn, ta ra Ä‘Æ°á»£c káº¿t luáº­n ráº±ng, há»‡ sá»‘ beta trung bÃ¬nh cho cá»• phiáº¿u HPG cÅ©ng lÃ  1.23, vÃ  ta cÃ³ thá»ƒ tá»± tin nÃ³i ráº±ng, 90% trÆ°á»ng há»£p há»‡ sá»‘ beta cá»§a HPG sáº½ trong vÃ¹ng 1.18 - 1.23.\nKáº¿t luáº­n Tá»« 3 phÆ°Æ¡ng phÃ¡p trÃªn ta Ä‘á»u thu Ä‘Æ°á»£c chung 1 káº¿t quáº£. NgoÃ i ra, má»—i phÆ°Æ¡ng phÃ¡p cÃ³ Æ°u vÃ  nhÆ°á»£c Ä‘iá»ƒm cá»§a mÃ¬nh.\nVá»›i phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n trá»±c tiáº¿p, ta cÃ³ thá»ƒ dá»… dÃ ng tÃ­nh toÃ¡n ra Ä‘Æ°á»£c Ä‘Ã¡p Ã¡n vá»›i Ä‘á»™ phá»©c táº¡p lÃ  tháº¥p nháº¥t. Tuy nhiÃªn, vá»›i nhá»¯ng trÆ°á»ng há»£p mÃ  dá»¯ liá»‡u lÃ  khÃ´ng kháº£ nghá»‹ch (singular), thÃ¬ bÃ i toÃ¡n cÃ³ kháº£ nÄƒng khÃ´ng cÃ³ kháº£ nÄƒng giáº£i Ä‘Æ°á»£c theo phÆ°Æ¡ng phÃ¡p nÃ y (analytical solution).\nNgÆ°á»£c láº¡i, vá»›i phÆ°Æ¡ng phÃ¡p deep learning, ta cÃ³ thá»ƒ dá»… dÃ ng xá»­ lÃ½ cÃ¡c bÃ i toÃ¡n khi khÃ´ng thá»ƒ giáº£i trá»±c tiáº¿p (analytical solution). ThÃ´ng qua Gradient Descent, ta cÃ³ thá»ƒ Æ°á»›c tÃ­nh Ä‘Æ°á»£c cÃ¡c Ä‘iá»ƒm tá»‘i Æ°u, tuy nhiÃªn Ä‘i kÃ¨m vá»›i Ä‘Ã³ lÃ  yÃªu cáº§u tÃ­nh toÃ¡n lá»›n hÆ¡n nhiá»u (computational cost).\nCuá»‘i cÃ¹ng, phÆ°Æ¡ng phÃ¡p Bayesian cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng khi ta mong muá»‘n Ä‘Æ°a niá»m tin cá»§a ta vÃ o mÃ´ hÃ¬nh, cÅ©ng nhÆ° Ä‘áº§u ra mong muá»‘n lÃ  má»™t phÃ¢n phá»‘i cÃ¡c káº¿t quáº£ cÃ³ thá»ƒ xáº£y ra thay vÃ¬ chá»‰ muá»‘n má»™t Ä‘iá»ƒm Æ°á»›c lÆ°á»£ng duy nháº¥t. Quan trá»ng nháº¥t lÃ  mÃ´ hÃ¬nh hoÃ¡ Ä‘Æ°á»£c Ä‘á»™ khÃ´ng cháº¯c cháº¯n (uncertainty). Tuy nhiÃªn, yáº¿u tá»‘ giáº£ Ä‘á»‹nh cá»§a ngÆ°á»i sá»­ dá»¥ng cÃ³ thá»ƒ cÃ³ tÃ¡c Ä‘á»™ng tiÃªu cá»±c Ä‘áº¿n mÃ´ hÃ¬nh náº¿u giáº£ Ä‘á»‹nh sai.\nVÃ  táº¥t nhiÃªn, 2 phÆ°Æ¡ng phÃ¡p sau lÃ  phá»©c táº¡p hoÃ¡ váº¥n Ä‘á» cho bÃ i toÃ¡n Ä‘Æ¡n giáº£n nÃ y. Tuy nhiÃªn, Ä‘á»ƒ gáº§n gÅ©i nháº¥t thÃ¬ mÃ¬nh chá»n Æ°á»›c lÆ°á»£ng beta cho dá»… hÃ¬nh dung!\n","permalink":"http://localhost:1313/posts/2024/2024-10-10-linear-regression/","summary":"\u003cp\u003eHÃ´m nay, chÃºng ta sáº½ cÃ¹ng nhau tháº£o luáº­n vá» cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhau Ä‘á»ƒ giáº£i quyáº¿t bÃ i toÃ¡n Linear regression - Há»“i quy tuyáº¿n tÃ­nh.\u003c/p\u003e\n\u003ch2 id=\"váº­y-linear-regression-lÃ -gÃ¬\"\u003eVáº­y Linear regression lÃ  gÃ¬?\u003c/h2\u003e\n\u003cp\u003eHá»“i quy tuyáº¿n tÃ­nh lÃ  phÆ°Æ¡ng phÃ¡p thá»‘ng kÃª dÃ¹ng Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a má»‘i quan há»‡ giá»¯a biáº¿n phá»¥ thuá»™c vÃ  cÃ¡c biáº¿n Ä‘á»™c láº­p. Hay nÃ³i cÃ¡ch khÃ¡c, há»“i quy tuyáº¿n tÃ­nh lÃ  Ä‘i tÃ¬m má»‘i quan há»‡ tuyáº¿n tÃ­nh (y=ax+b) giá»¯a 2 biáº¿n vá»›i nhau.\u003c/p\u003e","title":"Linear regression"},{"content":"Hi, mÃ¬nh lÃ  HÃ¹ng.\nHiá»‡n mÃ¬nh lÃ  sinh viÃªn tháº¡c sÄ© AI táº¡i RMIT Vietnam vá»›i Ä‘am mÃª dÃ nh cho máº£ng Ä‘áº§u tÆ° Ä‘á»‹nh lÆ°á»£ng (Quant).\nBlog nÃ y lÃ  nÆ¡i mÃ¬nh chia sáº» kiáº¿n thá»©c (Ä‘a pháº§n lÃ  lÆ°u trá»¯ cÃ¡i mÃ¬nh há»c) vÃ  náº¿u nÃ³ giÃºp Ä‘Æ°á»£c báº¡n trong chuyáº¿n phiÃªu lÆ°u ngÃ nh quant thÃ¬ mÃ¬nh ráº¥t vui!\nCÃ¡c máº£ng yÃªu thÃ­ch á»¨ng dá»¥ng: Learning to Rank, Probabilistic forecasting, Network Science â€¦ LÃ½ thuyáº¿t: Causal ML, Bayesian Inference, Stochastic process â€¦ TÃ i chÃ­nh: Portfolio optimization, Asset pricing, Factor investing, â€¦ Ráº¥t mong muá»‘n há»£p tÃ¡c Ä‘á»ƒ thá»±c hiá»‡n publication (khÃ´ng giá»›i háº¡n vÃ o cÃ¡c chá»§ Ä‘á» trÃªn) Há»c váº¥n MSc Artificial Intelligent (RMIT Vietnam) BSc Economics \u0026amp; Finance (RMIT Vietnam) CFA lev 1 Self taught Data Sci \u0026amp; Quant! LiÃªn há»‡ Email: hungha1412@gmail.com LinkedIn: qhung ","permalink":"http://localhost:1313/about/","summary":"\u003cp\u003eHi, mÃ¬nh lÃ  HÃ¹ng.\u003c/p\u003e\n\u003cp\u003eHiá»‡n mÃ¬nh lÃ  sinh viÃªn tháº¡c sÄ© AI táº¡i RMIT Vietnam vá»›i Ä‘am mÃª dÃ nh cho máº£ng Ä‘áº§u tÆ° Ä‘á»‹nh lÆ°á»£ng (Quant).\u003c/p\u003e\n\u003cp\u003eBlog nÃ y lÃ  nÆ¡i mÃ¬nh chia sáº» kiáº¿n thá»©c (Ä‘a pháº§n lÃ  lÆ°u trá»¯ cÃ¡i mÃ¬nh há»c) vÃ  náº¿u nÃ³ giÃºp Ä‘Æ°á»£c báº¡n trong chuyáº¿n phiÃªu lÆ°u ngÃ nh quant thÃ¬ mÃ¬nh ráº¥t vui!\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"cÃ¡c-máº£ng-yÃªu-thÃ­ch\"\u003eCÃ¡c máº£ng yÃªu thÃ­ch\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eá»¨ng dá»¥ng: Learning to Rank, Probabilistic forecasting, Network Science â€¦\u003c/li\u003e\n\u003cli\u003eLÃ½ thuyáº¿t: Causal ML, Bayesian Inference, Stochastic process â€¦\u003c/li\u003e\n\u003cli\u003eTÃ i chÃ­nh: Portfolio optimization, Asset pricing, Factor investing, â€¦\u003c/li\u003e\n\u003cli\u003eRáº¥t mong muá»‘n há»£p tÃ¡c Ä‘á»ƒ thá»±c hiá»‡n publication (khÃ´ng giá»›i háº¡n vÃ o cÃ¡c chá»§ Ä‘á» trÃªn)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003c!-- raw HTML omitted --\u003e\n\u003chr\u003e\n\u003ch2 id=\"há»c-váº¥n\"\u003eHá»c váº¥n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eMSc Artificial Intelligent (RMIT Vietnam)\u003c/li\u003e\n\u003cli\u003eBSc Economics \u0026amp; Finance (RMIT Vietnam)\u003c/li\u003e\n\u003cli\u003eCFA lev 1\u003c/li\u003e\n\u003cli\u003eSelf taught Data Sci \u0026amp; Quant!\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"liÃªn-há»‡\"\u003eLiÃªn há»‡\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eEmail: \u003ca href=\"mailto:hungha1412@gmail.com\"\u003ehungha1412@gmail.com\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eLinkedIn: \u003ca href=\"https://www.linkedin.com/in/haquochung11/\"\u003eqhung\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"About"},{"content":"","permalink":"http://localhost:1313/blog/","summary":"blog","title":"Blog"},{"content":"","permalink":"http://localhost:1313/research/","summary":"","title":"Research"}]